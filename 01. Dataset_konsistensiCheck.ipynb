{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98dc817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç DUAL DATASET CONSISTENCY CHECKER & FIXER\n",
      "======================================================================\n",
      "Processing both I3D and RLT datasets automatically\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# PROCESSING: I3D\n",
      "######################################################################\n",
      "\n",
      "üìÇ Loading I3D datasets...\n",
      "  ‚úì text_indonesian: 1,568 rows\n",
      "  ‚úì text_english: 1,568 rows\n",
      "  ‚úì number_features: 1,568 rows\n",
      "  ‚úì audio_features: 1,568 rows\n",
      "  ‚úì pause_features: 1,568 rows\n",
      "  ‚úì landmarks: 647,871 rows\n",
      "  ‚úì multimodal_full: 1,568 rows\n",
      "  ‚úì publication: 1,568 rows\n",
      "\n",
      "======================================================================\n",
      "üîç CHECKING I3D CONSISTENCY\n",
      "======================================================================\n",
      "\n",
      "üìä CHECK 1: UNIQUE VIDEOS\n",
      "----------------------------------------------------------------------\n",
      "  ‚Ä¢ landmarks: 1568 unique videos\n",
      "  ‚Ä¢ text_indonesian: 1568 unique videos\n",
      "  ‚Ä¢ text_english: 1568 unique videos\n",
      "  ‚Ä¢ audio_features: 1568 unique videos\n",
      "  ‚Ä¢ multimodal_full: 1568 unique videos\n",
      "  ‚úÖ All datasets have consistent video names\n",
      "\n",
      "üìä CHECK 2: SAMPLES PER VIDEO\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian:\n",
      "  ‚îú‚îÄ Min: 1\n",
      "  ‚îú‚îÄ Max: 1\n",
      "  ‚îú‚îÄ Mean: 1.00\n",
      "  ‚îî‚îÄ Median: 1\n",
      "\n",
      "text_english:\n",
      "  ‚îú‚îÄ Min: 1\n",
      "  ‚îú‚îÄ Max: 1\n",
      "  ‚îú‚îÄ Mean: 1.00\n",
      "  ‚îî‚îÄ Median: 1\n",
      "\n",
      "audio_features:\n",
      "  ‚îú‚îÄ Min: 1\n",
      "  ‚îú‚îÄ Max: 1\n",
      "  ‚îú‚îÄ Mean: 1.00\n",
      "  ‚îî‚îÄ Median: 1\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ Min frames: 36\n",
      "  ‚îú‚îÄ Max frames: 2401\n",
      "  ‚îú‚îÄ Mean frames: 413.18\n",
      "  ‚îî‚îÄ Median frames: 329\n",
      "\n",
      "üìä CHECK 3: CLASS DISTRIBUTION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian:\n",
      "  ‚îú‚îÄ TRUTH (0): 784 (50.0%)\n",
      "  ‚îî‚îÄ LIE (1): 784 (50.0%)\n",
      "\n",
      "audio_features:\n",
      "  ‚îú‚îÄ TRUTH (0): 784 (50.0%)\n",
      "  ‚îî‚îÄ LIE (1): 784 (50.0%)\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ TRUTH (0): 354,861 (54.8%)\n",
      "  ‚îî‚îÄ LIE (1): 293,010 (45.2%)\n",
      "\n",
      "multimodal_full:\n",
      "  ‚îú‚îÄ TRUTH (0): 784 (50.0%)\n",
      "  ‚îî‚îÄ LIE (1): 784 (50.0%)\n",
      "\n",
      "üìä CHECK 4: MISSING DATA\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian: ‚úÖ No missing values\n",
      "\n",
      "text_english: ‚úÖ No missing values\n",
      "\n",
      "number_features: ‚úÖ No missing values\n",
      "\n",
      "audio_features: ‚úÖ No missing values\n",
      "\n",
      "pause_features: ‚úÖ No missing values\n",
      "\n",
      "landmarks: ‚úÖ No missing values\n",
      "\n",
      "multimodal_full:\n",
      "  ‚îú‚îÄ Columns with missing: 1\n",
      "  ‚îî‚îÄ Total missing values: 1,568\n",
      "     ‚Ä¢ reextraction_params: 1,568 (100.0%)\n",
      "\n",
      "publication: ‚úÖ No missing values\n",
      "\n",
      "üìä CHECK 5: COLUMN CONSISTENCY\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian:\n",
      "  ‚îú‚îÄ Total columns: 16\n",
      "  ‚îî‚îÄ ‚úÖ All expected columns present\n",
      "\n",
      "text_english:\n",
      "  ‚îú‚îÄ Total columns: 11\n",
      "  ‚îî‚îÄ ‚úÖ All expected columns present\n",
      "\n",
      "audio_features:\n",
      "  ‚îú‚îÄ Total columns: 104\n",
      "  ‚îî‚îÄ ‚úÖ All expected columns present\n",
      "\n",
      "pause_features:\n",
      "  ‚îú‚îÄ Total columns: 20\n",
      "  ‚îî‚îÄ ‚úÖ All expected columns present\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ Total columns: 1536\n",
      "  ‚îî‚îÄ ‚úÖ All expected columns present\n",
      "\n",
      "üìä CHECK 6: DATA TYPES\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian:\n",
      "  ‚îú‚îÄ Numeric columns: 9\n",
      "  ‚îî‚îÄ Text columns: 6\n",
      "\n",
      "text_english:\n",
      "  ‚îú‚îÄ Numeric columns: 7\n",
      "  ‚îî‚îÄ Text columns: 4\n",
      "\n",
      "number_features:\n",
      "  ‚îú‚îÄ Numeric columns: 3\n",
      "  ‚îî‚îÄ Text columns: 2\n",
      "\n",
      "audio_features:\n",
      "  ‚îú‚îÄ Numeric columns: 101\n",
      "  ‚îî‚îÄ Text columns: 3\n",
      "\n",
      "pause_features:\n",
      "  ‚îú‚îÄ Numeric columns: 15\n",
      "  ‚îî‚îÄ Text columns: 2\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ Numeric columns: 1535\n",
      "  ‚îî‚îÄ Text columns: 1\n",
      "\n",
      "multimodal_full:\n",
      "  ‚îú‚îÄ Numeric columns: 132\n",
      "  ‚îî‚îÄ Text columns: 8\n",
      "\n",
      "publication:\n",
      "  ‚îú‚îÄ Numeric columns: 14\n",
      "  ‚îî‚îÄ Text columns: 5\n",
      "\n",
      "üìä CHECK 7: VALUE RANGES\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ Min value: -1.5429\n",
      "  ‚îî‚îÄ Max value: 1.5102\n",
      "  ‚ö†Ô∏è Values outside expected range [0, 1]!\n",
      "\n",
      "audio_features (MFCC):\n",
      "  ‚îú‚îÄ Min value: -781.15\n",
      "  ‚îî‚îÄ Max value: 242.82\n",
      "  ‚ö†Ô∏è Unusual MFCC values detected!\n",
      "\n",
      "üìä CHECK 8: LANDMARK QUALITY\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Detection rates:\n",
      "  ‚îú‚îÄ Face: 90.1% (583,776/647,871)\n",
      "  ‚îú‚îÄ Iris: 90.1% (583,776/647,871)\n",
      "  ‚îî‚îÄ Pose: 100.0% (647,871/647,871)\n",
      "\n",
      "======================================================================\n",
      "üîß APPLYING FIXES TO I3D\n",
      "======================================================================\n",
      "\n",
      "üîß Fix 1: Removing duplicates...\n",
      "  ‚Ä¢ text_indonesian: No duplicates found\n",
      "  ‚Ä¢ text_english: No duplicates found\n",
      "  ‚Ä¢ audio_features: No duplicates found\n",
      "  ‚Ä¢ multimodal_full: No duplicates found\n",
      "\n",
      "üîß Fix 2: Filling missing values...\n",
      "  ‚Ä¢ text_indonesian: No missing values\n",
      "  ‚Ä¢ text_english: No missing values\n",
      "  ‚Ä¢ number_features: No missing values\n",
      "  ‚Ä¢ audio_features: No missing values\n",
      "  ‚Ä¢ pause_features: No missing values\n",
      "  ‚Ä¢ landmarks: No missing values\n",
      "  ‚Ä¢ multimodal_full: Filled 1568 missing values\n",
      "  ‚Ä¢ publication: No missing values\n",
      "\n",
      "üîß Fix 3: Standardizing video names...\n",
      "  ‚Ä¢ text_indonesian: Standardized 1568 filenames\n",
      "  ‚Ä¢ text_english: Standardized 1568 filenames\n",
      "  ‚Ä¢ audio_features: Standardized 1568 filenames\n",
      "  ‚Ä¢ multimodal_full: Standardized 1568 filenames\n",
      "\n",
      "üîß Fix 4: Fixing data types...\n",
      "  ‚Ä¢ text_indonesian.label: Already correct type\n",
      "  ‚Ä¢ text_english.label: Already correct type\n",
      "  ‚Ä¢ number_features.label: Already correct type\n",
      "  ‚Ä¢ audio_features.label: Already correct type\n",
      "  ‚Ä¢ pause_features.label: Already correct type\n",
      "  ‚Ä¢ landmarks.Class: Already correct type\n",
      "  ‚Ä¢ multimodal_full.label: Already correct type\n",
      "  ‚Ä¢ publication.label: Already correct type\n",
      "\n",
      "üîß Fix 5: Removing videos with insufficient data...\n",
      "  ‚Ä¢ landmarks: All videos have sufficient frames\n",
      "\n",
      "‚úÖ Total fixes applied: 5\n",
      "\n",
      "üíæ Saving fixed I3D datasets...\n",
      "  ‚úì text_indonesian: Saved (1,568 rows)\n",
      "  ‚úì text_english: Saved (1,568 rows)\n",
      "  ‚úì number_features: Saved (1,568 rows)\n",
      "  ‚úì audio_features: Saved (1,568 rows)\n",
      "  ‚úì pause_features: Saved (1,568 rows)\n",
      "  ‚úì landmarks: Saved (647,871 rows)\n",
      "  ‚úì multimodal_full: Saved (1,568 rows)\n",
      "  ‚úì publication: Saved (1,568 rows)\n",
      "\n",
      "‚úÖ Saved 8 datasets\n",
      "\n",
      "üìù Generating I3D reports...\n",
      "  ‚úì Markdown: consistency_report_20260202_074930.md\n",
      "  ‚úì JSON: consistency_report_20260202_074930.json\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# PROCESSING: RLT\n",
      "######################################################################\n",
      "\n",
      "üìÇ Loading RLT datasets...\n",
      "  ‚úì text_indonesian: 121 rows\n",
      "  ‚úì text_english: 121 rows\n",
      "  ‚úì number_features: 121 rows\n",
      "  ‚úì audio_features: 121 rows\n",
      "  ‚úì pause_features: 121 rows\n",
      "  ‚úì landmarks: 89,853 rows\n",
      "  ‚úì multimodal_full: 121 rows\n",
      "  ‚úì publication: 121 rows\n",
      "\n",
      "======================================================================\n",
      "üîç CHECKING RLT CONSISTENCY\n",
      "======================================================================\n",
      "\n",
      "üìä CHECK 1: UNIQUE VIDEOS\n",
      "----------------------------------------------------------------------\n",
      "  ‚Ä¢ landmarks: 121 unique videos\n",
      "  ‚Ä¢ text_indonesian: 121 unique videos\n",
      "  ‚Ä¢ text_english: 121 unique videos\n",
      "  ‚Ä¢ audio_features: 121 unique videos\n",
      "  ‚Ä¢ multimodal_full: 121 unique videos\n",
      "  ‚úÖ All datasets have consistent video names\n",
      "\n",
      "üìä CHECK 2: SAMPLES PER VIDEO\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian:\n",
      "  ‚îú‚îÄ Min: 1\n",
      "  ‚îú‚îÄ Max: 1\n",
      "  ‚îú‚îÄ Mean: 1.00\n",
      "  ‚îî‚îÄ Median: 1\n",
      "\n",
      "text_english:\n",
      "  ‚îú‚îÄ Min: 1\n",
      "  ‚îú‚îÄ Max: 1\n",
      "  ‚îú‚îÄ Mean: 1.00\n",
      "  ‚îî‚îÄ Median: 1\n",
      "\n",
      "audio_features:\n",
      "  ‚îú‚îÄ Min: 1\n",
      "  ‚îú‚îÄ Max: 1\n",
      "  ‚îú‚îÄ Mean: 1.00\n",
      "  ‚îî‚îÄ Median: 1\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ Min frames: 113\n",
      "  ‚îú‚îÄ Max frames: 2443\n",
      "  ‚îú‚îÄ Mean frames: 742.59\n",
      "  ‚îî‚îÄ Median frames: 699\n",
      "\n",
      "üìä CHECK 3: CLASS DISTRIBUTION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian:\n",
      "  ‚îú‚îÄ TRUTH (0): 60 (49.6%)\n",
      "  ‚îî‚îÄ LIE (1): 61 (50.4%)\n",
      "\n",
      "audio_features:\n",
      "  ‚îú‚îÄ TRUTH (0): 60 (49.6%)\n",
      "  ‚îî‚îÄ LIE (1): 61 (50.4%)\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ TRUTH (0): 41,855 (46.6%)\n",
      "  ‚îî‚îÄ LIE (1): 47,998 (53.4%)\n",
      "\n",
      "multimodal_full:\n",
      "  ‚îú‚îÄ TRUTH (0): 60 (49.6%)\n",
      "  ‚îî‚îÄ LIE (1): 61 (50.4%)\n",
      "\n",
      "üìä CHECK 4: MISSING DATA\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian: ‚úÖ No missing values\n",
      "\n",
      "text_english: ‚úÖ No missing values\n",
      "\n",
      "number_features: ‚úÖ No missing values\n",
      "\n",
      "audio_features: ‚úÖ No missing values\n",
      "\n",
      "pause_features: ‚úÖ No missing values\n",
      "\n",
      "landmarks: ‚úÖ No missing values\n",
      "\n",
      "multimodal_full: ‚úÖ No missing values\n",
      "\n",
      "publication: ‚úÖ No missing values\n",
      "\n",
      "üìä CHECK 5: COLUMN CONSISTENCY\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian:\n",
      "  ‚îú‚îÄ Total columns: 10\n",
      "  ‚îú‚îÄ Missing expected: 2\n",
      "     ‚Ä¢ text_indonesian_normalized\n",
      "     ‚Ä¢ text_indonesian_original\n",
      "\n",
      "text_english:\n",
      "  ‚îú‚îÄ Total columns: 10\n",
      "  ‚îú‚îÄ Missing expected: 1\n",
      "     ‚Ä¢ text_english\n",
      "\n",
      "audio_features:\n",
      "  ‚îú‚îÄ Total columns: 103\n",
      "  ‚îî‚îÄ ‚úÖ All expected columns present\n",
      "\n",
      "pause_features:\n",
      "  ‚îú‚îÄ Total columns: 20\n",
      "  ‚îî‚îÄ ‚úÖ All expected columns present\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ Total columns: 1536\n",
      "  ‚îî‚îÄ ‚úÖ All expected columns present\n",
      "\n",
      "üìä CHECK 6: DATA TYPES\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "text_indonesian:\n",
      "  ‚îú‚îÄ Numeric columns: 7\n",
      "  ‚îî‚îÄ Text columns: 3\n",
      "\n",
      "text_english:\n",
      "  ‚îú‚îÄ Numeric columns: 7\n",
      "  ‚îî‚îÄ Text columns: 3\n",
      "\n",
      "number_features:\n",
      "  ‚îú‚îÄ Numeric columns: 3\n",
      "  ‚îî‚îÄ Text columns: 2\n",
      "\n",
      "audio_features:\n",
      "  ‚îú‚îÄ Numeric columns: 101\n",
      "  ‚îî‚îÄ Text columns: 2\n",
      "\n",
      "pause_features:\n",
      "  ‚îú‚îÄ Numeric columns: 15\n",
      "  ‚îî‚îÄ Text columns: 2\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ Numeric columns: 1535\n",
      "  ‚îî‚îÄ Text columns: 1\n",
      "\n",
      "multimodal_full:\n",
      "  ‚îú‚îÄ Numeric columns: 135\n",
      "  ‚îî‚îÄ Text columns: 5\n",
      "\n",
      "publication:\n",
      "  ‚îú‚îÄ Numeric columns: 17\n",
      "  ‚îî‚îÄ Text columns: 4\n",
      "\n",
      "üìä CHECK 7: VALUE RANGES\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "landmarks:\n",
      "  ‚îú‚îÄ Min value: -4.2945\n",
      "  ‚îî‚îÄ Max value: 5.5292\n",
      "  ‚ö†Ô∏è Values outside expected range [0, 1]!\n",
      "\n",
      "audio_features (MFCC):\n",
      "  ‚îú‚îÄ Min value: -602.86\n",
      "  ‚îî‚îÄ Max value: 191.83\n",
      "  ‚ö†Ô∏è Unusual MFCC values detected!\n",
      "\n",
      "üìä CHECK 8: LANDMARK QUALITY\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Detection rates:\n",
      "  ‚îú‚îÄ Face: 96.0% (86,248/89,853)\n",
      "  ‚îú‚îÄ Iris: 96.0% (86,248/89,853)\n",
      "  ‚îî‚îÄ Pose: 99.7% (89,583/89,853)\n",
      "\n",
      "======================================================================\n",
      "üîß APPLYING FIXES TO RLT\n",
      "======================================================================\n",
      "\n",
      "üîß Fix 1: Removing duplicates...\n",
      "  ‚Ä¢ text_indonesian: No duplicates found\n",
      "  ‚Ä¢ text_english: No duplicates found\n",
      "  ‚Ä¢ audio_features: No duplicates found\n",
      "  ‚Ä¢ multimodal_full: No duplicates found\n",
      "\n",
      "üîß Fix 2: Filling missing values...\n",
      "  ‚Ä¢ text_indonesian: No missing values\n",
      "  ‚Ä¢ text_english: No missing values\n",
      "  ‚Ä¢ number_features: No missing values\n",
      "  ‚Ä¢ audio_features: No missing values\n",
      "  ‚Ä¢ pause_features: No missing values\n",
      "  ‚Ä¢ landmarks: No missing values\n",
      "  ‚Ä¢ multimodal_full: No missing values\n",
      "  ‚Ä¢ publication: No missing values\n",
      "\n",
      "üîß Fix 3: Standardizing video names...\n",
      "  ‚Ä¢ text_indonesian: Standardized 121 filenames\n",
      "  ‚Ä¢ text_english: Standardized 121 filenames\n",
      "  ‚Ä¢ audio_features: Standardized 121 filenames\n",
      "  ‚Ä¢ multimodal_full: Standardized 121 filenames\n",
      "\n",
      "üîß Fix 4: Fixing data types...\n",
      "  ‚Ä¢ text_indonesian.label: Already correct type\n",
      "  ‚Ä¢ text_english.label: Already correct type\n",
      "  ‚Ä¢ number_features.label: Already correct type\n",
      "  ‚Ä¢ audio_features.label: Already correct type\n",
      "  ‚Ä¢ pause_features.label: Already correct type\n",
      "  ‚Ä¢ landmarks.Class: Already correct type\n",
      "  ‚Ä¢ multimodal_full.label: Already correct type\n",
      "  ‚Ä¢ publication.label: Already correct type\n",
      "\n",
      "üîß Fix 5: Removing videos with insufficient data...\n",
      "  ‚Ä¢ landmarks: All videos have sufficient frames\n",
      "\n",
      "‚úÖ Total fixes applied: 4\n",
      "\n",
      "üíæ Saving fixed RLT datasets...\n",
      "  ‚úì text_indonesian: Saved (121 rows)\n",
      "  ‚úì text_english: Saved (121 rows)\n",
      "  ‚úì number_features: Saved (121 rows)\n",
      "  ‚úì audio_features: Saved (121 rows)\n",
      "  ‚úì pause_features: Saved (121 rows)\n",
      "  ‚úì landmarks: Saved (89,853 rows)\n",
      "  ‚úì multimodal_full: Saved (121 rows)\n",
      "  ‚úì publication: Saved (121 rows)\n",
      "\n",
      "‚úÖ Saved 8 datasets\n",
      "\n",
      "üìù Generating RLT reports...\n",
      "  ‚úì Markdown: consistency_report_20260202_075611.md\n",
      "  ‚úì JSON: consistency_report_20260202_075611.json\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìä FINAL SUMMARY - BOTH DATASETS\n",
      "======================================================================\n",
      "\n",
      "I3D:\n",
      "  ‚úÖ Status: SUCCESS\n",
      "  ‚îú‚îÄ Issues found: 1\n",
      "  ‚îú‚îÄ Fixes applied: 5\n",
      "  ‚îú‚îÄ MD report: consistency_report_20260202_074930.md\n",
      "  ‚îî‚îÄ JSON report: consistency_report_20260202_074930.json\n",
      "\n",
      "RLT:\n",
      "  ‚úÖ Status: SUCCESS\n",
      "  ‚îú‚îÄ Issues found: 3\n",
      "  ‚îú‚îÄ Fixes applied: 4\n",
      "  ‚îú‚îÄ MD report: consistency_report_20260202_075611.md\n",
      "  ‚îî‚îÄ JSON report: consistency_report_20260202_075611.json\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DUAL DATASET PROCESSING COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "=============================================================================\n",
    "DATASET CONSISTENCY CHECKER & FIXER - DUAL PROCESSOR\n",
    "=============================================================================\n",
    "Verify and fix consistency for BOTH I3D and RLT datasets automatically\n",
    "No user interaction required - fully automated\n",
    "=============================================================================\n",
    "Version: 2.1\n",
    "Author: Yeni Dwi Rahayu\n",
    "Date: 2025-01-14\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "class ConsistencyConfig:\n",
    "    \"\"\"Configuration for consistency checker\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name: str = 'I3D'):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.base_dir = Path(os.getcwd())\n",
    "        self.dataset_dir = self.base_dir / \"dataset\" / \"processed\" / dataset_name\n",
    "        \n",
    "        # Expected paths\n",
    "        self.paths = {\n",
    "            'text_indonesian': self.dataset_dir / \"text\" / \"TextDataset_Indonesian.csv\",\n",
    "            'text_english': self.dataset_dir / \"text\" / \"TextDataset_English.csv\",\n",
    "            'number_features': self.dataset_dir / \"text\" / \"NumberFeatures.csv\",\n",
    "            'audio_features': self.dataset_dir / \"audio\" / \"AudioDataset_Features.csv\",\n",
    "            'pause_features': self.dataset_dir / \"audio\" / \"PauseFeatures.csv\",\n",
    "            'landmarks': self.dataset_dir / \"visual\" / \"LandmarkDataset.csv\",\n",
    "            'multimodal_full': self.dataset_dir / \"multimodal\" / \"MultimodalDataset_Full.csv\",\n",
    "            'publication': self.dataset_dir / \"multimodal\" / \"PublicationDataset.csv\",\n",
    "            'validation': self.dataset_dir.parent.parent / \"validation\" / dataset_name,\n",
    "            'reextraction': self.dataset_dir / \"reextraction\"\n",
    "        }\n",
    "        \n",
    "        # Create validation directory\n",
    "        self.paths['validation'].mkdir(parents=True, exist_ok=True)\n",
    "        self.paths['reextraction'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ==================== DATASET LOADER ====================\n",
    "class DatasetLoader:\n",
    "    \"\"\"Load all datasets with error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ConsistencyConfig):\n",
    "        self.config = config\n",
    "        self.datasets = {}\n",
    "        self.load_status = {}\n",
    "    \n",
    "    def load_all(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Load all datasets\"\"\"\n",
    "        print(f\"\\nüìÇ Loading {self.config.dataset_name} datasets...\")\n",
    "        \n",
    "        for name, path in self.config.paths.items():\n",
    "            if name in ['validation', 'reextraction']:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                if path.exists():\n",
    "                    df = pd.read_csv(path, encoding='utf-8')\n",
    "                    self.datasets[name] = df\n",
    "                    self.load_status[name] = 'success'\n",
    "                    print(f\"  ‚úì {name}: {len(df):,} rows\")\n",
    "                else:\n",
    "                    self.load_status[name] = 'not_found'\n",
    "                    print(f\"  ‚ö†Ô∏è {name}: File not found\")\n",
    "            except Exception as e:\n",
    "                self.load_status[name] = f'error: {str(e)}'\n",
    "                print(f\"  ‚ùå {name}: Error loading - {str(e)}\")\n",
    "        \n",
    "        return self.datasets\n",
    "    \n",
    "    def get_dataset(self, name: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Get specific dataset\"\"\"\n",
    "        return self.datasets.get(name)\n",
    "\n",
    "\n",
    "# ==================== CONSISTENCY CHECKER ====================\n",
    "class ConsistencyChecker:\n",
    "    \"\"\"Check consistency across datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, datasets: Dict[str, pd.DataFrame], config: ConsistencyConfig):\n",
    "        self.datasets = datasets\n",
    "        self.config = config\n",
    "        self.issues = []\n",
    "        self.stats = {}\n",
    "    \n",
    "    def check_all(self) -> Dict:\n",
    "        \"\"\"Run all consistency checks\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîç CHECKING {self.config.dataset_name} CONSISTENCY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        results = {\n",
    "            'unique_videos': self.check_unique_videos(),\n",
    "            'samples_per_video': self.check_samples_per_video(),\n",
    "            'class_distribution': self.check_class_distribution(),\n",
    "            'missing_data': self.check_missing_data(),\n",
    "            'column_consistency': self.check_column_consistency(),\n",
    "            'data_types': self.check_data_types(),\n",
    "            'value_ranges': self.check_value_ranges(),\n",
    "            'landmark_quality': self.check_landmark_quality()\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def check_unique_videos(self) -> Dict:\n",
    "        \"\"\"Check 1: Unique videos across datasets\"\"\"\n",
    "        print(f\"\\nüìä CHECK 1: UNIQUE VIDEOS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = {'status': 'pass', 'details': {}}\n",
    "        \n",
    "        # Get unique identifiers from each dataset\n",
    "        video_sets = {}\n",
    "        \n",
    "        if 'landmarks' in self.datasets:\n",
    "            video_sets['landmarks'] = set(self.datasets['landmarks']['Video_Name'].unique())\n",
    "        \n",
    "        for name in ['text_indonesian', 'text_english', 'audio_features', 'multimodal_full']:\n",
    "            if name in self.datasets:\n",
    "                # Remove file extensions\n",
    "                filenames = self.datasets[name]['filename'].unique()\n",
    "                video_sets[name] = set([f.rsplit('.', 1)[0] if '.' in f else f for f in filenames])\n",
    "        \n",
    "        # Print counts\n",
    "        for name, videos in video_sets.items():\n",
    "            print(f\"  ‚Ä¢ {name}: {len(videos)} unique videos\")\n",
    "            result['details'][name] = len(videos)\n",
    "        \n",
    "        # Check consistency\n",
    "        if len(video_sets) > 1:\n",
    "            reference_set = list(video_sets.values())[0]\n",
    "            all_match = all(vset == reference_set for vset in video_sets.values())\n",
    "            \n",
    "            if all_match:\n",
    "                print(f\"  ‚úÖ All datasets have consistent video names\")\n",
    "                result['status'] = 'pass'\n",
    "            else:\n",
    "                print(f\"  ‚ùå Video name mismatch detected\")\n",
    "                result['status'] = 'fail'\n",
    "                \n",
    "                # Find differences\n",
    "                for name1, set1 in video_sets.items():\n",
    "                    for name2, set2 in video_sets.items():\n",
    "                        if name1 < name2:  # Avoid duplicate comparisons\n",
    "                            missing_in_2 = set1 - set2\n",
    "                            missing_in_1 = set2 - set1\n",
    "                            \n",
    "                            if missing_in_2:\n",
    "                                print(f\"  ‚ö†Ô∏è In {name1} but NOT in {name2}: {len(missing_in_2)}\")\n",
    "                                self.issues.append({\n",
    "                                    'type': 'missing_videos',\n",
    "                                    'from': name1,\n",
    "                                    'to': name2,\n",
    "                                    'count': len(missing_in_2),\n",
    "                                    'videos': list(missing_in_2)[:10]\n",
    "                                })\n",
    "                            \n",
    "                            if missing_in_1:\n",
    "                                print(f\"  ‚ö†Ô∏è In {name2} but NOT in {name1}: {len(missing_in_1)}\")\n",
    "                                self.issues.append({\n",
    "                                    'type': 'missing_videos',\n",
    "                                    'from': name2,\n",
    "                                    'to': name1,\n",
    "                                    'count': len(missing_in_1),\n",
    "                                    'videos': list(missing_in_1)[:10]\n",
    "                                })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_samples_per_video(self) -> Dict:\n",
    "        \"\"\"Check 2: Samples/frames per video\"\"\"\n",
    "        print(f\"\\nüìä CHECK 2: SAMPLES PER VIDEO\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = {'status': 'pass', 'details': {}}\n",
    "        \n",
    "        # Text/Audio datasets (should have 1 sample per video)\n",
    "        for name in ['text_indonesian', 'text_english', 'audio_features']:\n",
    "            if name in self.datasets:\n",
    "                counts = self.datasets[name]['filename'].value_counts()\n",
    "                \n",
    "                print(f\"\\n{name}:\")\n",
    "                print(f\"  ‚îú‚îÄ Min: {counts.min()}\")\n",
    "                print(f\"  ‚îú‚îÄ Max: {counts.max()}\")\n",
    "                print(f\"  ‚îú‚îÄ Mean: {counts.mean():.2f}\")\n",
    "                print(f\"  ‚îî‚îÄ Median: {counts.median():.0f}\")\n",
    "                \n",
    "                result['details'][name] = {\n",
    "                    'min': int(counts.min()),\n",
    "                    'max': int(counts.max()),\n",
    "                    'mean': float(counts.mean()),\n",
    "                    'median': float(counts.median())\n",
    "                }\n",
    "                \n",
    "                # Check for duplicates\n",
    "                if counts.max() > 1:\n",
    "                    duplicates = counts[counts > 1]\n",
    "                    print(f\"  ‚ö†Ô∏è {len(duplicates)} videos have multiple entries!\")\n",
    "                    result['status'] = 'warning'\n",
    "                    self.issues.append({\n",
    "                        'type': 'duplicate_samples',\n",
    "                        'dataset': name,\n",
    "                        'count': len(duplicates),\n",
    "                        'examples': list(duplicates.index[:5])\n",
    "                    })\n",
    "        \n",
    "        # Landmarks (multiple frames per video is normal)\n",
    "        if 'landmarks' in self.datasets:\n",
    "            counts = self.datasets['landmarks']['Video_Name'].value_counts()\n",
    "            \n",
    "            print(f\"\\nlandmarks:\")\n",
    "            print(f\"  ‚îú‚îÄ Min frames: {counts.min()}\")\n",
    "            print(f\"  ‚îú‚îÄ Max frames: {counts.max()}\")\n",
    "            print(f\"  ‚îú‚îÄ Mean frames: {counts.mean():.2f}\")\n",
    "            print(f\"  ‚îî‚îÄ Median frames: {counts.median():.0f}\")\n",
    "            \n",
    "            result['details']['landmarks'] = {\n",
    "                'min': int(counts.min()),\n",
    "                'max': int(counts.max()),\n",
    "                'mean': float(counts.mean()),\n",
    "                'median': float(counts.median())\n",
    "            }\n",
    "            \n",
    "            # Check for videos with very few frames\n",
    "            low_frame_videos = counts[counts < 10]\n",
    "            if len(low_frame_videos) > 0:\n",
    "                print(f\"  ‚ö†Ô∏è {len(low_frame_videos)} videos have < 10 frames\")\n",
    "                result['status'] = 'warning'\n",
    "                self.issues.append({\n",
    "                    'type': 'low_frame_count',\n",
    "                    'dataset': 'landmarks',\n",
    "                    'count': len(low_frame_videos),\n",
    "                    'examples': list(low_frame_videos.index[:5])\n",
    "                })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_class_distribution(self) -> Dict:\n",
    "        \"\"\"Check 3: Class distribution (TRUTH vs LIE)\"\"\"\n",
    "        print(f\"\\nüìä CHECK 3: CLASS DISTRIBUTION\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = {'status': 'pass', 'details': {}}\n",
    "        \n",
    "        for name in ['text_indonesian', 'audio_features', 'landmarks', 'multimodal_full']:\n",
    "            if name in self.datasets:\n",
    "                df = self.datasets[name]\n",
    "                \n",
    "                # Determine label column\n",
    "                label_col = 'Class' if name == 'landmarks' else 'label'\n",
    "                \n",
    "                if label_col in df.columns:\n",
    "                    class_dist = df[label_col].value_counts()\n",
    "                    total = len(df)\n",
    "                    \n",
    "                    truth_count = class_dist.get(0, 0)\n",
    "                    lie_count = class_dist.get(1, 0)\n",
    "                    truth_pct = (truth_count / total * 100) if total > 0 else 0\n",
    "                    lie_pct = (lie_count / total * 100) if total > 0 else 0\n",
    "                    \n",
    "                    print(f\"\\n{name}:\")\n",
    "                    print(f\"  ‚îú‚îÄ TRUTH (0): {truth_count:,} ({truth_pct:.1f}%)\")\n",
    "                    print(f\"  ‚îî‚îÄ LIE (1): {lie_count:,} ({lie_pct:.1f}%)\")\n",
    "                    \n",
    "                    result['details'][name] = {\n",
    "                        'truth': int(truth_count),\n",
    "                        'lie': int(lie_count),\n",
    "                        'truth_pct': float(truth_pct),\n",
    "                        'lie_pct': float(lie_pct)\n",
    "                    }\n",
    "                    \n",
    "                    # Check balance (40-60% is acceptable)\n",
    "                    if not (40 <= lie_pct <= 60):\n",
    "                        print(f\"  ‚ö†Ô∏è Imbalanced dataset!\")\n",
    "                        result['status'] = 'warning'\n",
    "                        self.issues.append({\n",
    "                            'type': 'class_imbalance',\n",
    "                            'dataset': name,\n",
    "                            'lie_percentage': float(lie_pct)\n",
    "                        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_missing_data(self) -> Dict:\n",
    "        \"\"\"Check 4: Missing values\"\"\"\n",
    "        print(f\"\\nüìä CHECK 4: MISSING DATA\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = {'status': 'pass', 'details': {}}\n",
    "        \n",
    "        for name, df in self.datasets.items():\n",
    "            missing_counts = df.isnull().sum()\n",
    "            missing_cols = missing_counts[missing_counts > 0]\n",
    "            \n",
    "            if len(missing_cols) > 0:\n",
    "                total_missing = missing_cols.sum()\n",
    "                print(f\"\\n{name}:\")\n",
    "                print(f\"  ‚îú‚îÄ Columns with missing: {len(missing_cols)}\")\n",
    "                print(f\"  ‚îî‚îÄ Total missing values: {total_missing:,}\")\n",
    "                \n",
    "                # Show top 5 columns with most missing\n",
    "                top_missing = missing_cols.nlargest(5)\n",
    "                for col, count in top_missing.items():\n",
    "                    pct = (count / len(df) * 100)\n",
    "                    print(f\"     ‚Ä¢ {col}: {count:,} ({pct:.1f}%)\")\n",
    "                \n",
    "                result['details'][name] = {\n",
    "                    'columns_with_missing': len(missing_cols),\n",
    "                    'total_missing': int(total_missing),\n",
    "                    'top_missing': {col: int(count) for col, count in top_missing.items()}\n",
    "                }\n",
    "                \n",
    "                # Critical if > 10% missing in important columns\n",
    "                critical_cols = ['filename', 'Video_Name', 'label', 'Class', 'text_indonesian_normalized', 'text_english']\n",
    "                critical_missing = missing_cols[missing_cols.index.isin(critical_cols)]\n",
    "                \n",
    "                if len(critical_missing) > 0:\n",
    "                    print(f\"  ‚ùå Critical columns have missing data!\")\n",
    "                    result['status'] = 'fail'\n",
    "                    self.issues.append({\n",
    "                        'type': 'critical_missing',\n",
    "                        'dataset': name,\n",
    "                        'columns': list(critical_missing.index)\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"\\n{name}: ‚úÖ No missing values\")\n",
    "                result['details'][name] = {'status': 'clean'}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_column_consistency(self) -> Dict:\n",
    "        \"\"\"Check 5: Column name consistency\"\"\"\n",
    "        print(f\"\\nüìä CHECK 5: COLUMN CONSISTENCY\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = {'status': 'pass', 'details': {}}\n",
    "        \n",
    "        # Expected columns for each dataset\n",
    "        expected_cols = {\n",
    "            'text_indonesian': ['filename', 'text_indonesian_original', 'text_indonesian_normalized', \n",
    "                               'label', 'dataset', 'char_count_id', 'word_count_id'],\n",
    "            'text_english': ['filename', 'text_english', 'label', 'dataset', \n",
    "                            'char_count_en', 'word_count_en'],\n",
    "            'audio_features': ['filename', 'label', 'dataset', 'mfcc1_mean', 'mfcc1_std'],\n",
    "            'pause_features': ['filename', 'label', 'dataset', 'pause_num_pauses', \n",
    "                              'pause_hesitation_score'],\n",
    "            'landmarks': ['Video_Name', 'Frame', 'Landmark_0_X', 'Landmark_0_Y', \n",
    "                         'Landmark_0_Z', 'Class']\n",
    "        }\n",
    "        \n",
    "        for name, expected in expected_cols.items():\n",
    "            if name in self.datasets:\n",
    "                df = self.datasets[name]\n",
    "                actual_cols = set(df.columns)\n",
    "                expected_set = set(expected)\n",
    "                \n",
    "                missing = expected_set - actual_cols\n",
    "                extra = actual_cols - expected_set\n",
    "                \n",
    "                print(f\"\\n{name}:\")\n",
    "                print(f\"  ‚îú‚îÄ Total columns: {len(actual_cols)}\")\n",
    "                \n",
    "                if missing:\n",
    "                    print(f\"  ‚îú‚îÄ Missing expected: {len(missing)}\")\n",
    "                    for col in list(missing)[:5]:\n",
    "                        print(f\"     ‚Ä¢ {col}\")\n",
    "                    result['status'] = 'warning'\n",
    "                    self.issues.append({\n",
    "                        'type': 'missing_columns',\n",
    "                        'dataset': name,\n",
    "                        'columns': list(missing)\n",
    "                    })\n",
    "                \n",
    "                if len(missing) == 0:\n",
    "                    print(f\"  ‚îî‚îÄ ‚úÖ All expected columns present\")\n",
    "                \n",
    "                result['details'][name] = {\n",
    "                    'total': len(actual_cols),\n",
    "                    'missing': list(missing) if missing else [],\n",
    "                    'extra_count': len(extra)\n",
    "                }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_data_types(self) -> Dict:\n",
    "        \"\"\"Check 6: Data types\"\"\"\n",
    "        print(f\"\\nüìä CHECK 6: DATA TYPES\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = {'status': 'pass', 'details': {}}\n",
    "        \n",
    "        for name, df in self.datasets.items():\n",
    "            # Check for unexpected data types\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            object_cols = df.select_dtypes(include=['object']).columns\n",
    "            \n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  ‚îú‚îÄ Numeric columns: {len(numeric_cols)}\")\n",
    "            print(f\"  ‚îî‚îÄ Text columns: {len(object_cols)}\")\n",
    "            \n",
    "            result['details'][name] = {\n",
    "                'numeric': len(numeric_cols),\n",
    "                'text': len(object_cols)\n",
    "            }\n",
    "            \n",
    "            # Check if label/Class is numeric\n",
    "            label_col = 'Class' if name == 'landmarks' else 'label'\n",
    "            if label_col in df.columns:\n",
    "                if df[label_col].dtype not in [np.int64, np.int32, np.float64]:\n",
    "                    print(f\"  ‚ö†Ô∏è {label_col} is not numeric!\")\n",
    "                    result['status'] = 'warning'\n",
    "                    self.issues.append({\n",
    "                        'type': 'wrong_dtype',\n",
    "                        'dataset': name,\n",
    "                        'column': label_col,\n",
    "                        'actual': str(df[label_col].dtype)\n",
    "                    })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_value_ranges(self) -> Dict:\n",
    "        \"\"\"Check 7: Value ranges (sanity check)\"\"\"\n",
    "        print(f\"\\nüìä CHECK 7: VALUE RANGES\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = {'status': 'pass', 'details': {}}\n",
    "        \n",
    "        # Check landmarks (should be between 0 and 1)\n",
    "        if 'landmarks' in self.datasets:\n",
    "            df = self.datasets['landmarks']\n",
    "            landmark_cols = [col for col in df.columns if col.startswith('Landmark_') or col.startswith('Pose_')]\n",
    "            \n",
    "            if landmark_cols:\n",
    "                landmark_data = df[landmark_cols]\n",
    "                \n",
    "                # Exclude zeros (missing landmarks)\n",
    "                landmark_data_nonzero = landmark_data.replace(0, np.nan)\n",
    "                \n",
    "                min_val = landmark_data_nonzero.min().min()\n",
    "                max_val = landmark_data_nonzero.max().max()\n",
    "                \n",
    "                print(f\"\\nlandmarks:\")\n",
    "                print(f\"  ‚îú‚îÄ Min value: {min_val:.4f}\")\n",
    "                print(f\"  ‚îî‚îÄ Max value: {max_val:.4f}\")\n",
    "                \n",
    "                result['details']['landmarks'] = {\n",
    "                    'min': float(min_val) if not np.isnan(min_val) else None,\n",
    "                    'max': float(max_val) if not np.isnan(max_val) else None\n",
    "                }\n",
    "                \n",
    "                # Landmarks should be normalized (0-1 range)\n",
    "                if min_val < -0.5 or max_val > 1.5:\n",
    "                    print(f\"  ‚ö†Ô∏è Values outside expected range [0, 1]!\")\n",
    "                    result['status'] = 'warning'\n",
    "                    self.issues.append({\n",
    "                        'type': 'value_range',\n",
    "                        'dataset': 'landmarks',\n",
    "                        'min': float(min_val),\n",
    "                        'max': float(max_val)\n",
    "                    })\n",
    "        \n",
    "        # Check audio features (MFCC typically -50 to 50)\n",
    "        if 'audio_features' in self.datasets:\n",
    "            df = self.datasets['audio_features']\n",
    "            mfcc_cols = [col for col in df.columns if col.startswith('mfcc')]\n",
    "            \n",
    "            if mfcc_cols:\n",
    "                mfcc_data = df[mfcc_cols]\n",
    "                \n",
    "                min_val = mfcc_data.min().min()\n",
    "                max_val = mfcc_data.max().max()\n",
    "                \n",
    "                print(f\"\\naudio_features (MFCC):\")\n",
    "                print(f\"  ‚îú‚îÄ Min value: {min_val:.2f}\")\n",
    "                print(f\"  ‚îî‚îÄ Max value: {max_val:.2f}\")\n",
    "                \n",
    "                result['details']['audio_mfcc'] = {\n",
    "                    'min': float(min_val),\n",
    "                    'max': float(max_val)\n",
    "                }\n",
    "                \n",
    "                # MFCC typically in range [-100, 100]\n",
    "                if min_val < -200 or max_val > 200:\n",
    "                    print(f\"  ‚ö†Ô∏è Unusual MFCC values detected!\")\n",
    "                    result['status'] = 'warning'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def check_landmark_quality(self) -> Dict:\n",
    "        \"\"\"Check 8: Landmark detection quality\"\"\"\n",
    "        print(f\"\\nüìä CHECK 8: LANDMARK QUALITY\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = {'status': 'pass', 'details': {}}\n",
    "        \n",
    "        if 'landmarks' not in self.datasets:\n",
    "            print(\"  ‚ö†Ô∏è Landmark dataset not found\")\n",
    "            return result\n",
    "        \n",
    "        df = self.datasets['landmarks']\n",
    "        \n",
    "        # Face landmarks (0-467)\n",
    "        face_cols = [col for col in df.columns if col.startswith('Landmark_') and \n",
    "                     int(col.split('_')[1]) < 468]\n",
    "        \n",
    "        # Iris landmarks (468-477)\n",
    "        iris_cols = [col for col in df.columns if col.startswith('Landmark_') and \n",
    "                     468 <= int(col.split('_')[1]) < 478]\n",
    "        \n",
    "        # Pose landmarks\n",
    "        pose_cols = [col for col in df.columns if col.startswith('Pose_')]\n",
    "        \n",
    "        # Calculate detection rates\n",
    "        total_frames = len(df)\n",
    "        \n",
    "        face_detected = (df[face_cols].sum(axis=1) != 0).sum()\n",
    "        iris_detected = (df[iris_cols].sum(axis=1) != 0).sum() if iris_cols else 0\n",
    "        pose_detected = (df[pose_cols].sum(axis=1) != 0).sum() if pose_cols else 0\n",
    "        \n",
    "        face_rate = (face_detected / total_frames * 100) if total_frames > 0 else 0\n",
    "        iris_rate = (iris_detected / total_frames * 100) if total_frames > 0 else 0\n",
    "        pose_rate = (pose_detected / total_frames * 100) if total_frames > 0 else 0\n",
    "        \n",
    "        print(f\"\\nDetection rates:\")\n",
    "        print(f\"  ‚îú‚îÄ Face: {face_rate:.1f}% ({face_detected:,}/{total_frames:,})\")\n",
    "        print(f\"  ‚îú‚îÄ Iris: {iris_rate:.1f}% ({iris_detected:,}/{total_frames:,})\")\n",
    "        print(f\"  ‚îî‚îÄ Pose: {pose_rate:.1f}% ({pose_detected:,}/{total_frames:,})\")\n",
    "        \n",
    "        result['details'] = {\n",
    "            'face_rate': float(face_rate),\n",
    "            'iris_rate': float(iris_rate),\n",
    "            'pose_rate': float(pose_rate),\n",
    "            'total_frames': int(total_frames)\n",
    "        }\n",
    "        \n",
    "        # Warning if detection rate is too low\n",
    "        if face_rate < 50:\n",
    "            print(f\"  ‚ö†Ô∏è Low face detection rate!\")\n",
    "            result['status'] = 'warning'\n",
    "            self.issues.append({\n",
    "                'type': 'low_detection',\n",
    "                'feature': 'face',\n",
    "                'rate': float(face_rate)\n",
    "            })\n",
    "        \n",
    "        if iris_rate < 30:\n",
    "            print(f\"  ‚ö†Ô∏è Low iris detection rate!\")\n",
    "            result['status'] = 'warning'\n",
    "            self.issues.append({\n",
    "                'type': 'low_detection',\n",
    "                'feature': 'iris',\n",
    "                'rate': float(iris_rate)\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# ==================== FIXER ====================\n",
    "class DatasetFixer:\n",
    "    \"\"\"Fix common dataset issues\"\"\"\n",
    "    \n",
    "    def __init__(self, datasets: Dict[str, pd.DataFrame], issues: List[Dict], config: ConsistencyConfig):\n",
    "        self.datasets = datasets\n",
    "        self.issues = issues\n",
    "        self.config = config\n",
    "        self.fixes_applied = []\n",
    "    \n",
    "    def fix_all(self) -> Dict:\n",
    "        \"\"\"Apply all fixes\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîß APPLYING FIXES TO {self.config.dataset_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Fix 1: Remove duplicate samples\n",
    "        self.fix_duplicates()\n",
    "        \n",
    "        # Fix 2: Fill missing critical values\n",
    "        self.fix_missing_values()\n",
    "        \n",
    "        # Fix 3: Standardize video names\n",
    "        self.fix_video_names()\n",
    "        \n",
    "        # Fix 4: Fix data types\n",
    "        self.fix_data_types()\n",
    "        \n",
    "        # Fix 5: Remove videos with insufficient data\n",
    "        self.fix_insufficient_data()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Total fixes applied: {len(self.fixes_applied)}\")\n",
    "        \n",
    "        return {\n",
    "            'fixes_applied': len(self.fixes_applied),\n",
    "            'details': self.fixes_applied\n",
    "        }\n",
    "    \n",
    "    def fix_duplicates(self):\n",
    "        \"\"\"Remove duplicate samples\"\"\"\n",
    "        print(\"\\nüîß Fix 1: Removing duplicates...\")\n",
    "        \n",
    "        for name in ['text_indonesian', 'text_english', 'audio_features', 'multimodal_full']:\n",
    "            if name in self.datasets:\n",
    "                df = self.datasets[name]\n",
    "                original_len = len(df)\n",
    "                \n",
    "                # Keep first occurrence\n",
    "                df_clean = df.drop_duplicates(subset=['filename'], keep='first')\n",
    "                \n",
    "                if len(df_clean) < original_len:\n",
    "                    removed = original_len - len(df_clean)\n",
    "                    print(f\"  ‚Ä¢ {name}: Removed {removed} duplicates\")\n",
    "                    self.datasets[name] = df_clean\n",
    "                    self.fixes_applied.append({\n",
    "                        'type': 'remove_duplicates',\n",
    "                        'dataset': name,\n",
    "                        'removed': removed\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"  ‚Ä¢ {name}: No duplicates found\")\n",
    "    \n",
    "    def fix_missing_values(self):\n",
    "        \"\"\"Fill missing critical values\"\"\"\n",
    "        print(\"\\nüîß Fix 2: Filling missing values...\")\n",
    "        \n",
    "        for name, df in self.datasets.items():\n",
    "            fixed_count = 0\n",
    "            \n",
    "            # Fill missing text with empty string\n",
    "            text_cols = [col for col in df.columns if 'text' in col.lower()]\n",
    "            for col in text_cols:\n",
    "                if df[col].isnull().any():\n",
    "                    before = df[col].isnull().sum()\n",
    "                    df[col].fillna('', inplace=True)\n",
    "                    fixed_count += before\n",
    "                    self.fixes_applied.append({\n",
    "                        'type': 'fill_missing',\n",
    "                        'dataset': name,\n",
    "                        'column': col,\n",
    "                        'count': int(before)\n",
    "                    })\n",
    "            \n",
    "            # Fill missing numeric with 0\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                if df[col].isnull().any():\n",
    "                    before = df[col].isnull().sum()\n",
    "                    df[col].fillna(0, inplace=True)\n",
    "                    fixed_count += before\n",
    "                    self.fixes_applied.append({\n",
    "                        'type': 'fill_missing',\n",
    "                        'dataset': name,\n",
    "                        'column': col,\n",
    "                        'count': int(before)\n",
    "                    })\n",
    "            \n",
    "            if fixed_count > 0:\n",
    "                print(f\"  ‚Ä¢ {name}: Filled {fixed_count} missing values\")\n",
    "            else:\n",
    "                print(f\"  ‚Ä¢ {name}: No missing values\")\n",
    "    \n",
    "    def fix_video_names(self):\n",
    "        \"\"\"Standardize video names\"\"\"\n",
    "        print(\"\\nüîß Fix 3: Standardizing video names...\")\n",
    "        \n",
    "        for name in ['text_indonesian', 'text_english', 'audio_features', 'multimodal_full']:\n",
    "            if name in self.datasets:\n",
    "                df = self.datasets[name]\n",
    "                \n",
    "                # Remove file extensions\n",
    "                if 'filename' in df.columns:\n",
    "                    original = df['filename'].copy()\n",
    "                    df['filename'] = df['filename'].apply(lambda x: x.rsplit('.', 1)[0] if '.' in str(x) else str(x))\n",
    "                    \n",
    "                    changed = (original != df['filename']).sum()\n",
    "                    if changed > 0:\n",
    "                        print(f\"  ‚Ä¢ {name}: Standardized {changed} filenames\")\n",
    "                        self.fixes_applied.append({\n",
    "                            'type': 'standardize_names',\n",
    "                            'dataset': name,\n",
    "                            'count': int(changed)\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"  ‚Ä¢ {name}: All filenames already standardized\")\n",
    "    \n",
    "    def fix_data_types(self):\n",
    "        \"\"\"Fix data types\"\"\"\n",
    "        print(\"\\nüîß Fix 4: Fixing data types...\")\n",
    "        \n",
    "        for name, df in self.datasets.items():\n",
    "            # Ensure label/Class is integer\n",
    "            label_col = 'Class' if name == 'landmarks' else 'label'\n",
    "            \n",
    "            if label_col in df.columns:\n",
    "                if df[label_col].dtype not in [np.int64, np.int32]:\n",
    "                    try:\n",
    "                        df[label_col] = df[label_col].astype(int)\n",
    "                        print(f\"  ‚Ä¢ {name}.{label_col}: Converted to integer\")\n",
    "                        self.fixes_applied.append({\n",
    "                            'type': 'fix_dtype',\n",
    "                            'dataset': name,\n",
    "                            'column': label_col\n",
    "                        })\n",
    "                    except:\n",
    "                        print(f\"  ‚ö†Ô∏è {name}.{label_col}: Could not convert to integer\")\n",
    "                else:\n",
    "                    print(f\"  ‚Ä¢ {name}.{label_col}: Already correct type\")\n",
    "    \n",
    "    def fix_insufficient_data(self):\n",
    "        \"\"\"Remove videos with insufficient data\"\"\"\n",
    "        print(\"\\nüîß Fix 5: Removing videos with insufficient data...\")\n",
    "        \n",
    "        if 'landmarks' in self.datasets:\n",
    "            df = self.datasets['landmarks']\n",
    "            \n",
    "            # Count frames per video\n",
    "            frame_counts = df['Video_Name'].value_counts()\n",
    "            \n",
    "            # Remove videos with < 5 frames\n",
    "            low_frame_videos = frame_counts[frame_counts < 5].index\n",
    "            \n",
    "            if len(low_frame_videos) > 0:\n",
    "                df_clean = df[~df['Video_Name'].isin(low_frame_videos)]\n",
    "                removed_frames = len(df) - len(df_clean)\n",
    "                \n",
    "                print(f\"  ‚Ä¢ landmarks: Removed {len(low_frame_videos)} videos ({removed_frames:,} frames)\")\n",
    "                self.datasets['landmarks'] = df_clean\n",
    "                self.fixes_applied.append({\n",
    "                    'type': 'remove_insufficient',\n",
    "                    'dataset': 'landmarks',\n",
    "                    'videos_removed': len(low_frame_videos),\n",
    "                    'frames_removed': removed_frames\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  ‚Ä¢ landmarks: All videos have sufficient frames\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ landmarks: Dataset not found\")\n",
    "    \n",
    "    def save_fixed_datasets(self):\n",
    "        \"\"\"Save fixed datasets\"\"\"\n",
    "        print(f\"\\nüíæ Saving fixed {self.config.dataset_name} datasets...\")\n",
    "        \n",
    "        saved_count = 0\n",
    "        for name, df in self.datasets.items():\n",
    "            if name in self.config.paths:\n",
    "                output_path = self.config.paths[name]\n",
    "                \n",
    "                try:\n",
    "                    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "                    print(f\"  ‚úì {name}: Saved ({len(df):,} rows)\")\n",
    "                    saved_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå {name}: Error saving - {str(e)}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Saved {saved_count} datasets\")\n",
    "\n",
    "\n",
    "# ==================== REPORT GENERATOR ====================\n",
    "class ReportGenerator:\n",
    "    \"\"\"Generate consistency report\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ConsistencyConfig, check_results: Dict, \n",
    "                 fix_results: Dict, issues: List[Dict]):\n",
    "        self.config = config\n",
    "        self.check_results = check_results\n",
    "        self.fix_results = fix_results\n",
    "        self.issues = issues\n",
    "    \n",
    "    def generate_markdown(self) -> Path:\n",
    "        \"\"\"Generate markdown report\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        content = f\"\"\"# üìä Dataset Consistency Report\n",
    "\n",
    "**Dataset:** {self.config.dataset_name}  \n",
    "**Generated:** {timestamp}  \n",
    "**Version:** 2.1\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Check | Status |\n",
    "|-------|--------|\n",
    "\"\"\"\n",
    "        \n",
    "        for check_name, result in self.check_results.items():\n",
    "            status_icon = \"‚úÖ\" if result['status'] == 'pass' else (\"‚ö†Ô∏è\" if result['status'] == 'warning' else \"‚ùå\")\n",
    "            content += f\"| {check_name.replace('_', ' ').title()} | {status_icon} {result['status'].upper()} |\\n\"\n",
    "        \n",
    "        content += f\"\\n**Total Issues Found:** {len(self.issues)}\\n\"\n",
    "        content += f\"**Fixes Applied:** {self.fix_results.get('fixes_applied', 0)}\\n\"\n",
    "        \n",
    "        # Detailed results\n",
    "        content += \"\\n---\\n\\n## üìã Detailed Results\\n\\n\"\n",
    "        \n",
    "        for check_name, result in self.check_results.items():\n",
    "            content += f\"### {check_name.replace('_', ' ').title()}\\n\\n\"\n",
    "            content += f\"**Status:** {result['status'].upper()}\\n\\n\"\n",
    "            \n",
    "            if 'details' in result and result['details']:\n",
    "                content += \"```json\\n\"\n",
    "                content += json.dumps(result['details'], indent=2)\n",
    "                content += \"\\n```\\n\\n\"\n",
    "        \n",
    "        # Issues\n",
    "        if self.issues:\n",
    "            content += \"---\\n\\n## ‚ö†Ô∏è Issues Detected\\n\\n\"\n",
    "            \n",
    "            for i, issue in enumerate(self.issues, 1):\n",
    "                content += f\"### Issue {i}: {issue['type'].replace('_', ' ').title()}\\n\\n\"\n",
    "                content += \"```json\\n\"\n",
    "                content += json.dumps(issue, indent=2)\n",
    "                content += \"\\n```\\n\\n\"\n",
    "        \n",
    "        # Fixes\n",
    "        if self.fix_results.get('details'):\n",
    "            content += \"---\\n\\n## üîß Fixes Applied\\n\\n\"\n",
    "            \n",
    "            for i, fix in enumerate(self.fix_results['details'], 1):\n",
    "                content += f\"{i}. **{fix['type'].replace('_', ' ').title()}**\\n\"\n",
    "                content += f\"   - Dataset: `{fix.get('dataset', 'N/A')}`\\n\"\n",
    "                \n",
    "                if 'count' in fix:\n",
    "                    content += f\"   - Count: {fix['count']}\\n\"\n",
    "                if 'removed' in fix:\n",
    "                    content += f\"   - Removed: {fix['removed']}\\n\"\n",
    "                \n",
    "                content += \"\\n\"\n",
    "        \n",
    "        content += \"---\\n\\n*Generated by Dataset Consistency Checker v2.1*\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        report_path = self.config.paths['validation'] / f'consistency_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.md'\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return report_path\n",
    "    \n",
    "    def generate_json(self) -> Path:\n",
    "        \"\"\"Generate JSON report\"\"\"\n",
    "        report_data = {\n",
    "            'dataset': self.config.dataset_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'check_results': self.check_results,\n",
    "            'fix_results': self.fix_results,\n",
    "            'issues': self.issues\n",
    "        }\n",
    "        \n",
    "        # Convert numpy types\n",
    "        report_data = self._convert_numpy(report_data)\n",
    "        \n",
    "        report_path = self.config.paths['validation'] / f'consistency_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report_data, f, indent=2)\n",
    "        \n",
    "        return report_path\n",
    "    \n",
    "    def _convert_numpy(self, obj):\n",
    "        \"\"\"Convert numpy types to Python native\"\"\"\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_numpy(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_numpy(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "\n",
    "# ==================== DUAL PROCESSOR ====================\n",
    "class DualDatasetProcessor:\n",
    "    \"\"\"Process both I3D and RLT datasets\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.datasets = ['I3D', 'RLT']\n",
    "        self.results = {}\n",
    "    \n",
    "    def process_all(self):\n",
    "        \"\"\"Process both datasets\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"üîç DUAL DATASET CONSISTENCY CHECKER & FIXER\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Processing both I3D and RLT datasets automatically\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for dataset_name in self.datasets:\n",
    "            print(f\"\\n\\n{'#'*70}\")\n",
    "            print(f\"# PROCESSING: {dataset_name}\")\n",
    "            print(f\"{'#'*70}\")\n",
    "            \n",
    "            try:\n",
    "                result = self.process_single_dataset(dataset_name)\n",
    "                self.results[dataset_name] = result\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error processing {dataset_name}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                self.results[dataset_name] = {'status': 'error', 'message': str(e)}\n",
    "        \n",
    "        # Final summary\n",
    "        self.print_final_summary()\n",
    "    \n",
    "    def process_single_dataset(self, dataset_name: str) -> Dict:\n",
    "        \"\"\"Process single dataset\"\"\"\n",
    "        # Initialize\n",
    "        config = ConsistencyConfig(dataset_name)\n",
    "        \n",
    "        # Load datasets\n",
    "        loader = DatasetLoader(config)\n",
    "        datasets = loader.load_all()\n",
    "        \n",
    "        if not datasets:\n",
    "            print(f\"\\n‚ö†Ô∏è No datasets found for {dataset_name}\")\n",
    "            return {'status': 'no_data'}\n",
    "        \n",
    "        # Check consistency\n",
    "        checker = ConsistencyChecker(datasets, config)\n",
    "        check_results = checker.check_all()\n",
    "        \n",
    "        # Apply fixes\n",
    "        fixer = DatasetFixer(datasets, checker.issues, config)\n",
    "        fix_results = fixer.fix_all()\n",
    "        \n",
    "        # Save fixed datasets\n",
    "        fixer.save_fixed_datasets()\n",
    "        \n",
    "        # Generate reports\n",
    "        print(f\"\\nüìù Generating {dataset_name} reports...\")\n",
    "        reporter = ReportGenerator(config, check_results, fix_results, checker.issues)\n",
    "        \n",
    "        md_path = reporter.generate_markdown()\n",
    "        json_path = reporter.generate_json()\n",
    "        \n",
    "        print(f\"  ‚úì Markdown: {md_path.name}\")\n",
    "        print(f\"  ‚úì JSON: {json_path.name}\")\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'issues_found': len(checker.issues),\n",
    "            'fixes_applied': fix_results.get('fixes_applied', 0),\n",
    "            'md_report': str(md_path),\n",
    "            'json_report': str(json_path)\n",
    "        }\n",
    "    \n",
    "    def print_final_summary(self):\n",
    "        \"\"\"Print final summary for both datasets\"\"\"\n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"üìä FINAL SUMMARY - BOTH DATASETS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for dataset_name, result in self.results.items():\n",
    "            print(f\"\\n{dataset_name}:\")\n",
    "            \n",
    "            if result.get('status') == 'success':\n",
    "                print(f\"  ‚úÖ Status: SUCCESS\")\n",
    "                print(f\"  ‚îú‚îÄ Issues found: {result.get('issues_found', 0)}\")\n",
    "                print(f\"  ‚îú‚îÄ Fixes applied: {result.get('fixes_applied', 0)}\")\n",
    "                print(f\"  ‚îú‚îÄ MD report: {Path(result.get('md_report', '')).name}\")\n",
    "                print(f\"  ‚îî‚îÄ JSON report: {Path(result.get('json_report', '')).name}\")\n",
    "            elif result.get('status') == 'no_data':\n",
    "                print(f\"  ‚ö†Ô∏è Status: NO DATA FOUND\")\n",
    "            elif result.get('status') == 'error':\n",
    "                print(f\"  ‚ùå Status: ERROR\")\n",
    "                print(f\"  ‚îî‚îÄ Message: {result.get('message', 'Unknown error')}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ DUAL DATASET PROCESSING COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    processor = DualDatasetProcessor()\n",
    "    processor.process_all()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bisa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
