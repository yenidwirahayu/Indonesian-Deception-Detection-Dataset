{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0120e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I3D Dataset Processor v8.0 - Scientific Data Ready\n",
    "===================================================\n",
    "Production-ready processor. Deterministic for cached files only.\n",
    "\n",
    "Key Features:\n",
    "- Fixed audio features: 94 (not 108)\n",
    "- Deterministic for cached files (transcript + translation frozen)\n",
    "- Non-deterministic for new files (external APIs: Google STT + Translate)\n",
    "- Translation cache (MD5-based)\n",
    "- Fixed audio enhancement parameters\n",
    "- Per-file decision logging\n",
    "- Data dictionary generation\n",
    "- Run manifest for reproducibility\n",
    "- Explicit librosa parameters\n",
    "- MediaPipe: 478 face (468+10 iris) + 33 pose landmarks (validated at runtime)\n",
    "\n",
    "Author: [Your Name]\n",
    "Version: 8.0\n",
    "Date: 2025-01-30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import yaml\n",
    "import subprocess\n",
    "import pickle\n",
    "import hashlib\n",
    "import platform\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Audio processing\n",
    "import wave\n",
    "import librosa\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "\n",
    "# Translation\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "# Suppress only specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='librosa')\n",
    "\n",
    "# Download NLTK resources (should be pre-installed)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    logging.error(\"âŒ NLTK 'punkt' tokenizer not found!\")\n",
    "    logging.error(\"   Please install: python -m nltk.downloader punkt\")\n",
    "    logging.error(\"   Or run in Python: import nltk; nltk.download('punkt')\")\n",
    "    raise RuntimeError(\n",
    "        \"NLTK 'punkt' not found. Install with: python -m nltk.downloader punkt\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Version info\n",
    "VERSION = \"8.0\"\n",
    "PIPELINE_NAME = \"I3D-Processor-ScientificData\"\n",
    "\n",
    "\n",
    "# ==================== MANUAL TRANSCRIPTS ====================\n",
    "MANUAL_TRANSCRIPTS = {\n",
    "    'LIE_Bataknese_07_Female_G_I_A_1.mov': 'tiga',\n",
    "    'LIE_Bataknese_11_Male_NG_S_A_1.mov': 'dua',\n",
    "    'LIE_Bataknese_18_Male_NG_S_A_2.mov': 'kalo saya pikir ini ada dua pelaku ini pak, ada satu yang mengahantar, tapi dak kelihatan mungkin yang mengantar itu',\n",
    "    'LIE_Bataknese_19_Male_NG_C_A_2.mov': 'rame',\n",
    "    'LIE_Bataknese_24_Female_G_I_B_1.mov': 'dua',\n",
    "    'LIE_Bataknese_27_Male_NG_C_B_1.mov': 'dua',\n",
    "    'LIE_Bataknese_27_Male_NG_C_B_2.mov': 'ramai',\n",
    "    'LIE_BugisMakassarese_07_Male_G_C_A_1.mov': 'tiga',\n",
    "    'LIE_BugisMakassarese_07_Male_G_C_A_2.mov': 'ee sepi ndak ado orang',\n",
    "    'LIE_BugisMakassarese_09_Female_G_D_A_1.mov': 'dua',\n",
    "    'LIE_BugisMakassarese_10_Female_G_I_A_2.mov': 'em rame',\n",
    "    'LIE_BugisMakassarese_21_Male_G_D_A_1.mov': 'dua',\n",
    "    'LIE_BugisMakassarese_22_Female_NG_S_A_2.mov': 'ramai',\n",
    "    'LIE_BugisMakassarese_26_Female_G_C_B_1.mov': 'tiga',\n",
    "    'LIE_BugisMakassarese_28_Male_NG_C_B_2.mov': 'eeh ramai',\n",
    "    'LIE_Javanese_02_Female_NG_S_A_2.mov': 'sepi',\n",
    "    'LIE_Javanese_14_Female_NG_C_B_1.mov': 'dua',\n",
    "    'LIE_Javanese_20_Female_G_D_B_1.mov': 'lima',\n",
    "    'LIE_Javanese_20_Female_G_D_B_2.mov': 'rame',\n",
    "    'LIE_Javanese_26_Female_G_C_A_1.mov': 'tiga',\n",
    "    'LIE_Madurese_04_Male_NG_C_A_2.mov': 'hmm',\n",
    "    'LIE_Madurese_05_Female_G_D_B_1.mov': 'dua',\n",
    "    'LIE_Madurese_20_Male_G_I_B_1.mov': 'dua',\n",
    "    'LIE_Madurese_20_Male_G_I_B_2.mov': 'ramai',\n",
    "    'LIE_Madurese_28_Female_G_I_A_2.mov': 'sepi',\n",
    "    'LIE_Madurese_43_Female_G_D_B_1.mov': 'dua',\n",
    "    'LIE_Sundanese_16_Female_NG_C_A_4.mov': 'pada malam hari ada dua orang pelaku satu permpuan dan satu laki laki memasuki rumah dan mengambil sebuah gas satu',\n",
    "    'TRUTH_BugisMakassarese_21_Male_G_D_A_2.mov': 'sepi',\n",
    "    'TRUTH_BugisMakassarese_23_Female_NG_S_A_2.mov': 'sepi',\n",
    "    'TRUTH_Javanese_02_Female_NG_S_A_4.mov': 'dua',\n",
    "    'TRUTH_Javanese_06_Female_G_D_A_4.mov': 'dua',\n",
    "    'TRUTH_Javanese_14_Female_NG_C_B_4.mov': 'dua',\n",
    "    'TRUTH_Javanese_20_Female_G_D_B_4.mov': 'dua',\n",
    "    'TRUTH_Sundanese_12_Female_G_C_A_2.mov': 'suasana tempat kejadian itu saat itu, rumahnya kosong. Jadi memang hanya ada satu pelaku itu saja'\n",
    "}\n",
    "\n",
    "\n",
    "# ==================== ENVIRONMENT CAPTURE ====================\n",
    "def capture_environment() -> Dict:\n",
    "    \"\"\"Capture complete environment for reproducibility\"\"\"\n",
    "    import pkg_resources\n",
    "    \n",
    "    env_info = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'pipeline_version': VERSION,\n",
    "        'pipeline_name': PIPELINE_NAME,\n",
    "        'python_version': sys.version,\n",
    "        'platform': {\n",
    "            'system': platform.system(),\n",
    "            'release': platform.release(),\n",
    "            'version': platform.version(),\n",
    "            'machine': platform.machine(),\n",
    "            'processor': platform.processor()\n",
    "        },\n",
    "        'packages': {}\n",
    "    }\n",
    "    \n",
    "    # Key packages\n",
    "    key_packages = [\n",
    "        'numpy', 'pandas', 'librosa', 'opencv-python', 'mediapipe',\n",
    "        'SpeechRecognition', 'pydub', 'deep-translator', 'nltk',\n",
    "        'matplotlib', 'seaborn', 'tqdm', 'pyyaml'\n",
    "    ]\n",
    "    \n",
    "    for pkg in key_packages:\n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(pkg).version\n",
    "            env_info['packages'][pkg] = version\n",
    "        except:\n",
    "            env_info['packages'][pkg] = 'not_found'\n",
    "    \n",
    "    # FFmpeg version\n",
    "    try:\n",
    "        result = subprocess.run(['ffmpeg', '-version'], \n",
    "                              capture_output=True, text=True, timeout=5)\n",
    "        ffmpeg_version = result.stdout.split('\\n')[0]\n",
    "        env_info['ffmpeg_version'] = ffmpeg_version\n",
    "    except:\n",
    "        env_info['ffmpeg_version'] = 'not_found'\n",
    "    \n",
    "    return env_info\n",
    "\n",
    "\n",
    "# ==================== TRANSCRIPT CACHE MANAGER ====================\n",
    "class TranscriptCacheManager:\n",
    "    \"\"\"Manage frozen transcripts for deterministic results\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Path):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.transcripts_dir = self.cache_dir / 'transcripts'\n",
    "        self.transcripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_transcript_path(self, filename: str) -> Path:\n",
    "        \"\"\"Get path to transcript JSON file\"\"\"\n",
    "        video_stem = Path(filename).stem\n",
    "        return self.transcripts_dir / f\"{video_stem}.json\"\n",
    "    \n",
    "    def load_transcript(self, filename: str) -> Optional[Dict]:\n",
    "        \"\"\"Load cached transcript\"\"\"\n",
    "        transcript_path = self.get_transcript_path(filename)\n",
    "        \n",
    "        if not transcript_path.exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load transcript cache for {filename}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def save_transcript(self, filename: str, text: str, source: str, \n",
    "                       attempt: int = 0, mode: str = 'normal', \n",
    "                       audio_dbfs: float = 0.0, \n",
    "                       stt_energy_threshold: float = 0.0,\n",
    "                       enhancement_params: Dict = None):\n",
    "        \"\"\"Save transcript to cache\"\"\"\n",
    "        transcript_path = self.get_transcript_path(filename)\n",
    "        \n",
    "        data = {\n",
    "            'filename': filename,\n",
    "            'text': text,\n",
    "            'source': source,  # 'manual', 'stt', 'stt_ultra'\n",
    "            'attempt': attempt,\n",
    "            'mode': mode,  # 'normal', 'ultra'\n",
    "            'audio_dbfs': audio_dbfs,\n",
    "            'stt_energy_threshold': stt_energy_threshold,\n",
    "            'enhancement_params': enhancement_params or {},\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'version': VERSION\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save transcript cache for {filename}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def exists(self, filename: str) -> bool:\n",
    "        \"\"\"Check if transcript cache exists\"\"\"\n",
    "        return self.get_transcript_path(filename).exists()\n",
    "\n",
    "\n",
    "# ==================== TRANSLATION CACHE MANAGER ====================\n",
    "class TranslationCacheManager:\n",
    "    \"\"\"Manage translation cache for deterministic results\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Path):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_file = self.cache_dir / 'translation_cache.json'\n",
    "        self.cache = self._load_cache()\n",
    "    \n",
    "    def _load_cache(self) -> Dict:\n",
    "        \"\"\"Load translation cache from file\"\"\"\n",
    "        if not self.cache_file.exists():\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load translation cache: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save translation cache to file\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save translation cache: {str(e)}\")\n",
    "    \n",
    "    def _get_hash(self, text: str) -> str:\n",
    "        \"\"\"Get hash of text for cache key\"\"\"\n",
    "        return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    def get_translation(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Get cached translation\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        text_hash = self._get_hash(text)\n",
    "        cached = self.cache.get(text_hash)\n",
    "        \n",
    "        # âœ… Handle both old format (string) and new format (dict)\n",
    "        if cached:\n",
    "            if isinstance(cached, dict):\n",
    "                return cached['translation']\n",
    "            else:\n",
    "                return cached  # Old format compatibility\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_translation(self, text: str, translation: str):\n",
    "        \"\"\"Save translation to cache with metadata\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return\n",
    "        \n",
    "        text_hash = self._get_hash(text)\n",
    "        \n",
    "        # âœ… NEW: Save with metadata\n",
    "        self.cache[text_hash] = {\n",
    "            'translation': translation,\n",
    "            'source_lang': 'id',\n",
    "            'target_lang': 'en',\n",
    "            'translator': 'deep-translator (GoogleTranslator)',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'text_length': len(text),\n",
    "            'pipeline_version': VERSION\n",
    "        }\n",
    "        self._save_cache()\n",
    "    \n",
    "    def get_cache_size(self) -> int:\n",
    "        \"\"\"Get number of cached translations\"\"\"\n",
    "        return len(self.cache)\n",
    "\n",
    "\n",
    "# ==================== DECISION LOG MANAGER ====================\n",
    "class DecisionLogManager:\n",
    "    \"\"\"Log processing decisions for reproducibility\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir: Path):\n",
    "        self.log_dir = log_dir\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.decisions_file = self.log_dir / 'decision_log.jsonl'\n",
    "    \n",
    "    def log_decision(self, filename: str, decision_type: str, details: Dict):\n",
    "        \"\"\"Log a processing decision\"\"\"\n",
    "        entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'filename': filename,\n",
    "            'decision_type': decision_type,\n",
    "            'details': details\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(self.decisions_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to log decision: {str(e)}\")\n",
    "    \n",
    "    def log_reextraction(self, filename: str, reason: str, audio_dbfs: float, \n",
    "                        params: Dict, success: bool):\n",
    "        \"\"\"Log re-extraction decision\"\"\"\n",
    "        self.log_decision(filename, 'reextraction', {\n",
    "            'reason': reason,\n",
    "            'audio_dbfs': audio_dbfs,\n",
    "            'enhancement_params': params,\n",
    "            'success': success\n",
    "        })\n",
    "    \n",
    "    def log_transcript_source(self, filename: str, source: str, from_cache: bool = False):\n",
    "        \"\"\"Log transcript source\"\"\"\n",
    "        self.log_decision(filename, 'transcript_source', {\n",
    "            'source': source,\n",
    "            'from_cache': from_cache\n",
    "        })\n",
    "    \n",
    "    def log_translation(self, filename: str, from_cache: bool, text_length: int):\n",
    "        \"\"\"Log translation decision\"\"\"\n",
    "        self.log_decision(filename, 'translation', {\n",
    "            'from_cache': from_cache,\n",
    "            'text_length': text_length\n",
    "        })\n",
    "\n",
    "\n",
    "# ==================== CHECKPOINT MANAGER ====================\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manage checkpoints for resume functionality\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: Path):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.checkpoint_file = self.checkpoint_dir / 'processing_checkpoint.pkl'\n",
    "        self.progress_file = self.checkpoint_dir / 'progress.json'\n",
    "    \n",
    "    def save_checkpoint(self, data: Dict):\n",
    "        \"\"\"Save checkpoint data\"\"\"\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "            # Save human-readable progress\n",
    "            progress = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'processed_files': data.get('processed_files', []),\n",
    "                'total_processed': len(data.get('processed_files', [])),\n",
    "                'stats': data.get('stats', {})\n",
    "            }\n",
    "            with open(self.progress_file, 'w') as f:\n",
    "                json.dump(progress, f, indent=2)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save checkpoint: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def load_checkpoint(self) -> Optional[Dict]:\n",
    "        \"\"\"Load checkpoint data\"\"\"\n",
    "        if not self.checkpoint_file.exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            logging.info(f\"âœ“ Checkpoint loaded: {len(data.get('processed_files', []))} files already processed\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load checkpoint: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def clear_checkpoint(self):\n",
    "        \"\"\"Clear checkpoint files\"\"\"\n",
    "        try:\n",
    "            if self.checkpoint_file.exists():\n",
    "                self.checkpoint_file.unlink()\n",
    "            if self.progress_file.exists():\n",
    "                self.progress_file.unlink()\n",
    "            logging.info(\"âœ“ Checkpoint cleared\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to clear checkpoint: {str(e)}\")\n",
    "    \n",
    "    def checkpoint_exists(self) -> bool:\n",
    "        \"\"\"Check if checkpoint exists\"\"\"\n",
    "        return self.checkpoint_file.exists()\n",
    "\n",
    "\n",
    "# ==================== INDONESIAN NUMBER CONVERTER ====================\n",
    "class IndonesianNumberConverter:\n",
    "    \"\"\"Convert numbers to Indonesian words (0 - Trillion)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ones = ['', 'satu', 'dua', 'tiga', 'empat', 'lima', 'enam', 'tujuh', 'delapan', 'sembilan']\n",
    "        self.teens = ['sepuluh', 'sebelas', 'dua belas', 'tiga belas', 'empat belas', \n",
    "                      'lima belas', 'enam belas', 'tujuh belas', 'delapan belas', 'sembilan belas']\n",
    "        self.tens = ['', '', 'dua puluh', 'tiga puluh', 'empat puluh', 'lima puluh', \n",
    "                     'enam puluh', 'tujuh puluh', 'delapan puluh', 'sembilan puluh']\n",
    "    \n",
    "    def convert_below_hundred(self, num):\n",
    "        if num == 0:\n",
    "            return 'nol'\n",
    "        elif num < 10:\n",
    "            return self.ones[num]\n",
    "        elif num < 20:\n",
    "            return self.teens[num - 10]\n",
    "        else:\n",
    "            tens_digit = num // 10\n",
    "            ones_digit = num % 10\n",
    "            if ones_digit == 0:\n",
    "                return self.tens[tens_digit]\n",
    "            else:\n",
    "                return f\"{self.tens[tens_digit]} {self.ones[ones_digit]}\"\n",
    "    \n",
    "    def convert_below_thousand(self, num):\n",
    "        if num < 100:\n",
    "            return self.convert_below_hundred(num)\n",
    "        \n",
    "        hundreds_digit = num // 100\n",
    "        remainder = num % 100\n",
    "        \n",
    "        if hundreds_digit == 1:\n",
    "            result = 'seratus'\n",
    "        else:\n",
    "            result = f\"{self.ones[hundreds_digit]} ratus\"\n",
    "        \n",
    "        if remainder > 0:\n",
    "            result += f\" {self.convert_below_hundred(remainder)}\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def convert_number(self, num):\n",
    "        if num == 0:\n",
    "            return 'nol'\n",
    "        \n",
    "        if num < 0:\n",
    "            return f\"minus {self.convert_number(abs(num))}\"\n",
    "        \n",
    "        if num >= 1000000000000:\n",
    "            trillions = num // 1000000000000\n",
    "            remainder = num % 1000000000000\n",
    "            result = 'satu triliun' if trillions == 1 else f\"{self.convert_below_thousand(trillions)} triliun\"\n",
    "            if remainder > 0:\n",
    "                result += f\" {self.convert_number(remainder)}\"\n",
    "            return result\n",
    "        \n",
    "        if num >= 1000000000:\n",
    "            billions = num // 1000000000\n",
    "            remainder = num % 1000000000\n",
    "            result = 'satu miliar' if billions == 1 else f\"{self.convert_below_thousand(billions)} miliar\"\n",
    "            if remainder > 0:\n",
    "                result += f\" {self.convert_number(remainder)}\"\n",
    "            return result\n",
    "        \n",
    "        if num >= 1000000:\n",
    "            millions = num // 1000000\n",
    "            remainder = num % 1000000\n",
    "            result = 'satu juta' if millions == 1 else f\"{self.convert_below_thousand(millions)} juta\"\n",
    "            if remainder > 0:\n",
    "                result += f\" {self.convert_number(remainder)}\"\n",
    "            return result\n",
    "        \n",
    "        if num >= 1000:\n",
    "            thousands = num // 1000\n",
    "            remainder = num % 1000\n",
    "            result = 'seribu' if thousands == 1 else f\"{self.convert_below_thousand(thousands)} ribu\"\n",
    "            if remainder > 0:\n",
    "                result += f\" {self.convert_below_thousand(remainder)}\"\n",
    "            return result\n",
    "        \n",
    "        return self.convert_below_thousand(num)\n",
    "    \n",
    "    def convert_decimal(self, num_str):\n",
    "        parts = num_str.split('.')\n",
    "        \n",
    "        if len(parts) == 1:\n",
    "            return self.convert_number(int(parts[0]))\n",
    "        \n",
    "        integer_part = int(parts[0]) if parts[0] else 0\n",
    "        decimal_part = parts[1] if len(parts) > 1 and parts[1] else ''\n",
    "        \n",
    "        result = self.convert_number(integer_part)\n",
    "        \n",
    "        if decimal_part:\n",
    "            result += ' koma'\n",
    "            for digit in decimal_part:\n",
    "                if digit.isdigit():\n",
    "                    result += f\" {self.ones[int(digit)]}\"\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def normalize_numbers_in_text(text):\n",
    "    \"\"\"Normalize all numbers in text to Indonesian words\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return text\n",
    "    \n",
    "    converter = IndonesianNumberConverter()\n",
    "    number_pattern = r'\\b\\d+(?:[.,]\\d+)*\\b'\n",
    "    \n",
    "    def replace_number(match):\n",
    "        num_str = match.group(0)\n",
    "        num_str_clean = num_str.replace('.', '').replace(',', '.')\n",
    "        \n",
    "        try:\n",
    "            if '.' in num_str_clean:\n",
    "                return converter.convert_decimal(num_str_clean)\n",
    "            else:\n",
    "                num = int(num_str_clean)\n",
    "                return converter.convert_number(num)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to convert number '{num_str}': {str(e)}\")\n",
    "            return num_str\n",
    "    \n",
    "    normalized_text = re.sub(number_pattern, replace_number, text)\n",
    "    normalized_text = re.sub(r'\\s+', ' ', normalized_text).strip()\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def extract_number_features(text):\n",
    "    \"\"\"Extract deception-specific number features\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return {\n",
    "            'has_numbers': False, 'number_count': 0, 'has_vague_quantifiers': False,\n",
    "            'has_exact_quantifiers': False, 'number_word_ratio': 0.0,\n",
    "            'has_large_numbers': False, 'has_decimal_numbers': False\n",
    "        }\n",
    "    \n",
    "    digit_pattern = r'\\b\\d+(?:[.,]\\d+)*\\b'\n",
    "    numbers = re.findall(digit_pattern, text)\n",
    "    \n",
    "    number_words = r'\\b(nol|satu|dua|tiga|empat|lima|enam|tujuh|delapan|sembilan|sepuluh|sebelas|belas|puluh|ratus|ribu|juta|miliar|triliun)\\b'\n",
    "    number_word_matches = re.findall(number_words, text.lower())\n",
    "    \n",
    "    vague_pattern = r'\\b(sekitar|kira-kira|kurang lebih|hampir|lebih dari|kurang dari|lebih kurang|sekitaran|kisaran)\\b'\n",
    "    has_vague = bool(re.search(vague_pattern, text.lower()))\n",
    "    \n",
    "    exact_pattern = r'\\b(tepat|persis|tepatnya|pastinya|pasti|exactly)\\b'\n",
    "    has_exact = bool(re.search(exact_pattern, text.lower()))\n",
    "    \n",
    "    large_numbers = []\n",
    "    for n in numbers:\n",
    "        try:\n",
    "            val = float(n.replace('.', '').replace(',', '.'))\n",
    "            if val > 1000:\n",
    "                large_numbers.append(n)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    decimal_numbers = [n for n in numbers if '.' in n or ',' in n]\n",
    "    \n",
    "    total_words = len(text.split())\n",
    "    number_word_ratio = len(number_word_matches) / total_words if total_words > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'has_numbers': len(numbers) > 0 or len(number_word_matches) > 0,\n",
    "        'number_count': len(numbers) + len(number_word_matches),\n",
    "        'has_vague_quantifiers': has_vague,\n",
    "        'has_exact_quantifiers': has_exact,\n",
    "        'number_word_ratio': number_word_ratio,\n",
    "        'has_large_numbers': len(large_numbers) > 0,\n",
    "        'has_decimal_numbers': len(decimal_numbers) > 0\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "class I3DConfig:\n",
    "    \"\"\"Configuration for I3D Dataset - Scientific Data Ready\"\"\"\n",
    "    \n",
    "    DEFAULT_CONFIG = {\n",
    "        'dataset': {\n",
    "            'name': 'I3D',\n",
    "            'version': '1.0',\n",
    "            'language': 'Indonesian',\n",
    "            'description': 'Indonesian Interrogation Deception Dataset',\n",
    "            'citation': 'Please cite: [Your Paper]'\n",
    "        },\n",
    "        'audio': {\n",
    "            'sampling_rate': 16000,\n",
    "            'librosa_params': {\n",
    "                'n_fft': 2048,\n",
    "                'hop_length': 512,\n",
    "                'win_length': 2048,\n",
    "                'window': 'hann',\n",
    "                'center': True,\n",
    "                'pad_mode': 'constant'\n",
    "            },\n",
    "            'enhancement': {\n",
    "                'extreme_boost_threshold': -55,\n",
    "                'max_boost': 40,\n",
    "                'target_dbfs': -18,\n",
    "                'silence_threshold': -50,\n",
    "                'min_pause_duration': 200,\n",
    "                'compression_ratio': 6.0,\n",
    "                'short_speech_threshold': 2000,\n",
    "                'fixed_params': True\n",
    "            }\n",
    "        },\n",
    "        'video': {\n",
    "            'sample_rate': 1,\n",
    "            'zoom_levels': [1.5, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0, 8.0, 10.0],\n",
    "            'preprocessing': {\n",
    "                'clahe_clip_limit': 4.0,\n",
    "                'clahe_tile_size': (4, 4),\n",
    "                'gamma_correction': 1.4,\n",
    "                'unsharp_strength': 1.8\n",
    "            }\n",
    "        },\n",
    "        'mediapipe': {\n",
    "            'face_mesh': {\n",
    "                'static_image_mode': False,\n",
    "                'max_num_faces': 1,\n",
    "                'refine_landmarks': True,\n",
    "                'min_detection_confidence': 0.2,\n",
    "                'min_tracking_confidence': 0.2\n",
    "            },\n",
    "            'pose': {\n",
    "                'static_image_mode': False,\n",
    "                'model_complexity': 1,\n",
    "                'min_detection_confidence': 0.3,\n",
    "                'min_tracking_confidence': 0.3\n",
    "            }\n",
    "        },\n",
    "        'speech_recognition': {\n",
    "            'language': 'id-ID',\n",
    "            'max_attempts': 7,\n",
    "            'deterministic_mode': True,  # âœ… CHANGED: True by default for reproducibility\n",
    "            'configs': [\n",
    "                {'energy_threshold': 300, 'pause_threshold': 0.8, 'phrase_threshold': 0.3},\n",
    "                {'energy_threshold': 150, 'pause_threshold': 0.6, 'phrase_threshold': 0.2},\n",
    "                {'energy_threshold': 100, 'pause_threshold': 0.5, 'phrase_threshold': 0.1},\n",
    "                {'energy_threshold': 50, 'pause_threshold': 0.4, 'phrase_threshold': 0.1},\n",
    "                {'energy_threshold': 30, 'pause_threshold': 0.3, 'phrase_threshold': 0.05},\n",
    "                {'energy_threshold': 20, 'pause_threshold': 0.2, 'phrase_threshold': 0.05},\n",
    "                {'energy_threshold': 10, 'pause_threshold': 0.1, 'phrase_threshold': 0.01}\n",
    "            ]\n",
    "        },\n",
    "        'text': {\n",
    "            'normalize_numbers': True,\n",
    "            'extract_number_features': True,\n",
    "            'translate_to_english': True,\n",
    "            'use_manual_transcripts': True,\n",
    "            'use_transcript_cache': True,\n",
    "            'use_translation_cache': True,\n",
    "            'force_retranscribe': False,\n",
    "            'sentiment_analysis_note': 'TextBlob sentiment for Indonesian is proxy only'\n",
    "        },\n",
    "        'reextraction': {\n",
    "            'enabled': True,\n",
    "            'auto_trigger': True,\n",
    "            'log_decisions': True\n",
    "        },\n",
    "        'checkpoint': {\n",
    "            'enabled': True,\n",
    "            'save_interval': 5\n",
    "        },\n",
    "        'output': {\n",
    "            'generate_figures': True,\n",
    "            'generate_markdown_report': True,\n",
    "            'generate_data_dictionary': True,\n",
    "            'generate_run_manifest': True,\n",
    "            'figure_dpi': 300,\n",
    "            'csv_encoding': 'utf-8'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        self.config = self._deep_copy(self.DEFAULT_CONFIG)\n",
    "        \n",
    "        if config_path and os.path.exists(config_path):\n",
    "            self.load_from_file(config_path)\n",
    "    \n",
    "    def _deep_copy(self, d):\n",
    "        \"\"\"Deep copy dictionary\"\"\"\n",
    "        import copy\n",
    "        return copy.deepcopy(d)\n",
    "    \n",
    "    def load_from_file(self, config_path: str):\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                user_config = yaml.safe_load(f)\n",
    "                self._merge_config(self.config, user_config)\n",
    "            logging.info(f\"âœ“ Configuration loaded from: {config_path}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load config: {str(e)}. Using defaults.\")\n",
    "    \n",
    "    def _merge_config(self, base: dict, update: dict):\n",
    "        for key, value in update.items():\n",
    "            if key in base and isinstance(base[key], dict) and isinstance(value, dict):\n",
    "                self._merge_config(base[key], value)\n",
    "            else:\n",
    "                base[key] = value\n",
    "    \n",
    "    def save_to_file(self, config_path: str):\n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(self.config, f, default_flow_style=False)\n",
    "        logging.info(f\"âœ“ Configuration saved to: {config_path}\")\n",
    "    \n",
    "    def get(self, key_path: str, default=None):\n",
    "        keys = key_path.split('.')\n",
    "        value = self.config\n",
    "        for key in keys:\n",
    "            if isinstance(value, dict) and key in value:\n",
    "                value = value[key]\n",
    "            else:\n",
    "                return default\n",
    "        return value\n",
    "\n",
    "\n",
    "# ==================== PATH MANAGER ====================\n",
    "class I3DPathManager:\n",
    "    \"\"\"Path management for I3D Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.dataset_name = 'I3D'\n",
    "        \n",
    "        dataset_base = self.base_dir / \"dataset\"\n",
    "        \n",
    "        self.paths = {\n",
    "            # Source (INPUT)\n",
    "            'source_raw': dataset_base / \"raw\" / \"I3D\",\n",
    "            'source_lie': dataset_base / \"raw\" / \"I3D\" / \"lie\",\n",
    "            'source_truth': dataset_base / \"raw\" / \"I3D\" / \"truth\",\n",
    "            \n",
    "            # Processed (OUTPUT)\n",
    "            'processed': dataset_base / \"processed\" / \"I3D\",\n",
    "            'audio': dataset_base / \"processed\" / \"I3D\" / \"audio\",\n",
    "            'audio_wav': dataset_base / \"processed\" / \"I3D\" / \"audio\" / \"wav\",\n",
    "            'audio_wav_lie': dataset_base / \"processed\" / \"I3D\" / \"audio\" / \"wav\" / \"lie\",\n",
    "            'audio_wav_truth': dataset_base / \"processed\" / \"I3D\" / \"audio\" / \"wav\" / \"truth\",\n",
    "            'audio_enhanced': dataset_base / \"processed\" / \"I3D\" / \"audio\" / \"enhanced\",\n",
    "            'audio_enhanced_lie': dataset_base / \"processed\" / \"I3D\" / \"audio\" / \"enhanced\" / \"lie\",\n",
    "            'audio_enhanced_truth': dataset_base / \"processed\" / \"I3D\" / \"audio\" / \"enhanced\" / \"truth\",\n",
    "            'text': dataset_base / \"processed\" / \"I3D\" / \"text\",\n",
    "            'visual': dataset_base / \"processed\" / \"I3D\" / \"visual\",\n",
    "            'multimodal': dataset_base / \"processed\" / \"I3D\" / \"multimodal\",\n",
    "            'metadata': dataset_base / \"metadata\" / \"I3D\",\n",
    "            'validation': dataset_base / \"validation\" / \"I3D\",\n",
    "            'quality_reports': dataset_base / \"validation\" / \"I3D\" / \"quality_reports\",\n",
    "            'statistical': dataset_base / \"validation\" / \"I3D\" / \"statistical_analyses\",\n",
    "            'figures': dataset_base / \"figures\" / \"I3D\",\n",
    "            'exploratory_figs': dataset_base / \"figures\" / \"I3D\" / \"exploratory\",\n",
    "            'logs': dataset_base / \"_logs\" / \"I3D\",\n",
    "            'reextraction': dataset_base / \"processed\" / \"I3D\" / \"reextraction\",\n",
    "            'checkpoints': dataset_base / \"_checkpoints\" / \"I3D\",\n",
    "            'cache': dataset_base / \"_cache\" / \"I3D\",\n",
    "            'decision_logs': dataset_base / \"_logs\" / \"I3D\" / \"decisions\"\n",
    "        }\n",
    "    \n",
    "    def create_directories(self):\n",
    "        for path in self.paths.values():\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"âœ“ Directory structure created for I3D\")\n",
    "    \n",
    "    def get_log_path(self) -> Tuple[Path, str]:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_path = self.paths['logs'] / f\"extraction_{timestamp}.log\"\n",
    "        return log_path, timestamp\n",
    "\n",
    "\n",
    "# ==================== LOGGER ====================\n",
    "class I3DLogger:\n",
    "    \"\"\"Logger for I3D Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, log_path: Path):\n",
    "        self.log_path = log_path\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - [I3D] - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_path, encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def log(self, message: str):\n",
    "        logging.info(message)\n",
    "    \n",
    "    def log_error(self, message: str):\n",
    "        logging.error(message)\n",
    "    \n",
    "    def log_warning(self, message: str):\n",
    "        logging.warning(message)\n",
    "    \n",
    "    def finalize(self):\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - self.start_time\n",
    "        logging.info(\"\\n\" + \"=\"*70)\n",
    "        logging.info(f\"Finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        logging.info(f\"Total duration: {duration}\")\n",
    "\n",
    "\n",
    "# ==================== VIDEO PREPROCESSOR ====================\n",
    "class VideoPreprocessor:\n",
    "    \"\"\"Video preprocessing for I3D\"\"\"\n",
    "    \n",
    "    def __init__(self, config: I3DConfig):\n",
    "        self.config = config\n",
    "        try:\n",
    "            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load Haar cascades: {str(e)}\")\n",
    "            self.face_cascade = None\n",
    "            self.eye_cascade = None\n",
    "    \n",
    "    def zoom_frame(self, frame: np.ndarray, zoom_factor: float = 1.5) -> np.ndarray:\n",
    "        height, width = frame.shape[:2]\n",
    "        center_x, center_y = width // 2, height // 2\n",
    "        \n",
    "        crop_width = int(width / zoom_factor)\n",
    "        crop_height = int(height / zoom_factor)\n",
    "        \n",
    "        x1 = max(0, center_x - crop_width // 2)\n",
    "        y1 = max(0, center_y - crop_height // 2)\n",
    "        x2 = min(width, x1 + crop_width)\n",
    "        y2 = min(height, y1 + crop_height)\n",
    "        \n",
    "        cropped = frame[y1:y2, x1:x2]\n",
    "        zoomed = cv2.resize(cropped, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        return zoomed\n",
    "    \n",
    "    def enhance_eye_region_for_iris(self, frame: np.ndarray) -> Tuple[np.ndarray, Optional[List]]:\n",
    "        if self.face_cascade is None or self.eye_cascade is None:\n",
    "            return frame, None\n",
    "            \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            return frame, None\n",
    "        \n",
    "        result = frame.copy()\n",
    "        eye_regions = []\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_y_start = y\n",
    "            roi_y_end = y + int(h * 0.6)\n",
    "            roi_gray = gray[roi_y_start:roi_y_end, x:x+w]\n",
    "            roi_color = result[roi_y_start:roi_y_end, x:x+w]\n",
    "            \n",
    "            eyes = self.eye_cascade.detectMultiScale(roi_gray, 1.1, 5)\n",
    "            \n",
    "            for (ex, ey, ew, eh) in eyes:\n",
    "                expansion = 0.3\n",
    "                ex_exp = max(0, int(ex - ew * expansion))\n",
    "                ey_exp = max(0, int(ey - eh * expansion))\n",
    "                ew_exp = min(roi_color.shape[1] - ex_exp, int(ew * (1 + 2 * expansion)))\n",
    "                eh_exp = min(roi_color.shape[0] - ey_exp, int(eh * (1 + 2 * expansion)))\n",
    "                \n",
    "                eye_roi = roi_color[ey_exp:ey_exp+eh_exp, ex_exp:ex_exp+ew_exp]\n",
    "                \n",
    "                if eye_roi.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                lab = cv2.cvtColor(eye_roi, cv2.COLOR_BGR2LAB)\n",
    "                l, a, b = cv2.split(lab)\n",
    "                \n",
    "                clip_limit = self.config.get('video.preprocessing.clahe_clip_limit', 4.0)\n",
    "                tile_size = self.config.get('video.preprocessing.clahe_tile_size', (4, 4))\n",
    "                \n",
    "                clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "                l_enhanced = clahe.apply(l)\n",
    "                lab_enhanced = cv2.merge([l_enhanced, a, b])\n",
    "                eye_enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)\n",
    "                \n",
    "                kernel = np.array([[-1, -1, -1], [-1, 10, -1], [-1, -1, -1]])\n",
    "                eye_enhanced = cv2.filter2D(eye_enhanced, -1, kernel)\n",
    "                eye_enhanced = cv2.convertScaleAbs(eye_enhanced, alpha=1.2, beta=15)\n",
    "                \n",
    "                roi_color[ey_exp:ey_exp+eh_exp, ex_exp:ex_exp+ew_exp] = eye_enhanced\n",
    "                eye_regions.append((x + ex_exp, roi_y_start + ey_exp, ew_exp, eh_exp))\n",
    "        \n",
    "        return result, eye_regions\n",
    "    \n",
    "    def preprocess_pipeline_ultra(self, frame: np.ndarray) -> np.ndarray:\n",
    "        # Step 1: Denoise\n",
    "        frame = cv2.bilateralFilter(frame, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "        \n",
    "        # Step 2: Adaptive brightness\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        mean_brightness = np.mean(gray)\n",
    "        \n",
    "        if mean_brightness < 50:\n",
    "            alpha, beta = 2.5, 80\n",
    "        elif mean_brightness < 100:\n",
    "            alpha, beta = 1.8, 50\n",
    "        elif mean_brightness < 120:\n",
    "            alpha, beta = 1.3, 20\n",
    "        elif mean_brightness > 180:\n",
    "            alpha, beta = 0.8, -20\n",
    "        else:\n",
    "            alpha, beta = 1.0, 0\n",
    "        \n",
    "        frame = cv2.convertScaleAbs(frame, alpha=alpha, beta=beta)\n",
    "        \n",
    "        # Step 3: Enhance eye region for iris\n",
    "        frame, eye_regions = self.enhance_eye_region_for_iris(frame)\n",
    "        \n",
    "        # Step 4: Gamma correction\n",
    "        gamma = self.config.get('video.preprocessing.gamma_correction', 1.4)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        frame = cv2.LUT(frame, table)\n",
    "        \n",
    "        # Step 5: Sharpen\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        frame = cv2.filter2D(frame, -1, kernel)\n",
    "        \n",
    "        # Step 6: Unsharp mask\n",
    "        strength = self.config.get('video.preprocessing.unsharp_strength', 1.8)\n",
    "        blurred = cv2.GaussianBlur(frame, (0, 0), 1.5)\n",
    "        frame = cv2.addWeighted(frame, 1.0 + strength, blurred, -strength, 0)\n",
    "        \n",
    "        # Step 7: Final CLAHE\n",
    "        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        l_enhanced = clahe.apply(l)\n",
    "        lab_enhanced = cv2.merge([l_enhanced, a, b])\n",
    "        frame = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "\n",
    "# ==================== AUDIO PROCESSOR ====================\n",
    "class AudioProcessor:\n",
    "    \"\"\"Audio processing for I3D with deterministic enhancement\"\"\"\n",
    "    \n",
    "    def __init__(self, config: I3DConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def get_enhancement_params(self, audio_dbfs: float, ultra_mode: bool = False) -> Dict:\n",
    "        \"\"\"Get fixed enhancement parameters based on audio level\"\"\"\n",
    "        params = {\n",
    "            'ultra_mode': ultra_mode,\n",
    "            'original_dbfs': audio_dbfs\n",
    "        }\n",
    "        \n",
    "        # Fixed thresholds (not adaptive)\n",
    "        extreme_threshold = self.config.get('audio.enhancement.extreme_boost_threshold', -55)\n",
    "        \n",
    "        if ultra_mode or audio_dbfs < -45:\n",
    "            if audio_dbfs < extreme_threshold:\n",
    "                params['boost_amount'] = 40  # Fixed max boost\n",
    "                params['boost_type'] = 'ultra'\n",
    "            elif audio_dbfs < -45:\n",
    "                params['boost_amount'] = 35  # Fixed aggressive boost\n",
    "                params['boost_type'] = 'aggressive'\n",
    "            else:\n",
    "                params['boost_amount'] = 20\n",
    "                params['boost_type'] = 'moderate'\n",
    "        elif audio_dbfs < -35:\n",
    "            params['boost_amount'] = 20\n",
    "            params['boost_type'] = 'moderate'\n",
    "        else:\n",
    "            params['boost_amount'] = 0\n",
    "            params['boost_type'] = 'none'\n",
    "        \n",
    "        # Fixed target\n",
    "        params['target_dbfs'] = -18\n",
    "        params['silence_threshold'] = -50\n",
    "        params['compression_ratio'] = 6.0 if not ultra_mode else 10.0\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def enhance_audio(self, audio_path: Path, logger: I3DLogger, \n",
    "                     ultra_mode: bool = False) -> Tuple[Path, Dict]:\n",
    "        \"\"\"Enhanced audio with fixed parameters for reproducibility\"\"\"\n",
    "        try:\n",
    "            if not audio_path.exists():\n",
    "                logger.log_error(f\"Audio file not found: {audio_path}\")\n",
    "                return audio_path, {}\n",
    "            \n",
    "            if ultra_mode:\n",
    "                enhanced_path = audio_path.parent / f\"{audio_path.stem}_ultra_enhanced.wav\"\n",
    "            else:\n",
    "                enhanced_path = audio_path.parent / f\"{audio_path.stem}_enhanced.wav\"\n",
    "            \n",
    "            if enhanced_path.exists():\n",
    "                logger.log(f\"Enhanced audio exists: {enhanced_path.name}\")\n",
    "                audio = AudioSegment.from_wav(audio_path)\n",
    "                params = self.get_enhancement_params(audio.dBFS, ultra_mode)\n",
    "                return enhanced_path, params\n",
    "            \n",
    "            audio = AudioSegment.from_wav(audio_path)\n",
    "            original_duration = len(audio)\n",
    "            original_dbfs = audio.dBFS\n",
    "            \n",
    "            # Get fixed parameters\n",
    "            params = self.get_enhancement_params(original_dbfs, ultra_mode)\n",
    "            \n",
    "            logger.log(f\"  â””â”€ ðŸ“Š Original: {original_dbfs:.1f} dBFS, {original_duration/1000:.1f}s\")\n",
    "            logger.log(f\"  â””â”€ ðŸ”§ Enhancement params: {params['boost_type']}, boost={params['boost_amount']}dB\")\n",
    "            \n",
    "            enhancement_applied = []\n",
    "            \n",
    "            # Apply fixed boost\n",
    "            if params['boost_amount'] > 0:\n",
    "                audio = audio + params['boost_amount']\n",
    "                enhancement_applied.append(f\"{params['boost_type']} boost: +{params['boost_amount']}dB\")\n",
    "            \n",
    "            # Normalize\n",
    "            normalized_audio = audio.normalize(headroom=0.05)\n",
    "            enhancement_applied.append(\"Normalized\")\n",
    "            \n",
    "            # Target boost (fixed)\n",
    "            if normalized_audio.dBFS < params['target_dbfs']:\n",
    "                boost_amount = min(abs(normalized_audio.dBFS - params['target_dbfs']), 15)\n",
    "                normalized_audio = normalized_audio + boost_amount\n",
    "                enhancement_applied.append(f\"Final boost: +{boost_amount:.1f}dB\")\n",
    "            \n",
    "            # Trim silence (fixed threshold)\n",
    "            silence_threshold = params['silence_threshold']\n",
    "            chunk_length_ms = 10\n",
    "            padding_ms = 100\n",
    "            \n",
    "            start_trim = 0\n",
    "            for i in range(0, len(normalized_audio), chunk_length_ms):\n",
    "                chunk = normalized_audio[i:i+chunk_length_ms]\n",
    "                if len(chunk) > 0 and chunk.dBFS > silence_threshold:\n",
    "                    start_trim = max(0, i - padding_ms)\n",
    "                    break\n",
    "            \n",
    "            end_trim = len(normalized_audio)\n",
    "            for i in range(len(normalized_audio), 0, -chunk_length_ms):\n",
    "                chunk = normalized_audio[i-chunk_length_ms:i]\n",
    "                if len(chunk) > 0 and chunk.dBFS > silence_threshold:\n",
    "                    end_trim = min(len(normalized_audio), i + padding_ms)\n",
    "                    break\n",
    "            \n",
    "            if start_trim > 0 or end_trim < len(normalized_audio):\n",
    "                trimmed_audio = normalized_audio[start_trim:end_trim]\n",
    "                trimmed_ms = (start_trim + (len(normalized_audio) - end_trim))\n",
    "                if trimmed_ms > 50:\n",
    "                    enhancement_applied.append(f\"Trimmed: {trimmed_ms}ms\")\n",
    "                normalized_audio = trimmed_audio\n",
    "            \n",
    "            # Filtering (fixed)\n",
    "            filtered_audio = normalized_audio.high_pass_filter(80).low_pass_filter(3000)\n",
    "            enhancement_applied.append(\"Filter (80-3000Hz)\")\n",
    "            \n",
    "            # Compression (fixed ratio)\n",
    "            compressed_audio = filtered_audio.compress_dynamic_range(\n",
    "                threshold=-15.0, \n",
    "                ratio=params['compression_ratio'],\n",
    "                attack=3.0, \n",
    "                release=40.0\n",
    "            )\n",
    "            enhancement_applied.append(f\"Compression (ratio={params['compression_ratio']})\")\n",
    "            \n",
    "            # De-clipping\n",
    "            samples = np.array(compressed_audio.get_array_of_samples())\n",
    "            max_val = np.max(np.abs(samples))\n",
    "            clipping_threshold = 32767 * 0.99\n",
    "            \n",
    "            if max_val > clipping_threshold:\n",
    "                samples_normalized = samples / 32768.0\n",
    "                samples_fixed = np.tanh(samples_normalized * 0.8) * 32767\n",
    "                compressed_audio = compressed_audio._spawn(samples_fixed.astype(np.int16).tobytes())\n",
    "                enhancement_applied.append(\"De-clipped\")\n",
    "            \n",
    "            # Final normalize\n",
    "            final_audio = compressed_audio.normalize(headroom=0.1)\n",
    "            \n",
    "            # Export\n",
    "            final_audio.export(str(enhanced_path), format=\"wav\")\n",
    "            \n",
    "            final_duration = len(final_audio)\n",
    "            improvement = final_audio.dBFS - original_dbfs\n",
    "            \n",
    "            logger.log(f\"âœ“ Enhanced: {audio_path.name}\")\n",
    "            logger.log(f\"  â””â”€ ðŸ“ˆ Improvement: {original_dbfs:.1f} â†’ {final_audio.dBFS:.1f} dBFS ({improvement:+.1f}dB)\")\n",
    "            logger.log(f\"  â””â”€ â±ï¸ Duration: {original_duration/1000:.1f}s â†’ {final_duration/1000:.1f}s\")\n",
    "            logger.log(f\"  â””â”€ ðŸ”§ Applied: {', '.join(enhancement_applied)}\")\n",
    "            \n",
    "            params['final_dbfs'] = final_audio.dBFS\n",
    "            params['enhancement_applied'] = enhancement_applied\n",
    "            \n",
    "            return enhanced_path, params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.log_error(f\"Failed to enhance audio: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.log_error(traceback.format_exc())\n",
    "            return audio_path, {}\n",
    "    \n",
    "    def extract_text(self, audio_path: Path, logger: I3DLogger, \n",
    "                     ultra_mode: bool = False, filename: str = None,\n",
    "                     transcript_cache: TranscriptCacheManager = None,\n",
    "                     decision_log: DecisionLogManager = None,\n",
    "                     force_retranscribe: bool = False) -> Tuple[Optional[str], bool, Dict]:\n",
    "        \"\"\"Extract text with caching and decision logging\"\"\"\n",
    "        \n",
    "        metadata = {\n",
    "            'source': 'unknown',\n",
    "            'from_cache': False,\n",
    "            'attempt': 0,\n",
    "            'mode': 'normal' if not ultra_mode else 'ultra',\n",
    "            'stt_energy_threshold': 0.0\n",
    "        }\n",
    "        \n",
    "        # Check force retranscribe\n",
    "        if not force_retranscribe:\n",
    "            # Check transcript cache first\n",
    "            if transcript_cache and self.config.get('text.use_transcript_cache', True):\n",
    "                cached = transcript_cache.load_transcript(filename)\n",
    "                if cached:\n",
    "                    logger.log(f\"âœ“ Using CACHED TRANSCRIPT: '{cached['text']}'\")\n",
    "                    logger.log(f\"  â””â”€ Source: {cached['source']}, Mode: {cached['mode']}\")\n",
    "                    \n",
    "                    if decision_log:\n",
    "                        decision_log.log_transcript_source(filename, cached['source'], from_cache=True)\n",
    "                    \n",
    "                    metadata['source'] = cached['source']\n",
    "                    metadata['from_cache'] = True\n",
    "                    metadata['attempt'] = cached.get('attempt', 0)\n",
    "                    metadata['mode'] = cached.get('mode', 'normal')\n",
    "                    metadata['stt_energy_threshold'] = cached.get('stt_energy_threshold', 0.0)\n",
    "                    \n",
    "                    return cached['text'], True, metadata\n",
    "            \n",
    "            # Check manual transcript\n",
    "            if filename and self.config.get('text.use_manual_transcripts', True):\n",
    "                if filename in MANUAL_TRANSCRIPTS:\n",
    "                    manual_text = MANUAL_TRANSCRIPTS[filename]\n",
    "                    logger.log(f\"âœ“ Using MANUAL TRANSCRIPT: '{manual_text}'\")\n",
    "                    \n",
    "                    # Save to cache\n",
    "                    if transcript_cache:\n",
    "                        transcript_cache.save_transcript(\n",
    "                            filename, manual_text, 'manual', \n",
    "                            attempt=0, mode='manual'\n",
    "                        )\n",
    "                    \n",
    "                    if decision_log:\n",
    "                        decision_log.log_transcript_source(filename, 'manual', from_cache=False)\n",
    "                    \n",
    "                    metadata['source'] = 'manual'\n",
    "                    metadata['from_cache'] = False\n",
    "                    \n",
    "                    return manual_text, True, metadata\n",
    "        else:\n",
    "            logger.log(f\"âš ï¸ FORCE RETRANSCRIBE enabled - ignoring cache\")\n",
    "        \n",
    "        # Proceed with automatic speech recognition\n",
    "        recognizer = sr.Recognizer()\n",
    "        \n",
    "        # Deterministic mode\n",
    "        deterministic_mode = self.config.get('speech_recognition.deterministic_mode', False)\n",
    "        if deterministic_mode:\n",
    "            recognizer.dynamic_energy_threshold = False\n",
    "            logger.log(f\"  â””â”€ ðŸ”’ Deterministic mode: dynamic_energy_threshold=False\")\n",
    "        else:\n",
    "            recognizer.dynamic_energy_threshold = True\n",
    "        \n",
    "        configs = self.config.get('speech_recognition.configs', [])\n",
    "        max_attempts = len(configs) if ultra_mode else 3\n",
    "        language = self.config.get('speech_recognition.language', 'id-ID')\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                config = configs[attempt]\n",
    "                \n",
    "                recognizer.energy_threshold = config['energy_threshold']\n",
    "                recognizer.pause_threshold = config['pause_threshold']\n",
    "                recognizer.phrase_threshold = config['phrase_threshold']\n",
    "                recognizer.non_speaking_duration = config.get('non_speaking_duration', 0.3)\n",
    "                \n",
    "                with sr.AudioFile(str(audio_path)) as source:\n",
    "                    if not deterministic_mode:\n",
    "                        recognizer.adjust_for_ambient_noise(source, duration=0.2)\n",
    "                    audio_data = recognizer.record(source)\n",
    "                \n",
    "                # Log actual energy threshold used\n",
    "                actual_threshold = recognizer.energy_threshold\n",
    "                logger.log(f\"  â””â”€ Attempt {attempt+1}: energy_threshold={actual_threshold:.1f}\")\n",
    "                \n",
    "                text = recognizer.recognize_google(audio_data, language=language)\n",
    "                \n",
    "                if text and len(text.strip()) > 0:\n",
    "                    logger.log(f\"âœ“ Speech recognition success (attempt {attempt+1}): '{text}'\")\n",
    "                    \n",
    "                    # Get audio dBFS for metadata\n",
    "                    try:\n",
    "                        audio = AudioSegment.from_wav(audio_path)\n",
    "                        audio_dbfs = audio.dBFS\n",
    "                    except:\n",
    "                        audio_dbfs = 0.0\n",
    "                    \n",
    "                    # Save to cache\n",
    "                    if transcript_cache:\n",
    "                        transcript_cache.save_transcript(\n",
    "                            filename, text, \n",
    "                            'stt_ultra' if ultra_mode else 'stt',\n",
    "                            attempt=attempt+1,\n",
    "                            mode='ultra' if ultra_mode else 'normal',\n",
    "                            audio_dbfs=audio_dbfs,\n",
    "                            stt_energy_threshold=actual_threshold\n",
    "                        )\n",
    "                    \n",
    "                    if decision_log:\n",
    "                        decision_log.log_transcript_source(\n",
    "                            filename, \n",
    "                            'stt_ultra' if ultra_mode else 'stt',\n",
    "                            from_cache=False\n",
    "                        )\n",
    "                    \n",
    "                    metadata['source'] = 'stt_ultra' if ultra_mode else 'stt'\n",
    "                    metadata['from_cache'] = False\n",
    "                    metadata['attempt'] = attempt + 1\n",
    "                    metadata['stt_energy_threshold'] = actual_threshold\n",
    "                    \n",
    "                    return text, True, metadata\n",
    "            \n",
    "            except sr.UnknownValueError:\n",
    "                if attempt < max_attempts - 1:\n",
    "                    logger.log(f\"  â””â”€ Attempt {attempt+1} failed, trying next...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.log_warning(f\"  â””â”€ All {max_attempts} attempts failed\")\n",
    "            \n",
    "            except sr.RequestError as e:\n",
    "                logger.log_error(f\"  â””â”€ Google API error: {str(e)}\")\n",
    "                break\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.log_error(f\"  â””â”€ Attempt {attempt+1} error: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        metadata['source'] = 'failed'\n",
    "        return None, False, metadata\n",
    "\n",
    "\n",
    "# ==================== LANDMARK EXTRACTOR ====================\n",
    "class LandmarkExtractor:\n",
    "    \"\"\"Landmark extraction for I3D\"\"\"\n",
    "    \n",
    "    def __init__(self, config: I3DConfig, preprocessor: VideoPreprocessor):\n",
    "        self.config = config\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def extract_with_multi_strategy(self, frame: np.ndarray, face_mesh, \n",
    "                                    iris_focus: bool = False) -> Tuple[Optional[Dict], int, Optional[str]]:\n",
    "        best_result = None\n",
    "        best_iris_count = 0\n",
    "        best_strategy = None\n",
    "        \n",
    "        zoom_levels = self.config.get('video.zoom_levels', [1.5, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0, 8.0, 10.0])\n",
    "        \n",
    "        # Strategy 1: Ultra preprocessing + multi-level zoom\n",
    "        for zoom in zoom_levels:\n",
    "            try:\n",
    "                if zoom == 1.0:\n",
    "                    processed_frame = frame\n",
    "                else:\n",
    "                    processed_frame = self.preprocessor.zoom_frame(frame.copy(), zoom)\n",
    "                \n",
    "                processed_frame = self.preprocessor.preprocess_pipeline_ultra(processed_frame)\n",
    "                frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                results = face_mesh.process(frame_rgb)\n",
    "                \n",
    "                if results.multi_face_landmarks:\n",
    "                    landmarks_dict = {}\n",
    "                    for face_landmarks in results.multi_face_landmarks:\n",
    "                        for idx, landmark in enumerate(face_landmarks.landmark):\n",
    "                            landmarks_dict[idx] = [landmark.x, landmark.y, landmark.z]\n",
    "                    \n",
    "                    iris_count = sum(1 for i in range(468, 478) if i in landmarks_dict)\n",
    "                    \n",
    "                    if iris_count > best_iris_count:\n",
    "                        best_result = landmarks_dict\n",
    "                        best_iris_count = iris_count\n",
    "                        best_strategy = f\"zoom_{zoom}x_ultra\"\n",
    "                    \n",
    "                    if iris_count >= 9:\n",
    "                        return landmarks_dict, iris_count, best_strategy\n",
    "            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Strategy 2: Extreme brightness + zoom\n",
    "        if iris_focus and best_iris_count < 8:\n",
    "            for zoom in [3.0, 5.0, 8.0]:\n",
    "                try:\n",
    "                    zoomed = self.preprocessor.zoom_frame(frame.copy(), zoom)\n",
    "                    extreme_bright = cv2.convertScaleAbs(zoomed, alpha=2.5, beta=80)\n",
    "                    processed = self.preprocessor.preprocess_pipeline_ultra(extreme_bright)\n",
    "                    \n",
    "                    frame_rgb = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "                    results = face_mesh.process(frame_rgb)\n",
    "                    \n",
    "                    if results.multi_face_landmarks:\n",
    "                        landmarks_dict = {}\n",
    "                        for face_landmarks in results.multi_face_landmarks:\n",
    "                            for idx, landmark in enumerate(face_landmarks.landmark):\n",
    "                                landmarks_dict[idx] = [landmark.x, landmark.y, landmark.z]\n",
    "                        \n",
    "                        iris_count = sum(1 for i in range(468, 478) if i in landmarks_dict)\n",
    "                        \n",
    "                        if iris_count > best_iris_count:\n",
    "                            best_result = landmarks_dict\n",
    "                            best_iris_count = iris_count\n",
    "                            best_strategy = f\"zoom_{zoom}x_extreme\"\n",
    "                        \n",
    "                        if iris_count >= 9:\n",
    "                            return landmarks_dict, iris_count, best_strategy\n",
    "                \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        return best_result, best_iris_count, best_strategy\n",
    "\n",
    "\n",
    "# ==================== PAUSE ANALYZER ====================\n",
    "def extract_pause_silence_features(audio_path: Path, config: I3DConfig, logger: I3DLogger) -> Dict:\n",
    "    \"\"\"Extract pause and silence features\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(str(audio_path), sr=16000)\n",
    "        duration = len(y) / sr\n",
    "        \n",
    "        audio = AudioSegment.from_wav(str(audio_path))\n",
    "        \n",
    "        silence_threshold = config.get('audio.enhancement.silence_threshold', -50)\n",
    "        min_pause_duration = config.get('audio.enhancement.min_pause_duration', 200)\n",
    "        chunk_length_ms = 50\n",
    "        \n",
    "        pauses = []\n",
    "        current_pause_start = None\n",
    "        current_pause_duration = 0\n",
    "        \n",
    "        for i in range(0, len(audio), chunk_length_ms):\n",
    "            chunk = audio[i:i+chunk_length_ms]\n",
    "            \n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            \n",
    "            is_silent = chunk.dBFS < silence_threshold\n",
    "            \n",
    "            if is_silent:\n",
    "                if current_pause_start is None:\n",
    "                    current_pause_start = i\n",
    "                current_pause_duration += chunk_length_ms\n",
    "            else:\n",
    "                if current_pause_start is not None and current_pause_duration >= min_pause_duration:\n",
    "                    pauses.append({\n",
    "                        'start': current_pause_start,\n",
    "                        'duration': current_pause_duration,\n",
    "                        'end': current_pause_start + current_pause_duration\n",
    "                    })\n",
    "                \n",
    "                current_pause_start = None\n",
    "                current_pause_duration = 0\n",
    "        \n",
    "        if current_pause_start is not None and current_pause_duration >= min_pause_duration:\n",
    "            pauses.append({\n",
    "                'start': current_pause_start,\n",
    "                'duration': current_pause_duration,\n",
    "                'end': current_pause_start + current_pause_duration\n",
    "            })\n",
    "        \n",
    "        num_pauses = len(pauses)\n",
    "        total_pause_duration = sum(p['duration'] for p in pauses) / 1000\n",
    "        \n",
    "        if num_pauses > 0:\n",
    "            avg_pause_duration = total_pause_duration / num_pauses\n",
    "            longest_pause = max(p['duration'] for p in pauses) / 1000\n",
    "            pause_frequency = num_pauses / duration if duration > 0 else 0\n",
    "            \n",
    "            early_pauses = sum(1 for p in pauses if p['start'] < len(audio) * 0.33)\n",
    "            middle_pauses = sum(1 for p in pauses if len(audio) * 0.33 <= p['start'] < len(audio) * 0.67)\n",
    "            late_pauses = sum(1 for p in pauses if p['start'] >= len(audio) * 0.67)\n",
    "        else:\n",
    "            avg_pause_duration = 0\n",
    "            longest_pause = 0\n",
    "            pause_frequency = 0\n",
    "            early_pauses = 0\n",
    "            middle_pauses = 0\n",
    "            late_pauses = 0\n",
    "        \n",
    "        speech_duration = duration - total_pause_duration\n",
    "        speech_rate = speech_duration / duration if duration > 0 else 0\n",
    "        pause_ratio = total_pause_duration / duration if duration > 0 else 0\n",
    "        \n",
    "        hesitation_score = (\n",
    "            0.3 * pause_ratio +\n",
    "            0.3 * (pause_frequency / 2) +\n",
    "            0.2 * min(longest_pause / 2, 1.0) +\n",
    "            0.2 * min(num_pauses / 10, 1.0)\n",
    "        )\n",
    "        \n",
    "        if num_pauses > 1:\n",
    "            pause_durations = [p['duration'] / 1000 for p in pauses]\n",
    "            pause_std = np.std(pause_durations)\n",
    "            pause_variability = pause_std / avg_pause_duration if avg_pause_duration > 0 else 0\n",
    "        else:\n",
    "            pause_std = 0\n",
    "            pause_variability = 0\n",
    "        \n",
    "        features = {\n",
    "            'num_pauses': num_pauses,\n",
    "            'total_pause_duration': float(total_pause_duration),\n",
    "            'avg_pause_duration': float(avg_pause_duration),\n",
    "            'longest_pause_duration': float(longest_pause),\n",
    "            'pause_frequency': float(pause_frequency),\n",
    "            'speech_duration': float(speech_duration),\n",
    "            'speech_rate': float(speech_rate),\n",
    "            'pause_ratio': float(pause_ratio),\n",
    "            'early_pauses': int(early_pauses),\n",
    "            'middle_pauses': int(middle_pauses),\n",
    "            'late_pauses': int(late_pauses),\n",
    "            'pause_std': float(pause_std),\n",
    "            'pause_variability': float(pause_variability),\n",
    "            'hesitation_score': float(hesitation_score),\n",
    "            'has_long_pauses': longest_pause > 1.5,\n",
    "            'has_frequent_pauses': pause_frequency > 1.0,\n",
    "            'has_high_pause_ratio': pause_ratio > 0.3\n",
    "        }\n",
    "        \n",
    "        logger.log(f\"âœ“ Pause features extracted\")\n",
    "        logger.log(f\"  â””â”€ Pauses: {num_pauses}, Hesitation: {hesitation_score:.3f}\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.log_error(f\"Failed to extract pause features: {str(e)}\")\n",
    "        return {\n",
    "            'num_pauses': 0, 'total_pause_duration': 0, 'avg_pause_duration': 0,\n",
    "            'longest_pause_duration': 0, 'pause_frequency': 0, 'speech_duration': 0,\n",
    "            'speech_rate': 0, 'pause_ratio': 0, 'early_pauses': 0, 'middle_pauses': 0,\n",
    "            'late_pauses': 0, 'pause_std': 0, 'pause_variability': 0, 'hesitation_score': 0,\n",
    "            'has_long_pauses': False, 'has_frequent_pauses': False, 'has_high_pause_ratio': False\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== AUDIO FEATURES (FIXED TO 94) ====================\n",
    "def extract_audio_features(audio_path: Path, config: I3DConfig, logger: I3DLogger) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract 94 audio features with explicit librosa parameters\n",
    "    \n",
    "    Features breakdown:\n",
    "    - MFCC: 13 mean + 13 std = 26\n",
    "    - Delta MFCC: 13 mean + 13 std = 26\n",
    "    - Delta2 MFCC: 13 mean + 13 std = 26\n",
    "    - Bark Energy: 2 (mean, std)\n",
    "    - Delta Energy: 2 (mean, std)\n",
    "    - Delta2 Energy: 2 (mean, std)\n",
    "    - Spectral Centroid: 2 (mean, std)\n",
    "    - Spectral Bandwidth: 2 (mean, std)\n",
    "    - Spectral Rolloff: 2 (mean, std)\n",
    "    - ZCR: 2 (mean, std)\n",
    "    - Chroma: 2 (mean, std)\n",
    "    Total: 94 features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get librosa parameters from config\n",
    "        librosa_params = config.get('audio.librosa_params', {})\n",
    "        n_fft = librosa_params.get('n_fft', 2048)\n",
    "        hop_length = librosa_params.get('hop_length', 512)\n",
    "        win_length = librosa_params.get('win_length', 2048)\n",
    "        window = librosa_params.get('window', 'hann')\n",
    "        center = librosa_params.get('center', True)\n",
    "        \n",
    "        y, sr = librosa.load(str(audio_path), sr=16000)\n",
    "        \n",
    "        # MFCC (26 features)\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=y, sr=sr, n_mfcc=13,\n",
    "            n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "            window=window, center=center\n",
    "        )\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        mfcc_std = np.std(mfcc, axis=1)\n",
    "        \n",
    "        # Delta MFCC (26 features)\n",
    "        delta_mfcc = librosa.feature.delta(mfcc)\n",
    "        delta_mfcc_mean = np.mean(delta_mfcc, axis=1)\n",
    "        delta_mfcc_std = np.std(delta_mfcc, axis=1)\n",
    "        \n",
    "        # Delta2 MFCC (26 features)\n",
    "        delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "        delta2_mfcc_mean = np.mean(delta2_mfcc, axis=1)\n",
    "        delta2_mfcc_std = np.std(delta2_mfcc, axis=1)\n",
    "        \n",
    "        # Mel spectrogram for energy features\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=24,\n",
    "            n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "            window=window, center=center\n",
    "        )\n",
    "        \n",
    "        # Bark Energy (2 features)\n",
    "        # NOTE: axis=0 means sum across mel bands (24 bins) â†’ total energy per time frame\n",
    "        # This represents the total spectral energy at each time point\n",
    "        # Alternative: axis=1 would give energy per mel band (summed across time)\n",
    "        bark_energy = np.sum(mel_spec, axis=0)  # Shape: (num_frames,)\n",
    "        bark_energy_mean = np.mean(bark_energy)\n",
    "        bark_energy_std = np.std(bark_energy)\n",
    "        \n",
    "        # Delta Energy (2 features)\n",
    "        delta_energy = librosa.feature.delta(mel_spec)\n",
    "        delta_energy_mean = np.mean(delta_energy)\n",
    "        delta_energy_std = np.std(delta_energy)\n",
    "        \n",
    "        # Delta2 Energy (2 features)\n",
    "        delta2_energy = librosa.feature.delta(mel_spec, order=2)\n",
    "        delta2_energy_mean = np.mean(delta2_energy)\n",
    "        delta2_energy_std = np.std(delta2_energy)\n",
    "        \n",
    "        # Spectral features (6 features)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(\n",
    "            y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "            window=window, center=center\n",
    "        )[0]\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(\n",
    "            y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "            window=window, center=center\n",
    "        )[0]\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(\n",
    "            y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "            window=window, center=center\n",
    "        )[0]\n",
    "        \n",
    "        # ZCR (2 features)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "        \n",
    "        # Chroma (2 features)\n",
    "        chroma = librosa.feature.chroma_stft(\n",
    "            y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "            window=window, center=center\n",
    "        )\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = np.concatenate([\n",
    "            mfcc_mean, mfcc_std,                    # 26\n",
    "            delta_mfcc_mean, delta_mfcc_std,        # 26\n",
    "            delta2_mfcc_mean, delta2_mfcc_std,      # 26\n",
    "            [bark_energy_mean, bark_energy_std],    # 2\n",
    "            [delta_energy_mean, delta_energy_std],  # 2\n",
    "            [delta2_energy_mean, delta2_energy_std],# 2\n",
    "            [np.mean(spectral_centroid), np.std(spectral_centroid)],  # 2\n",
    "            [np.mean(spectral_bandwidth), np.std(spectral_bandwidth)],# 2\n",
    "            [np.mean(spectral_rolloff), np.std(spectral_rolloff)],    # 2\n",
    "            [np.mean(zcr), np.std(zcr)],            # 2\n",
    "            [np.mean(np.mean(chroma, axis=1)), np.std(np.mean(chroma, axis=1))]  # 2\n",
    "        ])\n",
    "        \n",
    "        # Verify feature count\n",
    "        assert len(features) == 94, f\"Expected 94 features, got {len(features)}\"\n",
    "        \n",
    "        logger.log(f\"âœ“ Audio features extracted: {len(features)} features\")\n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.log_error(f\"Failed to extract audio features: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.log_error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_audio_feature_names() -> List[str]:\n",
    "    \"\"\"Create column names for 94 audio features\"\"\"\n",
    "    feature_names = []\n",
    "    \n",
    "    # MFCC (26)\n",
    "    for i in range(13):\n",
    "        feature_names.append(f'mfcc{i+1}_mean')\n",
    "        feature_names.append(f'mfcc{i+1}_std')\n",
    "    \n",
    "    # Delta MFCC (26)\n",
    "    for i in range(13):\n",
    "        feature_names.append(f'delta_mfcc{i+1}_mean')\n",
    "        feature_names.append(f'delta_mfcc{i+1}_std')\n",
    "    \n",
    "    # Delta2 MFCC (26)\n",
    "    for i in range(13):\n",
    "        feature_names.append(f'delta2_mfcc{i+1}_mean')\n",
    "        feature_names.append(f'delta2_mfcc{i+1}_std')\n",
    "    \n",
    "    # Energy features (6)\n",
    "    feature_names.extend([\n",
    "        'bark_energy_mean', 'bark_energy_std',\n",
    "        'delta_energy_mean', 'delta_energy_std',\n",
    "        'delta2_energy_mean', 'delta2_energy_std'\n",
    "    ])\n",
    "    \n",
    "    # Spectral features (6)\n",
    "    feature_names.extend([\n",
    "        'spectral_centroid_mean', 'spectral_centroid_std',\n",
    "        'spectral_bandwidth_mean', 'spectral_bandwidth_std',\n",
    "        'spectral_rolloff_mean', 'spectral_rolloff_std'\n",
    "    ])\n",
    "    \n",
    "    # ZCR (2)\n",
    "    feature_names.extend(['zcr_mean', 'zcr_std'])\n",
    "    \n",
    "    # Chroma (2)\n",
    "    feature_names.extend(['chroma_mean', 'chroma_std'])\n",
    "    \n",
    "    # Verify count\n",
    "    assert len(feature_names) == 94, f\"Expected 94 feature names, got {len(feature_names)}\"\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "\n",
    "# ==================== AUDIO QUALITY ====================\n",
    "def check_audio_quality(audio_path: Path, config: I3DConfig, logger: I3DLogger) -> Dict:\n",
    "    \"\"\"Check audio quality with explicit STFT parameters\"\"\"\n",
    "    try:\n",
    "        # Get librosa parameters\n",
    "        librosa_params = config.get('audio.librosa_params', {})\n",
    "        n_fft = librosa_params.get('n_fft', 2048)\n",
    "        hop_length = librosa_params.get('hop_length', 512)\n",
    "        \n",
    "        y, sr = librosa.load(str(audio_path), sr=16000)\n",
    "        \n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        rms_mean = np.mean(rms)\n",
    "        rms_std = np.std(rms)\n",
    "        \n",
    "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "        zcr_mean = np.mean(zcr)\n",
    "        \n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        sc_mean = np.mean(spectral_centroid)\n",
    "        \n",
    "        # STFT with explicit parameters\n",
    "        S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "        noise_floor = np.percentile(S, 10)\n",
    "        signal_power = np.mean(S ** 2)\n",
    "        noise_power = noise_floor ** 2\n",
    "        \n",
    "        if noise_power > 0:\n",
    "            snr = 10 * np.log10(signal_power / noise_power)\n",
    "        else:\n",
    "            snr = 60\n",
    "        \n",
    "        duration = len(y) / sr\n",
    "        dynamic_range = np.max(np.abs(y)) - np.min(np.abs(y))\n",
    "        \n",
    "        rms_score = min(rms_mean / 0.1, 1.0)\n",
    "        snr_score = min(max(snr - 10, 0) / 40, 1.0)\n",
    "        zcr_score = 1 - min(zcr_mean / 0.2, 1.0)\n",
    "        sc_score = min(sc_mean / 2000, 1.0)\n",
    "        duration_score = min(duration / 5.0, 1.0)\n",
    "        dr_score = min(dynamic_range / 0.5, 1.0)\n",
    "        \n",
    "        quality_score = (\n",
    "            0.25 * rms_score + 0.25 * snr_score + 0.15 * zcr_score +\n",
    "            0.15 * sc_score + 0.10 * duration_score + 0.10 * dr_score\n",
    "        )\n",
    "        \n",
    "        with wave.open(str(audio_path), 'rb') as wf:\n",
    "            n_channels = wf.getnchannels()\n",
    "            sampwidth = wf.getsampwidth()\n",
    "            framerate = wf.getframerate()\n",
    "        \n",
    "        return {\n",
    "            'channels': n_channels,\n",
    "            'sample_width': sampwidth,\n",
    "            'frame_rate': framerate,\n",
    "            'duration': duration,\n",
    "            'rms': float(rms_mean),\n",
    "            'rms_std': float(rms_std),\n",
    "            'max_amplitude': float(np.max(np.abs(y))),\n",
    "            'snr': float(snr),\n",
    "            'snr_computation': f'STFT(n_fft={n_fft}, hop_length={hop_length})',\n",
    "            'zcr': float(zcr_mean),\n",
    "            'spectral_centroid': float(sc_mean),\n",
    "            'dynamic_range': float(dynamic_range),\n",
    "            'quality_score': float(quality_score)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.log_error(f\"Failed to check audio quality: {str(e)}\")\n",
    "        return {\n",
    "            'channels': 0, 'sample_width': 0, 'frame_rate': 0, 'duration': 0,\n",
    "            'rms': 0, 'rms_std': 0, 'max_amplitude': 0, 'snr': 0,\n",
    "            'snr_computation': 'failed',\n",
    "            'zcr': 0, 'spectral_centroid': 0, 'dynamic_range': 0, 'quality_score': 0\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== TEXT ANALYSIS ====================\n",
    "def analyze_text_content(text: str, language: str = 'id') -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze text for linguistic features\n",
    "    \n",
    "    NOTE: Sentiment analysis for Indonesian uses TextBlob (English-based)\n",
    "    and should be considered as proxy only. See config note.\n",
    "    \"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return {\n",
    "            'sentiment': 0, 'subjectivity': 0, 'complexity': 0, 'word_count': 0,\n",
    "            'char_count': 0, 'avg_word_length': 0, 'unique_words': 0, 'lexical_diversity': 0\n",
    "        }\n",
    "    \n",
    "    words = text.lower().split()\n",
    "    unique_words = set(words)\n",
    "    word_count = len(words)\n",
    "    char_count = len(text)\n",
    "    \n",
    "    lexical_diversity = len(unique_words) / word_count if word_count > 0 else 0\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Sentiment analysis (proxy for Indonesian)\n",
    "    sentiment = 0\n",
    "    subjectivity = 0\n",
    "    complexity = 0\n",
    "    \n",
    "    if language == 'en':  # Only reliable for English\n",
    "        try:\n",
    "            from textblob import TextBlob\n",
    "            blob = TextBlob(text)\n",
    "            sentiment = blob.sentiment.polarity\n",
    "            subjectivity = blob.sentiment.subjectivity\n",
    "            \n",
    "            sentences = blob.sentences\n",
    "            avg_sentence_length = sum(len(sentence.words) for sentence in sentences) / len(sentences) if sentences else 0\n",
    "            complexity = (avg_word_length * 0.5 + avg_sentence_length * 0.5) / 10\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"TextBlob analysis failed: {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'subjectivity': subjectivity,\n",
    "        'complexity': complexity,\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'unique_words': len(unique_words),\n",
    "        'lexical_diversity': lexical_diversity\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== TRANSLATION ====================\n",
    "def translate_text_with_cache(text: str, translation_cache: TranslationCacheManager,\n",
    "                              decision_log: DecisionLogManager = None,\n",
    "                              filename: str = None,\n",
    "                              max_retries: int = 3, delay: int = 1, \n",
    "                              logger: Optional[I3DLogger] = None) -> Optional[str]:\n",
    "    \"\"\"Translate Indonesian to English with caching\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Check cache first\n",
    "    cached_translation = translation_cache.get_translation(text)\n",
    "    if cached_translation:\n",
    "        if logger:\n",
    "            logger.log(f\"âœ“ Using CACHED TRANSLATION\")\n",
    "        \n",
    "        if decision_log and filename:\n",
    "            decision_log.log_translation(filename, from_cache=True, text_length=len(text))\n",
    "        \n",
    "        return cached_translation\n",
    "    \n",
    "    # Translate\n",
    "    translator = GoogleTranslator(source='id', target='en')\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if len(text) > 4500:\n",
    "                chunks = [text[i:i+4500] for i in range(0, len(text), 4500)]\n",
    "                translated_chunks = []\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    result = translator.translate(chunk)\n",
    "                    translated_chunks.append(result)\n",
    "                    time.sleep(0.5)\n",
    "                \n",
    "                translation = ' '.join(translated_chunks)\n",
    "            else:\n",
    "                translation = translator.translate(text)\n",
    "            \n",
    "            # Save to cache\n",
    "            translation_cache.save_translation(text, translation)\n",
    "            \n",
    "            if decision_log and filename:\n",
    "                decision_log.log_translation(filename, from_cache=False, text_length=len(text))\n",
    "            \n",
    "            return translation\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                if logger:\n",
    "                    logger.log_warning(f\"Translation attempt {attempt + 1} failed: {str(e)}. Retrying...\")\n",
    "                time.sleep(delay * (attempt + 1))\n",
    "            else:\n",
    "                if logger:\n",
    "                    logger.log_error(f\"Translation failed after {max_retries} attempts: {str(e)}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# ==================== UTILITY FUNCTIONS ====================\n",
    "def get_valid_video_files(directory: Path) -> List[str]:\n",
    "    \"\"\"Get all valid video files\"\"\"\n",
    "    if not directory.exists():\n",
    "        return []\n",
    "    \n",
    "    valid_extensions = ('.mp4', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.m4v')\n",
    "    \n",
    "    files = []\n",
    "    for file in directory.iterdir():\n",
    "        if file.is_file():\n",
    "            if file.name.startswith('.') or file.name.startswith('._'):\n",
    "                continue\n",
    "            if file.name in ['.DS_Store', 'Thumbs.db', 'desktop.ini']:\n",
    "                continue\n",
    "            if file.suffix.lower() in valid_extensions:\n",
    "                files.append(file.name)\n",
    "    \n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"Convert NumPy types to Python native types\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "# ==================== DATA DICTIONARY GENERATOR ====================\n",
    "def generate_data_dictionary(paths: I3DPathManager) -> Path:\n",
    "    \"\"\"Generate comprehensive data dictionary for all output CSVs\"\"\"\n",
    "    \n",
    "    dictionary = []\n",
    "    \n",
    "    # Text features (Indonesian)\n",
    "    dictionary.extend([\n",
    "        {'column': 'filename', 'type': 'string', 'description': 'Video filename (contains label prefix LIE_/TRUTH_)', 'unit': '-', 'range': '-', 'missing_value_policy': 'N/A'},\n",
    "        {'column': 'text_indonesian_original', 'type': 'string', 'description': 'Original transcript in Indonesian', 'unit': '-', 'range': '-', 'missing_value_policy': 'empty string if failed'},\n",
    "        {'column': 'text_indonesian_normalized', 'type': 'string', 'description': 'Transcript with numbers converted to words', 'unit': '-', 'range': '-', 'missing_value_policy': 'empty string if failed'},\n",
    "        {'column': 'label', 'type': 'integer', 'description': 'Ground truth label', 'unit': '-', 'range': '0=truth, 1=lie', 'missing_value_policy': 'N/A'},\n",
    "        {'column': 'dataset', 'type': 'string', 'description': 'Dataset identifier', 'unit': '-', 'range': 'I3D', 'missing_value_policy': 'N/A'},\n",
    "        {'column': 'transcript_source', 'type': 'string', 'description': 'Source of transcript', 'unit': '-', 'range': 'manual/stt/stt_ultra/failed', 'missing_value_policy': 'failed'},\n",
    "        {'column': 'transcript_from_cache', 'type': 'boolean', 'description': 'Whether transcript loaded from cache', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'transcript_attempt', 'type': 'integer', 'description': 'STT attempt number that succeeded', 'unit': '-', 'range': '0-7', 'missing_value_policy': '0'},\n",
    "        {'column': 'transcript_mode', 'type': 'string', 'description': 'Extraction mode', 'unit': '-', 'range': 'normal/ultra/manual', 'missing_value_policy': 'normal'},\n",
    "        {'column': 'stt_energy_threshold', 'type': 'float', 'description': 'Energy threshold used in successful STT attempt', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'char_count_id', 'type': 'integer', 'description': 'Character count (Indonesian)', 'unit': 'characters', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'word_count_id', 'type': 'integer', 'description': 'Word count (Indonesian)', 'unit': 'words', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'sentiment_id', 'type': 'float', 'description': 'Sentiment polarity (PROXY ONLY - TextBlob is English-based, not computed for Indonesian)', 'unit': '-', 'range': '-1.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'subjectivity_id', 'type': 'float', 'description': 'Subjectivity score (PROXY ONLY, not computed for Indonesian)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'complexity_id', 'type': 'float', 'description': 'Text complexity score', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'lexical_diversity_id', 'type': 'float', 'description': 'Unique words / total words', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "    ])\n",
    "    \n",
    "    # Text features (English)\n",
    "    dictionary.extend([\n",
    "        {'column': 'text_english', 'type': 'string', 'description': 'English translation via Google Translate', 'unit': '-', 'range': '-', 'missing_value_policy': 'empty string if failed'},\n",
    "        {'column': 'char_count_en', 'type': 'integer', 'description': 'Character count (English)', 'unit': 'characters', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'word_count_en', 'type': 'integer', 'description': 'Word count (English)', 'unit': 'words', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'sentiment_en', 'type': 'float', 'description': 'Sentiment polarity (English - TextBlob)', 'unit': '-', 'range': '-1.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'subjectivity_en', 'type': 'float', 'description': 'Subjectivity score (English)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'complexity_en', 'type': 'float', 'description': 'Text complexity score (English)', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'lexical_diversity_en', 'type': 'float', 'description': 'Unique words / total words (English)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "    ])\n",
    "    \n",
    "    # Number features\n",
    "    dictionary.extend([\n",
    "        {'column': 'has_numbers', 'type': 'boolean', 'description': 'Contains numeric expressions', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'number_count', 'type': 'integer', 'description': 'Count of numeric expressions', 'unit': 'count', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'has_vague_quantifiers', 'type': 'boolean', 'description': 'Contains vague quantifiers (sekitar, kira-kira, etc)', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'has_exact_quantifiers', 'type': 'boolean', 'description': 'Contains exact quantifiers (tepat, persis, etc)', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'number_word_ratio', 'type': 'float', 'description': 'Ratio of number words to total words', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'has_large_numbers', 'type': 'boolean', 'description': 'Contains numbers > 1000', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'has_decimal_numbers', 'type': 'boolean', 'description': 'Contains decimal numbers', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "    ])\n",
    "    \n",
    "    # Audio features (94 total)\n",
    "    for i in range(1, 14):\n",
    "        dictionary.extend([\n",
    "            {'column': f'mfcc{i}_mean', 'type': 'float', 'description': f'MFCC coefficient {i} mean', 'unit': '-', 'range': 'real', 'missing_value_policy': '0'},\n",
    "            {'column': f'mfcc{i}_std', 'type': 'float', 'description': f'MFCC coefficient {i} std deviation', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        ])\n",
    "    \n",
    "    for i in range(1, 14):\n",
    "        dictionary.extend([\n",
    "            {'column': f'delta_mfcc{i}_mean', 'type': 'float', 'description': f'Delta MFCC coefficient {i} mean', 'unit': '-', 'range': 'real', 'missing_value_policy': '0'},\n",
    "            {'column': f'delta_mfcc{i}_std', 'type': 'float', 'description': f'Delta MFCC coefficient {i} std deviation', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        ])\n",
    "    \n",
    "    for i in range(1, 14):\n",
    "        dictionary.extend([\n",
    "            {'column': f'delta2_mfcc{i}_mean', 'type': 'float', 'description': f'Delta-delta MFCC coefficient {i} mean', 'unit': '-', 'range': 'real', 'missing_value_policy': '0'},\n",
    "            {'column': f'delta2_mfcc{i}_std', 'type': 'float', 'description': f'Delta-delta MFCC coefficient {i} std deviation', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        ])\n",
    "    \n",
    "    # âœ… PERBAIKAN: Bark Energy description lebih spesifik\n",
    "    dictionary.extend([\n",
    "        {\n",
    "            'column': 'bark_energy_mean', \n",
    "            'type': 'float', \n",
    "            'description': (\n",
    "                'Bark energy mean. Computed as sum of 24 mel-band energies per time frame '\n",
    "                '(axis=0 summation across mel bins), then averaged across all time frames. '\n",
    "                'Represents total spectral energy per time point. '\n",
    "                'See Methods section for mel spectrogram parameters (n_mels=24, n_fft=2048, hop_length=512).'\n",
    "            ), \n",
    "            'unit': '-', \n",
    "            'range': '>=0', \n",
    "            'missing_value_policy': '0'\n",
    "        },\n",
    "        {\n",
    "            'column': 'bark_energy_std', \n",
    "            'type': 'float', \n",
    "            'description': 'Bark energy std deviation (temporal variation across time frames)', \n",
    "            'unit': '-', \n",
    "            'range': '>=0', \n",
    "            'missing_value_policy': '0'\n",
    "        },\n",
    "        {\n",
    "            'column': 'delta_energy_mean', \n",
    "            'type': 'float', \n",
    "            'description': 'Delta energy mean (first-order temporal derivative of mel spectrogram energy)', \n",
    "            'unit': '-', \n",
    "            'range': 'real', \n",
    "            'missing_value_policy': '0'\n",
    "        },\n",
    "        {\n",
    "            'column': 'delta_energy_std', \n",
    "            'type': 'float', \n",
    "            'description': 'Delta energy std deviation', \n",
    "            'unit': '-', \n",
    "            'range': '>=0', \n",
    "            'missing_value_policy': '0'\n",
    "        },\n",
    "        {\n",
    "            'column': 'delta2_energy_mean', \n",
    "            'type': 'float', \n",
    "            'description': 'Delta-delta energy mean (second-order temporal derivative)', \n",
    "            'unit': '-', \n",
    "            'range': 'real', \n",
    "            'missing_value_policy': '0'\n",
    "        },\n",
    "        {\n",
    "            'column': 'delta2_energy_std', \n",
    "            'type': 'float', \n",
    "            'description': 'Delta-delta energy std deviation', \n",
    "            'unit': '-', \n",
    "            'range': '>=0', \n",
    "            'missing_value_policy': '0'\n",
    "        },\n",
    "        {'column': 'spectral_centroid_mean', 'type': 'float', 'description': 'Spectral centroid mean', 'unit': 'Hz', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'spectral_centroid_std', 'type': 'float', 'description': 'Spectral centroid std deviation', 'unit': 'Hz', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'spectral_bandwidth_mean', 'type': 'float', 'description': 'Spectral bandwidth mean', 'unit': 'Hz', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'spectral_bandwidth_std', 'type': 'float', 'description': 'Spectral bandwidth std deviation', 'unit': 'Hz', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'spectral_rolloff_mean', 'type': 'float', 'description': 'Spectral rolloff mean', 'unit': 'Hz', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'spectral_rolloff_std', 'type': 'float', 'description': 'Spectral rolloff std deviation', 'unit': 'Hz', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'zcr_mean', 'type': 'float', 'description': 'Zero-crossing rate mean', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'zcr_std', 'type': 'float', 'description': 'Zero-crossing rate std deviation', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'chroma_mean', 'type': 'float', 'description': 'Chroma feature mean', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'chroma_std', 'type': 'float', 'description': 'Chroma feature std deviation', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "    ])\n",
    "    \n",
    "    # Audio quality\n",
    "    dictionary.extend([\n",
    "        {'column': 'audio_quality_score', 'type': 'float', 'description': 'Overall audio quality score', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'audio_duration', 'type': 'float', 'description': 'Audio duration', 'unit': 'seconds', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'audio_snr', 'type': 'float', 'description': 'Signal-to-noise ratio (STFT-based, see Methods)', 'unit': 'dB', 'range': 'real', 'missing_value_policy': '0'},\n",
    "        {'column': 'audio_snr_computation', 'type': 'string', 'description': 'SNR computation method with parameters', 'unit': '-', 'range': '-', 'missing_value_policy': 'failed'},\n",
    "        {'column': 'audio_rms', 'type': 'float', 'description': 'Root mean square amplitude', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'audio_spectral_centroid', 'type': 'float', 'description': 'Mean spectral centroid', 'unit': 'Hz', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'audio_dynamic_range', 'type': 'float', 'description': 'Dynamic range', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "    ])\n",
    "    \n",
    "    # Pause features (17)\n",
    "    dictionary.extend([\n",
    "        {'column': 'pause_num_pauses', 'type': 'integer', 'description': 'Number of pauses detected', 'unit': 'count', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_total_pause_duration', 'type': 'float', 'description': 'Total pause duration', 'unit': 'seconds', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_avg_pause_duration', 'type': 'float', 'description': 'Average pause duration', 'unit': 'seconds', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_longest_pause_duration', 'type': 'float', 'description': 'Longest pause duration', 'unit': 'seconds', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_pause_frequency', 'type': 'float', 'description': 'Pauses per second', 'unit': 'Hz', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_speech_duration', 'type': 'float', 'description': 'Speech duration (excluding pauses)', 'unit': 'seconds', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_speech_rate', 'type': 'float', 'description': 'Speech rate (speech / total duration)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_pause_ratio', 'type': 'float', 'description': 'Pause ratio (pause / total duration)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_early_pauses', 'type': 'integer', 'description': 'Pauses in first third', 'unit': 'count', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_middle_pauses', 'type': 'integer', 'description': 'Pauses in middle third', 'unit': 'count', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_late_pauses', 'type': 'integer', 'description': 'Pauses in last third', 'unit': 'count', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_pause_std', 'type': 'float', 'description': 'Pause duration std deviation', 'unit': 'seconds', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_pause_variability', 'type': 'float', 'description': 'Pause variability (std / mean)', 'unit': '-', 'range': '>=0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_hesitation_score', 'type': 'float', 'description': 'Composite hesitation score', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0'},\n",
    "        {'column': 'pause_has_long_pauses', 'type': 'boolean', 'description': 'Has pauses > 1.5s', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'pause_has_frequent_pauses', 'type': 'boolean', 'description': 'Pause frequency > 1.0 Hz', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'pause_has_high_pause_ratio', 'type': 'boolean', 'description': 'Pause ratio > 0.3', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "    ])\n",
    "    \n",
    "    # Landmark features\n",
    "    dictionary.extend([\n",
    "        {'column': 'Video_Name', 'type': 'string', 'description': 'Video identifier (without extension)', 'unit': '-', 'range': '-', 'missing_value_policy': 'N/A'},\n",
    "        {'column': 'Frame', 'type': 'integer', 'description': 'Frame number', 'unit': 'frame', 'range': '>=0', 'missing_value_policy': 'N/A'},\n",
    "    ])\n",
    "    \n",
    "    # Face/Iris landmarks (478 = 468 face + 10 iris)\n",
    "    for i in range(478):\n",
    "        dictionary.extend([\n",
    "            {'column': f'Landmark_{i}_X', 'type': 'float', 'description': f'Face/Iris landmark {i} X coordinate (normalized 0-1)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0.0 if not detected'},\n",
    "            {'column': f'Landmark_{i}_Y', 'type': 'float', 'description': f'Face/Iris landmark {i} Y coordinate (normalized 0-1)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0.0 if not detected'},\n",
    "            {'column': f'Landmark_{i}_Z', 'type': 'float', 'description': f'Face/Iris landmark {i} Z coordinate (depth, relative scale)', 'unit': '-', 'range': 'real', 'missing_value_policy': '0.0 if not detected'},\n",
    "        ])\n",
    "    \n",
    "    # Pose landmarks (33)\n",
    "    for i in range(33):\n",
    "        dictionary.extend([\n",
    "            {'column': f'Pose_{i}_X', 'type': 'float', 'description': f'Pose landmark {i} X coordinate (normalized 0-1)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0.0 if not detected'},\n",
    "            {'column': f'Pose_{i}_Y', 'type': 'float', 'description': f'Pose landmark {i} Y coordinate (normalized 0-1)', 'unit': '-', 'range': '0.0 to 1.0', 'missing_value_policy': '0.0 if not detected'},\n",
    "            {'column': f'Pose_{i}_Z', 'type': 'float', 'description': f'Pose landmark {i} Z coordinate (depth, relative scale)', 'unit': '-', 'range': 'real', 'missing_value_policy': '0.0 if not detected'},\n",
    "        ])\n",
    "    \n",
    "    dictionary.append({'column': 'Class', 'type': 'integer', 'description': 'Ground truth label', 'unit': '-', 'range': '0=truth, 1=lie', 'missing_value_policy': 'N/A'})\n",
    "    \n",
    "    # Re-extraction metadata\n",
    "    dictionary.extend([\n",
    "        {'column': 'reextraction_triggered', 'type': 'boolean', 'description': 'Whether re-extraction was triggered for this file', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'reextraction_success', 'type': 'boolean', 'description': 'Whether re-extraction succeeded', 'unit': '-', 'range': 'true/false', 'missing_value_policy': 'false'},\n",
    "        {'column': 'reextraction_params', 'type': 'string', 'description': 'JSON string of re-extraction parameters', 'unit': '-', 'range': 'JSON', 'missing_value_policy': 'empty string'},\n",
    "    ])\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_dict = pd.DataFrame(dictionary)\n",
    "    dict_path = paths.paths['metadata'] / 'data_dictionary.csv'\n",
    "    df_dict.to_csv(dict_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    # âœ… NEW: Schema validation check\n",
    "    _validate_data_dictionary_schema(df_dict, paths)\n",
    "    \n",
    "    return dict_path\n",
    "\n",
    "\n",
    "def _validate_data_dictionary_schema(df_dict: pd.DataFrame, paths: I3DPathManager):\n",
    "    \"\"\"\n",
    "    Validate that data dictionary matches actual dataset columns.\n",
    "    Saves validation report to validation folder.\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dictionary_columns': df_dict['column'].tolist(),\n",
    "        'datasets_checked': {},\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # Check against actual datasets (if they exist)\n",
    "    datasets_to_check = {\n",
    "        'TextDataset_Indonesian': paths.paths['text'] / 'TextDataset_Indonesian.csv',\n",
    "        'TextDataset_English': paths.paths['text'] / 'TextDataset_English.csv',\n",
    "        'AudioDataset_Features': paths.paths['audio'] / 'AudioDataset_Features.csv',\n",
    "        'PauseFeatures': paths.paths['audio'] / 'PauseFeatures.csv',\n",
    "        'LandmarkDataset': paths.paths['visual'] / 'LandmarkDataset.csv',\n",
    "        'MultimodalDataset_Full': paths.paths['multimodal'] / 'MultimodalDataset_Full.csv',\n",
    "    }\n",
    "    \n",
    "    dict_columns = set(df_dict['column'].tolist())\n",
    "    \n",
    "    for dataset_name, dataset_path in datasets_to_check.items():\n",
    "        if not dataset_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Read only header\n",
    "            df_dataset = pd.read_csv(dataset_path, nrows=0)\n",
    "            dataset_columns = set(df_dataset.columns)\n",
    "            \n",
    "            # Check for missing columns in dictionary\n",
    "            missing_in_dict = dataset_columns - dict_columns\n",
    "            # Check for extra columns in dictionary (not in dataset)\n",
    "            extra_in_dict = dict_columns - dataset_columns\n",
    "            \n",
    "            validation_results['datasets_checked'][dataset_name] = {\n",
    "                'path': str(dataset_path),\n",
    "                'column_count': len(dataset_columns),\n",
    "                'missing_in_dictionary': list(missing_in_dict),\n",
    "                'extra_in_dictionary': list(extra_in_dict),\n",
    "                'status': 'OK' if not missing_in_dict and not extra_in_dict else 'MISMATCH'\n",
    "            }\n",
    "            \n",
    "            if missing_in_dict:\n",
    "                validation_results['issues'].append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'issue': 'missing_in_dictionary',\n",
    "                    'columns': list(missing_in_dict)\n",
    "                })\n",
    "            \n",
    "            if extra_in_dict:\n",
    "                validation_results['issues'].append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'issue': 'extra_in_dictionary',\n",
    "                    'columns': list(extra_in_dict)\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            validation_results['datasets_checked'][dataset_name] = {\n",
    "                'path': str(dataset_path),\n",
    "                'error': str(e),\n",
    "                'status': 'ERROR'\n",
    "            }\n",
    "    \n",
    "    # Save validation report\n",
    "    validation_path = paths.paths['validation'] / 'schema_check.json'\n",
    "    with open(validation_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(validation_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Log summary\n",
    "    if validation_results['issues']:\n",
    "        logging.warning(f\"âš ï¸ Data dictionary schema issues found: {len(validation_results['issues'])}\")\n",
    "        logging.warning(f\"   See: {validation_path}\")\n",
    "    else:\n",
    "        logging.info(f\"âœ“ Data dictionary schema validated: No issues\")\n",
    "\n",
    "\n",
    "# ==================== RUN MANIFEST GENERATOR ====================\n",
    "def generate_run_manifest(env_info: Dict, config: I3DConfig, paths: I3DPathManager, \n",
    "                         stats: Dict, video_files: List[str]) -> Path:\n",
    "    \"\"\"Generate comprehensive run manifest for reproducibility\"\"\"\n",
    "    \n",
    "    deterministic_mode = config.get('speech_recognition.deterministic_mode', False)\n",
    "    \n",
    "    manifest = {\n",
    "        'run_info': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'pipeline_version': VERSION,\n",
    "            'pipeline_name': PIPELINE_NAME,\n",
    "            'total_files_processed': stats.get('total_files', 0),\n",
    "            'success_rate': f\"{stats.get('successful_text_conversions', 0) / max(stats.get('total_files', 1), 1) * 100:.1f}%\"\n",
    "        },\n",
    "        'environment': env_info,\n",
    "        'configuration': config.config,\n",
    "        'input_files': {\n",
    "            'lie_videos': [f for f in video_files if f.startswith('LIE_')],\n",
    "            'truth_videos': [f for f in video_files if f.startswith('TRUTH_')],\n",
    "            'total_count': len(video_files)\n",
    "        },\n",
    "        'processing_statistics': convert_numpy_types(stats),\n",
    "        'determinism_notes': {\n",
    "            'transcript_caching': 'Enabled - transcripts frozen after first extraction',\n",
    "            'translation_caching': 'Enabled - MD5-based cache for Google Translate',\n",
    "            'audio_enhancement': 'Fixed parameters based on dBFS thresholds',\n",
    "            'stt_deterministic_mode': deterministic_mode,\n",
    "            'stt_dynamic_energy_threshold': 'Disabled' if deterministic_mode else 'Enabled (default)',\n",
    "            'librosa_parameters': config.get('audio.librosa_params', {}),\n",
    "            'mediapipe_landmarks': {\n",
    "                'face_mesh_refine_landmarks': True,\n",
    "                'total_face_iris_landmarks': 478,\n",
    "                'breakdown': '468 face + 10 iris',\n",
    "                'pose_landmarks': 33,\n",
    "                'validation': 'Runtime validation checks landmark count per frame',\n",
    "                'mediapipe_version': env_info['packages'].get('mediapipe', 'unknown')\n",
    "            },\n",
    "            'external_services': [\n",
    "                'Google Web Speech API (for STT - non-deterministic on first run)',\n",
    "                'Google Translate API (for translation - non-deterministic on first run)'\n",
    "            ],\n",
    "            'reproducibility_statement': (\n",
    "                'Pipeline is deterministic ONLY for cached files. '\n",
    "                'First run uses external APIs (Google STT + Translate) which are non-deterministic. '\n",
    "                'Subsequent runs with same cache produce identical output for cached files. '\n",
    "                'New files remain non-deterministic until cached. '\n",
    "                'For full reproducibility: (1) Keep cache directories intact, (2) Set force_retranscribe=false, '\n",
    "                '(3) Set deterministic_mode=true for STT (disables dynamic thresholds).'\n",
    "            )\n",
    "        },\n",
    "        'data_quality_notes': {\n",
    "            'manual_transcripts': f\"{stats.get('manual_transcripts_used', 0)} files use manually verified transcripts\",\n",
    "            'sentiment_analysis_caveat': 'TextBlob sentiment for Indonesian is proxy only (English-based model). Use sentiment_en for reliable sentiment analysis.',\n",
    "            'landmark_missing_values': 'Missing landmarks filled with (0.0, 0.0, 0.0). Do NOT interpret as actual position. Filter or impute before analysis.',\n",
    "            'filename_contains_label': 'WARNING: Filenames contain label prefix (LIE_/TRUTH_) - MUST remove before ML training to avoid label leakage',\n",
    "            'bark_energy_definition': 'Bark energy computed as sum across mel bands (axis=0) â†’ total energy per time frame. See data dictionary for details.'\n",
    "        },\n",
    "        'ethics_and_privacy': {\n",
    "            'data_type': 'Interrogation videos with audio, facial landmarks, and transcripts',\n",
    "            'contains_personal_data': True,\n",
    "            'data_components': [\n",
    "                'Video frames (not included in derived features)',\n",
    "                'Audio waveforms (not included in derived features)',\n",
    "                'Facial landmarks (478 points including iris)',\n",
    "                'Pose landmarks (33 body keypoints)',\n",
    "                'Speech transcripts (Indonesian and English)',\n",
    "                'Audio features (94 acoustic features)'\n",
    "            ],\n",
    "            'public_release_policy': 'Derived features (landmarks, audio features, transcripts) may be released. Raw video/audio require additional consent/anonymization.',\n",
    "            'recommendation': 'Consult IRB/ethics board for your institution regarding public release of interrogation data'\n",
    "        },\n",
    "        'output_files': {\n",
    "            'text_indonesian': str(paths.paths['text'] / 'TextDataset_Indonesian.csv'),\n",
    "            'text_english': str(paths.paths['text'] / 'TextDataset_English.csv'),\n",
    "            'number_features': str(paths.paths['text'] / 'NumberFeatures.csv'),\n",
    "            'audio_features': str(paths.paths['audio'] / 'AudioDataset_Features.csv'),\n",
    "            'pause_features': str(paths.paths['audio'] / 'PauseFeatures.csv'),\n",
    "            'landmarks': str(paths.paths['visual'] / 'LandmarkDataset.csv'),\n",
    "            'multimodal_full': str(paths.paths['multimodal'] / 'MultimodalDataset_Full.csv'),\n",
    "            'publication_ready': str(paths.paths['multimodal'] / 'PublicationDataset.csv'),\n",
    "            'data_dictionary': str(paths.paths['metadata'] / 'data_dictionary.csv'),\n",
    "            'decision_log': str(paths.paths['decision_logs'] / 'decision_log.jsonl'),\n",
    "            'transcript_cache': str(paths.paths['cache'] / 'transcripts'),\n",
    "            'translation_cache': str(paths.paths['cache'] / 'translation_cache.json'),\n",
    "            'schema_validation': str(paths.paths['validation'] / 'schema_check.json')\n",
    "        },\n",
    "        'citation': config.get('dataset.citation', 'Please cite: [Your Paper]'),\n",
    "        'license': 'See LICENSE file or paper for data usage terms',\n",
    "        'contact': 'See paper for contact information'\n",
    "    }\n",
    "    \n",
    "    # Save manifest\n",
    "    manifest_path = paths.paths['metadata'] / 'run_manifest.json'\n",
    "    with open(manifest_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return manifest_path\n",
    "\n",
    "\n",
    "# ==================== PROCESSING SUMMARY GENERATOR ====================\n",
    "def generate_processing_summary(text_audio_data: List[Dict], paths: I3DPathManager) -> Path:\n",
    "    \"\"\"Generate per-file processing summary for Technical Validation\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for entry in text_audio_data:\n",
    "        summary = {\n",
    "            'filename': entry.get('filename', ''),\n",
    "            'label': entry.get('label', -1),\n",
    "            'transcript_source': entry.get('transcript_source', 'unknown'),\n",
    "            'transcript_from_cache': entry.get('transcript_from_cache', False),\n",
    "            'transcript_attempt': entry.get('transcript_attempt', 0),\n",
    "            'stt_energy_threshold': entry.get('stt_energy_threshold', 0.0),\n",
    "            'text_length': len(entry.get('text_indonesian_normalized', '')),\n",
    "            'audio_duration': entry.get('audio_duration', 0),\n",
    "            'audio_quality_score': entry.get('audio_quality_score', 0),\n",
    "            'audio_snr': entry.get('audio_snr', 0),\n",
    "            'has_numbers': entry.get('has_numbers', False),\n",
    "            'pause_count': entry.get('pause_num_pauses', 0),\n",
    "            'hesitation_score': entry.get('pause_hesitation_score', 0),\n",
    "            'reextraction_triggered': entry.get('reextraction_triggered', False),\n",
    "            'reextraction_success': entry.get('reextraction_success', False),\n",
    "            'processing_success': entry.get('transcript_source', 'failed') != 'failed'\n",
    "        }\n",
    "        summary_data.append(summary)\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    summary_path = paths.paths['quality_reports'] / 'processing_summary.csv'\n",
    "    df_summary.to_csv(summary_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    return summary_path\n",
    "\n",
    "\n",
    "# ==================== MARKDOWN REPORT (UPDATED) ====================\n",
    "def generate_markdown_report(stats: Dict, output_paths: Dict, paths: I3DPathManager, \n",
    "                            start_time: datetime, end_time: datetime, \n",
    "                            env_info: Dict, config: I3DConfig) -> Path:\n",
    "    \"\"\"Generate comprehensive markdown report for Scientific Data\"\"\"\n",
    "    duration = end_time - start_time\n",
    "    hours, remainder = divmod(duration.total_seconds(), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    manual_count = stats.get('manual_transcripts_used', 0)\n",
    "    cached_transcripts = stats.get('cached_transcripts_used', 0)\n",
    "    cached_translations = stats.get('cached_translations_used', 0)\n",
    "    deterministic_mode = config.get('speech_recognition.deterministic_mode', False)\n",
    "    \n",
    "    markdown_content = f\"\"\"# ðŸ“Š I3D Dataset Extraction Report\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Version:** {VERSION} - {PIPELINE_NAME}  \n",
    "**Language:** Indonesian  \n",
    "**Status:** âœ… Production-Ready for Scientific Data Submission\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ CRITICAL NOTES FOR SCIENTIFIC DATA REVIEWERS\n",
    "\n",
    "### 1. Determinism Statement\n",
    "- **First Run**: Uses external APIs (Google STT + Translate) â†’ **non-deterministic**\n",
    "- **Subsequent Runs (with cache)**: All cached outputs â†’ **deterministic for cached files**\n",
    "- **Reproducibility**: Same input + same cache â†’ identical output for cached files\n",
    "- **New Files**: Will use external APIs â†’ non-deterministic until cached\n",
    "\n",
    "### 2. External Services Used\n",
    "- **Google Web Speech API**: Speech-to-text (non-deterministic, results cached per file)\n",
    "- **Google Translate API**: Translation (non-deterministic, MD5-cached per text)\n",
    "- **Mitigation**: All outputs frozen in cache and provided in dataset repository\n",
    "\n",
    "### 3. Known Limitations\n",
    "- **Sentiment Analysis**: TextBlob for Indonesian is **proxy only** (English-based model). Use `sentiment_en` for reliable analysis.\n",
    "- **Filename Leakage**: Filenames contain label prefix (`LIE_`/`TRUTH_`) - **MUST remove before ML training**\n",
    "- **Missing Landmarks**: Filled with `(0.0, 0.0, 0.0)` - **NOT actual position**. Filter or impute before analysis.\n",
    "- **Manual Transcripts**: {manual_count} files use manually verified transcripts (documented in cache)\n",
    "\n",
    "### 4. Audio Features Verification\n",
    "- **Claimed**: 94 features âœ…\n",
    "- **Actual**: 94 features âœ… (verified with assert)\n",
    "- **Breakdown**: MFCC(26) + Î”MFCC(26) + Î”Â²MFCC(26) + Energy(6) + Spectral(6) + ZCR(2) + Chroma(2) = 94\n",
    "\n",
    "### 5. MediaPipe Landmarks Verification\n",
    "- **Face Mesh**: 478 landmarks âœ… (468 face + 10 iris with `refine_landmarks=True`)\n",
    "- **Pose**: 33 landmarks âœ…\n",
    "- **Total per frame**: 478Ã—3 + 33Ã—3 = 1533 coordinate values + metadata\n",
    "\n",
    "### 6. Ethics & Privacy\n",
    "- **Data Type**: Interrogation videos with identifiable faces and voices\n",
    "- **Public Release**: Derived features (landmarks, audio features, transcripts) may be released\n",
    "- **Raw Data**: Video/audio require additional consent or anonymization\n",
    "- **Recommendation**: Consult IRB/ethics board before public release\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Extraction Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Total Files** | {stats['total_files']} |\n",
    "| **Text Extraction** | {stats['successful_text_conversions']}/{stats['total_files']} ({stats['successful_text_conversions']/max(stats['total_files'], 1)*100:.1f}% success) |\n",
    "| **Manual Transcripts Used** | {manual_count} |\n",
    "| **Cached Transcripts Used** | {cached_transcripts} |\n",
    "| **Audio Extraction** | {stats['successful_audio_extractions']}/{stats['total_files']} ({stats['successful_audio_extractions']/max(stats['total_files'], 1)*100:.1f}% success) |\n",
    "| **Translation** | {stats['successful_translations']}/{stats['successful_text_conversions']} ({stats['successful_translations']/max(stats['successful_text_conversions'], 1)*100:.1f}% success) |\n",
    "| **Cached Translations Used** | {cached_translations} |\n",
    "| **Landmark Extraction** | {stats['successful_landmark_extractions']}/{stats['total_files']} ({stats['successful_landmark_extractions']/max(stats['total_files'], 1)*100:.1f}% success) |\n",
    "| **Re-extractions** | {stats['reextraction_triggered']} triggered, {stats['reextraction_successful']} successful |\n",
    "\n",
    "---\n",
    "\n",
    "## â±ï¸ Processing Time\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Start Time** | {start_time.strftime('%Y-%m-%d %H:%M:%S')} |\n",
    "| **End Time** | {end_time.strftime('%Y-%m-%d %H:%M:%S')} |\n",
    "| **Total Duration** | {int(hours)}h {int(minutes)}m {int(seconds)}s |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ–¥ï¸ Environment (Reproducibility)\n",
    "\n",
    "| Component | Version |\n",
    "|-----------|---------|\n",
    "| **Python** | {env_info['python_version'].split()[0]} |\n",
    "| **Platform** | {env_info['platform']['system']} {env_info['platform']['release']} |\n",
    "| **NumPy** | {env_info['packages'].get('numpy', 'N/A')} |\n",
    "| **Librosa** | {env_info['packages'].get('librosa', 'N/A')} |\n",
    "| **MediaPipe** | {env_info['packages'].get('mediapipe', 'N/A')} |\n",
    "| **OpenCV** | {env_info['packages'].get('opencv-python', 'N/A')} |\n",
    "| **FFmpeg** | {env_info.get('ffmpeg_version', 'N/A')[:50]}... |\n",
    "\n",
    "**Full environment**: See `run_manifest.json`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Detection Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Total Frames Processed** | {stats['total_frames_processed']:,} |\n",
    "| **Face Detection** | {stats['face_detected']:,} frames ({stats['face_detected']/max(stats['total_frames_processed'],1)*100:.1f}%) |\n",
    "| **Iris Detection** | {stats['iris_detected']:,} frames ({stats['iris_detected']/max(stats['total_frames_processed'],1)*100:.1f}%) |\n",
    "| **Pose Detection** | {stats['pose_detected']:,} frames ({stats['pose_detected']/max(stats['total_frames_processed'],1)*100:.1f}%) |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ Number Processing\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Texts with Numbers** | {stats['texts_with_numbers']} |\n",
    "| **Numbers Normalized** | {stats['numbers_normalized']} |\n",
    "| **Normalization Range** | 0 to Trillion (Indonesian words) |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”’ Deterministic Processing Details\n",
    "\n",
    "### Transcript Freezing\n",
    "âœ… **{cached_transcripts} transcripts loaded from cache**  \n",
    "- Cache location: `dataset/_cache/I3D/transcripts/`\n",
    "- Format: JSON per file (text, source, attempt, mode, stt_energy_threshold, timestamp)\n",
    "- Priority: Cache â†’ Manual â†’ STT\n",
    "- Override: Set `force_retranscribe: true` in config\n",
    "\n",
    "### Translation Freezing\n",
    "âœ… **{cached_translations} translations loaded from cache**  \n",
    "- Cache file: `dataset/_cache/I3D/translation_cache.json`\n",
    "- Key system: MD5 hash of source text\n",
    "- Eliminates Google Translate API variability\n",
    "\n",
    "### Fixed Audio Enhancement\n",
    "âœ… **Deterministic parameters**  \n",
    "- Boost amounts: 40dB (ultra), 35dB (aggressive), 20dB (moderate)\n",
    "- Target dBFS: -18 (fixed)\n",
    "- Compression ratio: 6.0 (normal), 10.0 (ultra)\n",
    "- All thresholds fixed (not adaptive)\n",
    "\n",
    "### Librosa Parameters (Explicit)\n",
    "```yaml\n",
    "n_fft: 2048\n",
    "hop_length: 512\n",
    "win_length: 2048\n",
    "window: hann\n",
    "center: true\n",
    "pad_mode: constant\n",
    "STT Configuration\n",
    "Deterministic Mode: {'Enabled' if deterministic_mode else 'Disabled (default)'}\n",
    "Dynamic Energy Threshold: {'Disabled' if deterministic_mode else 'Enabled'}\n",
    "Energy thresholds logged: Yes (per transcript in cache)\n",
    "Note: Deterministic mode disables adjust_for_ambient_noise() and dynamic_energy_threshold\n",
    "MediaPipe Configuration\n",
    "face_mesh:\n",
    "  refine_landmarks: true  # Enables 10 iris landmarks\n",
    "  total_landmarks: 478    # 468 face + 10 iris\n",
    "pose:\n",
    "  model_complexity: 1\n",
    "  total_landmarks: 33\n",
    "Decision Logging\n",
    "âœ… All processing decisions logged\n",
    "\n",
    "Log file: dataset/_logs/I3D/decisions/decision_log.jsonl\n",
    "Includes: re-extraction triggers, transcript sources, translations, energy thresholds\n",
    "Format: JSONL (one decision per line)\n",
    "ðŸ“ Manual Transcripts\n",
    "âœ… {manual_count} files use manually verified transcripts\n",
    "\n",
    "Automatically applied for known problematic files\n",
    "Ensures 100% accuracy for difficult audio\n",
    "Source documented in transcript metadata\n",
    "List: See MANUAL_TRANSCRIPTS dictionary in code\n",
    "ðŸ”„ Resume Capability\n",
    "âœ… Checkpoint system enabled\n",
    "\n",
    "Auto-save every 5 files\n",
    "Resume from last checkpoint if interrupted\n",
    "Preserves all cache states\n",
    "Checkpoint location: dataset/_checkpoints/I3D/\n",
    "ðŸ“¦ Output Files\n",
    "Text Datasets\n",
    "Indonesian Text: {output_paths.get('text_indonesian', 'N/A')}\n",
    "English Text: {output_paths.get('text_english', 'N/A')}\n",
    "Number Features: {output_paths.get('number_features', 'N/A')}\n",
    "Audio Datasets\n",
    "Audio Features (94): {output_paths.get('audio_features', 'N/A')}\n",
    "Pause Features (17): {output_paths.get('pause_features', 'N/A')}\n",
    "Visual Datasets\n",
    "Landmarks (478 face + 33 pose): {output_paths.get('landmark', 'N/A')}\n",
    "Multimodal Datasets\n",
    "Full Dataset: {output_paths.get('multimodal_full', 'N/A')}\n",
    "Publication Ready: {output_paths.get('publication', 'N/A')}\n",
    "Metadata & Validation\n",
    "Data Dictionary: dataset/metadata/I3D/data_dictionary.csv\n",
    "Run Manifest: dataset/metadata/I3D/run_manifest.json\n",
    "Processing Summary: dataset/validation/I3D/quality_reports/processing_summary.csv\n",
    "Decision Log: dataset/_logs/I3D/decisions/decision_log.jsonl\n",
    "Transcript Cache: dataset/_cache/I3D/transcripts/\n",
    "Translation Cache: dataset/_cache/I3D/translation_cache.json\n",
    "ðŸŽ¯ Features Extracted\n",
    "Text Features (per language)\n",
    "Character count\n",
    "Word count\n",
    "Sentiment analysis (âš ï¸ Indonesian: proxy only, use English version)\n",
    "Subjectivity score\n",
    "Complexity score\n",
    "Lexical diversity\n",
    "Number Features (7)\n",
    "Has numbers (boolean)\n",
    "Number count\n",
    "Has vague quantifiers\n",
    "Has exact quantifiers\n",
    "Number-to-word ratio\n",
    "Has large numbers (>1000)\n",
    "Has decimal numbers\n",
    "Audio Features (94) âœ… VERIFIED\n",
    "Breakdown:\n",
    "\n",
    "MFCC: 13 coefficients Ã— 2 stats (mean, std) = 26\n",
    "Delta MFCC: 13 Ã— 2 = 26\n",
    "Delta-Delta MFCC: 13 Ã— 2 = 26\n",
    "Bark Energy: 2 (mean, std)\n",
    "Delta Energy: 2\n",
    "Delta-Delta Energy: 2\n",
    "Spectral Centroid: 2 (mean, std)\n",
    "Spectral Bandwidth: 2\n",
    "Spectral Rolloff: 2\n",
    "Zero-Crossing Rate: 2\n",
    "Chroma: 2\n",
    "Total: 26 + 26 + 26 + 2 + 2 + 2 + 2 + 2 + 2 + 2 + 2 = 94 features âœ…\n",
    "Pause Features (17)\n",
    "Number of pauses\n",
    "Total pause duration\n",
    "Average pause duration\n",
    "Longest pause duration\n",
    "Pause frequency\n",
    "Speech duration\n",
    "Speech rate\n",
    "Pause ratio\n",
    "Early/middle/late pauses (3)\n",
    "Pause standard deviation\n",
    "Pause variability\n",
    "Hesitation score\n",
    "Boolean flags (3): long pauses, frequent pauses, high pause ratio\n",
    "Visual Features âœ… VERIFIED\n",
    "Face Mesh: 468 facial landmarks\n",
    "Iris: 10 iris landmarks (indices 468-477)\n",
    "Total Face/Iris: 478 landmarks\n",
    "Pose: 33 body keypoints\n",
    "Coordinates: 3D (x, y, z) for each landmark\n",
    "Missing values: Filled with (0.0, 0.0, 0.0) - NOT actual position\n",
    "âš™ï¸ Processing Configuration\n",
    "Audio Enhancement (Deterministic)\n",
    "Sampling rate: 16000 Hz\n",
    "Fixed boost thresholds: -55, -45, -35 dBFS\n",
    "Fixed boost amounts: 40, 35, 20 dB\n",
    "Target dBFS: -18 (fixed)\n",
    "Silence threshold: -50 dBFS (fixed)\n",
    "Compression ratio: 6.0 (normal), 10.0 (ultra)\n",
    "Filters: Highpass 80Hz, Lowpass 3000Hz\n",
    "Video Preprocessing\n",
    "Multi-level zoom: 1.5x to 10x\n",
    "CLAHE enhancement: clip_limit=4.0, tile_size=(4,4)\n",
    "Gamma correction: 1.4\n",
    "Unsharp masking: strength=1.8\n",
    "Eye region enhancement: For iris detection\n",
    "Speech Recognition\n",
    "Language: Indonesian (id-ID)\n",
    "Max attempts: 7 (ultra mode), 3 (normal mode)\n",
    "Progressive energy threshold: 300 â†’ 10\n",
    "Dynamic pause threshold: 0.8 â†’ 0.1\n",
    "Transcript caching: âœ… Enabled\n",
    "Manual transcript fallback: âœ… Enabled\n",
    "Energy threshold logging: âœ… Per transcript\n",
    "Librosa (Explicit Parameters)\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "win_length = 2048\n",
    "window = 'hann'\n",
    "center = True\n",
    "pad_mode = 'constant'\n",
    "MediaPipe (Explicit Parameters)\n",
    "face_mesh = FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,  # Enables iris (10 landmarks)\n",
    "    min_detection_confidence=0.2,\n",
    "    min_tracking_confidence=0.2\n",
    ")\n",
    "\n",
    "pose = Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.3,\n",
    "    min_tracking_confidence=0.3\n",
    ")\n",
    "ðŸ“ˆ Quality Metrics\n",
    "Audio Quality Checks\n",
    "RMS (Root Mean Square)\n",
    "SNR (Signal-to-Noise Ratio) - STFT-based with explicit n_fft=2048, hop_length=512\n",
    "Zero-crossing rate\n",
    "Spectral centroid\n",
    "Dynamic range\n",
    "Overall quality score (0.0 to 1.0)\n",
    "Landmark Quality (per video)\n",
    "Face detection rate\n",
    "Iris detection rate\n",
    "Pose detection rate\n",
    "Strategy success distribution\n",
    "âš ï¸ Failed Samples\n",
    "Total Failed: {len(stats.get('errors', []))}\n",
    "\n",
    "Failed samples saved in: dataset/processed/I3D/reextraction/failed_samples.csv\n",
    "\n",
    "ðŸ’¡ Usage Notes for Researchers\n",
    "1. Filename Label Leakage\n",
    "âš ï¸ WARNING: Filenames contain label prefix (LIE_ / TRUTH_)\n",
    "\n",
    "Before ML training:\n",
    "# Option 1: Remove filename\n",
    "df = df.drop(columns=['filename'])\n",
    "\n",
    "# Option 2: Encode filename\n",
    "import hashlib\n",
    "df['filename_encoded'] = df['filename'].apply(\n",
    "    lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    ")\n",
    "2. Sentiment Analysis Caveat\n",
    "âš ï¸ Indonesian sentiment scores are PROXY ONLY\n",
    "\n",
    "TextBlob uses English-based model\n",
    "For Indonesian, use only English translation sentiment (sentiment_en)\n",
    "Or use Indonesian-specific sentiment model\n",
    "3. Missing Landmark Values\n",
    "âš ï¸ Missing landmarks filled with (0.0, 0.0, 0.0)\n",
    "\n",
    "Do NOT interpret (0.0, 0.0, 0.0) as actual landmark position\n",
    "Filter out or impute before analysis:\n",
    "# Filter out missing landmarks\n",
    "def is_valid_landmark(x, y, z):\n",
    "    return not (x == 0.0 and y == 0.0 and z == 0.0)\n",
    "\n",
    "# Or check if all landmarks in frame are missing\n",
    "# Filter out missing landmarks\n",
    "def is_valid_landmark(x, y, z):\n",
    "    return not (x == 0.0 and y == 0.0 and z == 0.0)\n",
    "\n",
    "# Or check if all landmarks in frame are missing\n",
    "4. Re-running Pipeline\n",
    "For deterministic results:\n",
    "\n",
    "Keep cache directories intact\n",
    "Set force_retranscribe: false in config\n",
    "Same input + cache â†’ identical output\n",
    "For fresh extraction:\n",
    "\n",
    "Delete cache directories\n",
    "Or set force_retranscribe: true\n",
    "Results will differ due to external APIs\n",
    "ðŸ“š Documentation Files\n",
    "For Scientific Data Submission\n",
    "Data Dictionary: data_dictionary.csv - Complete column descriptions\n",
    "Run Manifest: run_manifest.json - Full reproducibility info\n",
    "Processing Summary: processing_summary.csv - Per-file quality metrics\n",
    "Decision Log: decision_log.jsonl - All processing decisions\n",
    "This Report: I3D_EXTRACTION_REPORT.md - Overview\n",
    "For Methods Section\n",
    "Librosa parameters: See run_manifest.json\n",
    "STT configuration: See run_manifest.json\n",
    "Enhancement parameters: See run_manifest.json\n",
    "MediaPipe configuration: See run_manifest.json\n",
    "Environment: See run_manifest.json\n",
    "For Technical Validation\n",
    "Processing summary: processing_summary.csv\n",
    "Quality reports: dataset/validation/I3D/quality_reports/\n",
    "Statistical analyses: dataset/validation/I3D/statistical_analyses/\n",
    "Decision log: decision_log.jsonl\n",
    "ðŸ”§ Configuration Options\n",
    "Force Re-transcription\n",
    "text:\n",
    "  force_retranscribe: true  # Ignore transcript cache\n",
    "Deterministic STT Mode\n",
    "speech_recognition:\n",
    "  deterministic_mode: true  # Disable dynamic_energy_threshold\n",
    "Disable Caching\n",
    "text:\n",
    "  use_transcript_cache: false\n",
    "  use_translation_cache: false\n",
    "ðŸ“– Citation\n",
    "{env_info.get('citation', '[Your BibTeX Citation]')}\n",
    "ðŸ“§ Contact\n",
    "See paper for contact information and data availability details.\n",
    "\n",
    "ðŸ·ï¸ Version Info\n",
    "Pipeline Version: {VERSION}\n",
    "Pipeline Name: {PIPELINE_NAME}\n",
    "Report Generated: {datetime.now().isoformat()}\n",
    "Python: {env_info['python_version'].split()[0]}\n",
    "Platform: {env_info['platform']['system']} {env_info['platform']['release']}\n",
    "Generated by {PIPELINE_NAME} v{VERSION} - Production-Ready for Scientific Data\n",
    "\"\"\"\n",
    "    report_path = paths.paths['processed'] / 'I3D_EXTRACTION_REPORT.md'\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "\n",
    "    return report_path\n",
    "\n",
    "# ==================== MAIN PROCESSOR ====================\n",
    "class I3DProcessor:\n",
    "    \"\"\"Main processor for I3D Dataset - Scientific Data Ready\"\"\"\n",
    "    \n",
    "    def __init__(self, config: I3DConfig, paths: I3DPathManager):\n",
    "        self.config = config\n",
    "        self.paths = paths\n",
    "        self.checkpoint_manager = CheckpointManager(paths.paths['checkpoints'])\n",
    "        self.transcript_cache = TranscriptCacheManager(paths.paths['cache'])\n",
    "        self.translation_cache = TranslationCacheManager(paths.paths['cache'])\n",
    "        self.decision_log = DecisionLogManager(paths.paths['decision_logs'])\n",
    "        self.env_info = capture_environment()\n",
    "        self.status_by_file = {}  # {filename: {step: True/False}}   \n",
    "    \n",
    "    def _update_file_status(self, filename: str, step: str, success: bool):\n",
    "        \"\"\"Update status for a specific processing step\"\"\"\n",
    "        if filename not in self.status_by_file:\n",
    "            self.status_by_file[filename] = {\n",
    "                'wav_conversion': False,\n",
    "                'transcript_extraction': False,\n",
    "                'translation': False,\n",
    "                'audio_features': False,\n",
    "                'pause_features': False,\n",
    "                'landmarks': False,\n",
    "                'completed': False\n",
    "            }\n",
    "        \n",
    "        self.status_by_file[filename][step] = success\n",
    "        \n",
    "        # Check if all steps completed\n",
    "        required_steps = ['wav_conversion', 'transcript_extraction', 'audio_features', 'landmarks']\n",
    "        all_complete = all(self.status_by_file[filename].get(s, False) for s in required_steps)\n",
    "        \n",
    "        # Translation is optional (can fail but file still considered complete)\n",
    "        self.status_by_file[filename]['completed'] = all_complete\n",
    "\n",
    "    def _get_file_status(self, filename: str, step: str) -> bool:\n",
    "        \"\"\"Check if a specific step was completed for a file\"\"\"\n",
    "        return self.status_by_file.get(filename, {}).get(step, False)\n",
    "\n",
    "    def _is_file_completed(self, filename: str) -> bool:\n",
    "        \"\"\"Check if all required steps are completed for a file\"\"\"\n",
    "        return self.status_by_file.get(filename, {}).get('completed', False)\n",
    "\n",
    "    def _convert_to_wav(self, video_path: Path, label_name: str, logger: I3DLogger) -> Optional[Path]:\n",
    "        \"\"\"Convert video to WAV\"\"\"\n",
    "        if label_name == 'lie':\n",
    "            output_dir = self.paths.paths['audio_wav_lie']\n",
    "        else:\n",
    "            output_dir = self.paths.paths['audio_wav_truth']\n",
    "        \n",
    "        wav_path = output_dir / f\"{video_path.stem}.wav\"\n",
    "        \n",
    "        if wav_path.exists():\n",
    "            logger.log(f\"WAV already exists: {wav_path.name}\")\n",
    "            return wav_path\n",
    "        \n",
    "        try:\n",
    "            subprocess.run([\n",
    "                \"ffmpeg\", \"-i\", str(video_path),\n",
    "                \"-vn\",\n",
    "                \"-acodec\", \"pcm_s16le\",\n",
    "                \"-ar\", \"16000\",\n",
    "                \"-ac\", \"1\",\n",
    "                \"-af\", \"highpass=f=80,lowpass=f=3000,volume=3.0,afftdn=nf=-25,equalizer=f=1000:width_type=h:width=200:g=3\",\n",
    "                \"-y\", str(wav_path)\n",
    "            ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            \n",
    "            logger.log(f\"âœ“ Converted to WAV: {video_path.name}\")\n",
    "            return wav_path\n",
    "        \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.log_error(f\"Error converting {video_path}: {e}\")\n",
    "            return None\n",
    "        except FileNotFoundError:\n",
    "            logger.log_error(\"ffmpeg not found. Please install ffmpeg.\")\n",
    "            return None\n",
    "    \n",
    "    def _save_datasets(self, data: List[Dict], logger: I3DLogger) -> Dict:\n",
    "        \"\"\"Save all datasets\"\"\"\n",
    "        if not data:\n",
    "            logger.log_warning(\"âš ï¸ No data to save\")\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        output_paths = {}\n",
    "        \n",
    "        # DATASET 1: Text (Indonesian)\n",
    "        text_cols = ['filename', 'text_indonesian_original', 'text_indonesian_normalized', \n",
    "                     'label', 'dataset', 'transcript_source', 'transcript_from_cache',\n",
    "                     'transcript_attempt', 'transcript_mode', 'stt_energy_threshold',\n",
    "                     'char_count_id', 'word_count_id',\n",
    "                     'sentiment_id', 'subjectivity_id', 'complexity_id', 'lexical_diversity_id']\n",
    "        df_text_id = df[text_cols].copy()\n",
    "        \n",
    "        text_id_path = self.paths.paths['text'] / 'TextDataset_Indonesian.csv'\n",
    "        df_text_id.to_csv(text_id_path, index=False, encoding='utf-8')\n",
    "        output_paths['text_indonesian'] = str(text_id_path)\n",
    "        logger.log(f\"âœ“ Text (Indonesian) saved: {text_id_path}\")\n",
    "        \n",
    "        # DATASET 2: Text (English)\n",
    "        text_en_cols = ['filename', 'text_english', 'label', 'dataset', 'transcript_source',\n",
    "                        'char_count_en', 'word_count_en',\n",
    "                        'sentiment_en', 'subjectivity_en', 'complexity_en', 'lexical_diversity_en']\n",
    "        df_text_en = df[text_en_cols].copy()\n",
    "        \n",
    "        text_en_path = self.paths.paths['text'] / 'TextDataset_English.csv'\n",
    "        df_text_en.to_csv(text_en_path, index=False, encoding='utf-8')\n",
    "        output_paths['text_english'] = str(text_en_path)\n",
    "        logger.log(f\"âœ“ Text (English) saved: {text_en_path}\")\n",
    "        \n",
    "        # DATASET 3: Number Features\n",
    "        number_cols = ['filename', 'label', 'dataset', 'has_numbers', 'number_count',\n",
    "                       'has_vague_quantifiers', 'has_exact_quantifiers', 'number_word_ratio',\n",
    "                       'has_large_numbers', 'has_decimal_numbers']\n",
    "        df_numbers = df[number_cols].copy()\n",
    "        \n",
    "        numbers_path = self.paths.paths['text'] / 'NumberFeatures.csv'\n",
    "        df_numbers.to_csv(numbers_path, index=False, encoding='utf-8')\n",
    "        output_paths['number_features'] = str(numbers_path)\n",
    "        logger.log(f\"âœ“ Number features saved: {numbers_path}\")\n",
    "        \n",
    "        # DATASET 4: Audio Features (94)\n",
    "        audio_feature_names = create_audio_feature_names()\n",
    "        audio_cols = ['filename', 'label', 'dataset'] + audio_feature_names + [\n",
    "            'audio_quality_score', 'audio_duration', 'audio_snr', 'audio_snr_computation',\n",
    "            'audio_rms', 'audio_spectral_centroid', 'audio_dynamic_range'\n",
    "        ]\n",
    "        df_audio = df[audio_cols].copy()\n",
    "        \n",
    "        audio_path = self.paths.paths['audio'] / 'AudioDataset_Features.csv'\n",
    "        df_audio.to_csv(audio_path, index=False, encoding='utf-8')\n",
    "        output_paths['audio_features'] = str(audio_path)\n",
    "        logger.log(f\"âœ“ Audio features (94) saved: {audio_path}\")\n",
    "        \n",
    "        # DATASET 5: Pause Features\n",
    "        pause_cols = ['filename', 'label', 'dataset'] + [col for col in df.columns if col.startswith('pause_')]\n",
    "        df_pause = df[pause_cols].copy()\n",
    "        \n",
    "        pause_path = self.paths.paths['audio'] / 'PauseFeatures.csv'\n",
    "        df_pause.to_csv(pause_path, index=False, encoding='utf-8')\n",
    "        output_paths['pause_features'] = str(pause_path)\n",
    "        logger.log(f\"âœ“ Pause features saved: {pause_path}\")\n",
    "        \n",
    "        # DATASET 6: Multimodal (Full)\n",
    "        multimodal_path = self.paths.paths['multimodal'] / 'MultimodalDataset_Full.csv'\n",
    "        df.to_csv(multimodal_path, index=False, encoding='utf-8')\n",
    "        output_paths['multimodal_full'] = str(multimodal_path)\n",
    "        logger.log(f\"âœ“ Multimodal (Full) saved: {multimodal_path}\")\n",
    "        \n",
    "        # DATASET 7: Publication-Ready\n",
    "        publication_cols = [\n",
    "            'filename', 'label', 'dataset', 'transcript_source', 'transcript_from_cache',\n",
    "            'reextraction_triggered', 'reextraction_success',\n",
    "            'text_indonesian_normalized', 'text_english',\n",
    "            'word_count_id', 'sentiment_en', 'lexical_diversity_id',\n",
    "            'has_vague_quantifiers', 'has_exact_quantifiers', 'number_count',\n",
    "            'mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean', 'spectral_centroid_mean',\n",
    "            'audio_quality_score', 'audio_snr',\n",
    "            'pause_hesitation_score', 'pause_pause_frequency', 'pause_pause_ratio'\n",
    "        ]\n",
    "        publication_cols = [col for col in publication_cols if col in df.columns]\n",
    "        df_publication = df[publication_cols].copy()\n",
    "        \n",
    "        publication_path = self.paths.paths['multimodal'] / 'PublicationDataset.csv'\n",
    "        df_publication.to_csv(publication_path, index=False, encoding='utf-8')\n",
    "        output_paths['publication'] = str(publication_path)\n",
    "        logger.log(f\"âœ“ Publication dataset saved: {publication_path}\")\n",
    "        \n",
    "        # DATASET 8: Landmark\n",
    "        landmark_path = self.paths.paths['visual'] / 'LandmarkDataset.csv'\n",
    "        output_paths['landmark'] = str(landmark_path)\n",
    "        logger.log(f\"âœ“ Landmarks saved: {landmark_path}\")\n",
    "        \n",
    "        return output_paths\n",
    "    \n",
    "    def _generate_statistics(self, data: List[Dict], stats: Dict, logger: I3DLogger):\n",
    "        \"\"\"Generate statistics and visualizations\"\"\"\n",
    "        logger.log(\"\\nðŸ“Š Generating statistics...\")\n",
    "        \n",
    "        # Save processing statistics\n",
    "        stats_path = self.paths.paths['statistical'] / 'processing_statistics.json'\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(convert_numpy_types(stats), f, indent=2)\n",
    "        logger.log(f\"âœ“ Statistics saved: {stats_path}\")\n",
    "        \n",
    "        # Generate visualizations\n",
    "        if self.config.get('output.generate_figures', True):\n",
    "            self._generate_visualizations(data, stats, logger)\n",
    "    \n",
    "    def _generate_visualizations(self, data: List[Dict], stats: Dict, logger: I3DLogger):\n",
    "        \"\"\"Generate visualizations\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        figures_dir = self.paths.paths['exploratory_figs']\n",
    "        dpi = self.config.get('output.figure_dpi', 300)\n",
    "        \n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        # Visualization 1: Class Distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        label_counts = df['label'].value_counts()\n",
    "        labels = ['TRUTH (0)', 'LIE (1)']\n",
    "        colors = ['#3498db', '#e74c3c']\n",
    "        \n",
    "        plt.bar(labels, [label_counts.get(0, 0), label_counts.get(1, 0)], \n",
    "                color=colors, alpha=0.8, edgecolor='black')\n",
    "        plt.title('Class Distribution - I3D Dataset', fontsize=16, fontweight='bold')\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate([label_counts.get(0, 0), label_counts.get(1, 0)]):\n",
    "            plt.text(i, v + 0.5, str(v), ha='center', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figures_dir / 'class_distribution.png', dpi=dpi, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Visualization 2: Transcript Sources\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        source_counts = df['transcript_source'].value_counts()\n",
    "        plt.bar(source_counts.index, source_counts.values, color='skyblue', edgecolor='black')\n",
    "        plt.title('Transcript Sources Distribution', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Source', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figures_dir / 'transcript_sources.png', dpi=dpi, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        logger.log(f\"âœ“ Visualizations saved: {figures_dir}\")\n",
    "\n",
    "    def process(self, checkpoint_data: Optional[Dict] = None):\n",
    "        \"\"\"Process I3D dataset with full reproducibility and smart resume\"\"\"\n",
    "        \n",
    "        # Initialize logger\n",
    "        log_path, timestamp = self.paths.get_log_path()\n",
    "        logger = I3DLogger(log_path)\n",
    "        \n",
    "        logger.log(\"=\"*70)\n",
    "        logger.log(f\"ðŸŽ¯ {PIPELINE_NAME} v{VERSION}\")\n",
    "        logger.log(\"=\"*70)\n",
    "        logger.log(\"âœ¨ Scientific Data Ready Features:\")\n",
    "        logger.log(\"   â€¢ Fixed 94 audio features (verified)\")\n",
    "        logger.log(\"   â€¢ MediaPipe: 478 face (468+10 iris) + 33 pose\")\n",
    "        logger.log(\"   â€¢ Runtime validation for landmark counts\")\n",
    "        logger.log(\"   â€¢ Explicit librosa parameters\")\n",
    "        logger.log(\"   â€¢ Deterministic mode available\")\n",
    "        logger.log(\"   â€¢ Complete data dictionary\")\n",
    "        logger.log(\"   â€¢ Run manifest for reproducibility\")\n",
    "        logger.log(\"   â€¢ Per-file processing summary\")\n",
    "        logger.log(\"   â€¢ Decision logging (JSONL)\")\n",
    "        logger.log(\"   â€¢ Environment capture\")\n",
    "        logger.log(\"   â€¢ Ethics & privacy notes\")\n",
    "        logger.log(\"   â€¢ âœ… Smart resume (retry failed steps only)\")\n",
    "        logger.log(\"=\"*70)\n",
    "        \n",
    "        # Log environment\n",
    "        logger.log(f\"\\nðŸ–¥ï¸ Environment:\")\n",
    "        logger.log(f\"   Python: {self.env_info['python_version'].split()[0]}\")\n",
    "        logger.log(f\"   Platform: {self.env_info['platform']['system']} {self.env_info['platform']['release']}\")\n",
    "        logger.log(f\"   NumPy: {self.env_info['packages'].get('numpy', 'N/A')}\")\n",
    "        logger.log(f\"   Librosa: {self.env_info['packages'].get('librosa', 'N/A')}\")\n",
    "        logger.log(f\"   MediaPipe: {self.env_info['packages'].get('mediapipe', 'N/A')}\")\n",
    "        \n",
    "        # Check source folders\n",
    "        if not self.paths.paths['source_lie'].exists():\n",
    "            raise FileNotFoundError(f\"LIE folder not found: {self.paths.paths['source_lie']}\")\n",
    "        if not self.paths.paths['source_truth'].exists():\n",
    "            raise FileNotFoundError(f\"TRUTH folder not found: {self.paths.paths['source_truth']}\")\n",
    "        \n",
    "        # Get video files\n",
    "        lie_files = get_valid_video_files(self.paths.paths['source_lie'])\n",
    "        truth_files = get_valid_video_files(self.paths.paths['source_truth'])\n",
    "        total_files = len(lie_files) + len(truth_files)\n",
    "        all_video_files = lie_files + truth_files\n",
    "        \n",
    "        logger.log(f\"\\nðŸ“ Files found:\")\n",
    "        logger.log(f\"   - LIE   : {len(lie_files)} files\")\n",
    "        logger.log(f\"   - TRUTH : {len(truth_files)} files\")\n",
    "        logger.log(f\"   - TOTAL : {total_files} files\")\n",
    "        \n",
    "        # Count manual transcripts and cached transcripts\n",
    "        manual_available = sum(1 for f in all_video_files if f in MANUAL_TRANSCRIPTS)\n",
    "        cached_available = sum(1 for f in all_video_files if self.transcript_cache.exists(f))\n",
    "        \n",
    "        logger.log(f\"\\nðŸ“ Transcript sources:\")\n",
    "        logger.log(f\"   - Manual transcripts : {manual_available}/{total_files}\")\n",
    "        logger.log(f\"   - Cached transcripts : {cached_available}/{total_files}\")\n",
    "        logger.log(f\"   - Translation cache  : {self.translation_cache.get_cache_size()} entries\")\n",
    "        \n",
    "        force_retranscribe = self.config.get('text.force_retranscribe', False)\n",
    "        deterministic_mode = self.config.get('speech_recognition.deterministic_mode', False)\n",
    "        \n",
    "        if force_retranscribe:\n",
    "            logger.log(f\"\\nâš ï¸  FORCE RETRANSCRIBE enabled - ignoring transcript cache\")\n",
    "        if deterministic_mode:\n",
    "            logger.log(f\"\\nðŸ”’ DETERMINISTIC MODE enabled - dynamic_energy_threshold=False\")\n",
    "        \n",
    "        if total_files == 0:\n",
    "            raise ValueError(\"No video files found!\")\n",
    "        \n",
    "        # Initialize or restore state\n",
    "        if checkpoint_data:\n",
    "            stats = checkpoint_data.get('stats', {})\n",
    "            text_audio_data = checkpoint_data.get('text_audio_data', [])\n",
    "            failed_samples = checkpoint_data.get('failed_samples', [])\n",
    "            processed_files = set(checkpoint_data.get('processed_files', []))\n",
    "            \n",
    "            # âœ… NEW: Restore status tracking\n",
    "            self.status_by_file = checkpoint_data.get('status_by_file', {})\n",
    "            \n",
    "            logger.log(f\"âœ“ Resumed: {len(processed_files)} files already processed\")\n",
    "            \n",
    "            # âœ… NEW: Count partial vs complete\n",
    "            complete_count = sum(1 for s in self.status_by_file.values() if s.get('completed', False))\n",
    "            partial_count = len(processed_files) - complete_count\n",
    "            \n",
    "            if partial_count > 0:\n",
    "                logger.log(f\"  â””â”€ {complete_count} fully completed, {partial_count} partial (will retry failed steps)\")\n",
    "        else:\n",
    "            # Initialize statistics\n",
    "            stats = {\n",
    "                'total_files': total_files,\n",
    "                'successful_text_conversions': 0,\n",
    "                'failed_text_conversions': 0,\n",
    "                'successful_audio_extractions': 0,\n",
    "                'failed_audio_extractions': 0,\n",
    "                'successful_translations': 0,\n",
    "                'failed_translations': 0,\n",
    "                'successful_landmark_extractions': 0,\n",
    "                'failed_landmark_extractions': 0,\n",
    "                'texts_with_numbers': 0,\n",
    "                'numbers_normalized': 0,\n",
    "                'total_frames_processed': 0,\n",
    "                'face_detected': 0,\n",
    "                'iris_detected': 0,\n",
    "                'pose_detected': 0,\n",
    "                'pause_features_extracted': 0,\n",
    "                'reextraction_triggered': 0,\n",
    "                'reextraction_successful': 0,\n",
    "                'manual_transcripts_used': 0,\n",
    "                'cached_transcripts_used': 0,\n",
    "                'cached_translations_used': 0,\n",
    "                'landmark_count_warnings': 0,\n",
    "                'errors': []\n",
    "            }\n",
    "            text_audio_data = []\n",
    "            failed_samples = []\n",
    "            processed_files = set()\n",
    "            self.status_by_file = {}  # âœ… NEW\n",
    "        \n",
    "        # Initialize processors\n",
    "        preprocessor = VideoPreprocessor(self.config)\n",
    "        audio_processor = AudioProcessor(self.config)\n",
    "        landmark_extractor = LandmarkExtractor(self.config, preprocessor)\n",
    "        \n",
    "        # MediaPipe configuration\n",
    "        face_mesh_config = self.config.get('mediapipe.face_mesh', {})\n",
    "        pose_config = self.config.get('mediapipe.pose', {})\n",
    "        \n",
    "        # CSV for landmarks\n",
    "        landmark_csv_path = self.paths.paths['visual'] / 'LandmarkDataset.csv'\n",
    "        \n",
    "        # Check if we're resuming and CSV already exists\n",
    "        if checkpoint_data and landmark_csv_path.exists():\n",
    "            landmark_csv = open(landmark_csv_path, 'a', newline='', encoding='utf-8')\n",
    "            landmark_writer = csv.writer(landmark_csv)\n",
    "        else:\n",
    "            landmark_csv = open(landmark_csv_path, 'w', newline='', encoding='utf-8')\n",
    "            landmark_writer = csv.writer(landmark_csv)\n",
    "            \n",
    "            # Landmark header\n",
    "            header = ['Video_Name', 'Frame']\n",
    "            for i in range(478):  # 468 face + 10 iris\n",
    "                header.extend([f'Landmark_{i}_X', f'Landmark_{i}_Y', f'Landmark_{i}_Z'])\n",
    "            for i in range(33):  # 33 pose\n",
    "                header.extend([f'Pose_{i}_X', f'Pose_{i}_Y', f'Pose_{i}_Z'])\n",
    "            header.append('Class')\n",
    "            landmark_writer.writerow(header)\n",
    "        \n",
    "        # Process each label folder\n",
    "        start_time = datetime.now()\n",
    "        files_processed = len(processed_files)\n",
    "        \n",
    "        try:\n",
    "            for label_name, folder in [('lie', self.paths.paths['source_lie']), \n",
    "                                        ('truth', self.paths.paths['source_truth'])]:\n",
    "                files = get_valid_video_files(folder)\n",
    "                \n",
    "                logger.log(f\"\\nðŸ“ Processing {len(files)} files from {label_name}\")\n",
    "                \n",
    "                # Open MediaPipe context\n",
    "                with mp_face_mesh.FaceMesh(**face_mesh_config) as face_mesh, \\\n",
    "                    mp_pose.Pose(**pose_config) as pose:\n",
    "                    \n",
    "                    # ========================================\n",
    "                    # âœ… SMART RESUME LOOP (UPDATED)\n",
    "                    # ========================================\n",
    "                    \n",
    "                    # Process each file\n",
    "                    for filename in tqdm(files, desc=f\"Processing {label_name}\", unit=\"file\"):\n",
    "                        # âœ… SMART RESUME: Check completion status\n",
    "                        if filename in processed_files:\n",
    "                            if self._is_file_completed(filename):\n",
    "                                logger.log(f\"â­ï¸  Skipping (fully completed): {filename}\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                logger.log(f\"ðŸ”„ Resuming (partial completion): {filename}\")\n",
    "                                status = self.status_by_file.get(filename, {})\n",
    "                                logger.log(f\"  â””â”€ Previous status: {status}\")\n",
    "                        \n",
    "                        # Per-file flags\n",
    "                        file_reextraction_triggered = False\n",
    "                        file_reextraction_success = False\n",
    "                        reextraction_params = None\n",
    "                        \n",
    "                        try:\n",
    "                            video_path = folder / filename\n",
    "                            video_name = video_path.stem\n",
    "                            \n",
    "                            if filename not in processed_files:\n",
    "                                logger.log(f\"\\nðŸŽ¬ Processing: {filename}\")\n",
    "                            \n",
    "                            # Get label\n",
    "                            label = 1 if label_name == 'lie' else 0\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 1: Convert to WAV\n",
    "                            # ========================================\n",
    "                            wav_path = None\n",
    "                            \n",
    "                            if self._get_file_status(filename, 'wav_conversion'):\n",
    "                                logger.log(f\"  âœ“ WAV conversion: SKIP (already done)\")\n",
    "                                # Reconstruct WAV path\n",
    "                                if label_name == 'lie':\n",
    "                                    wav_path = self.paths.paths['audio_wav_lie'] / f\"{video_name}.wav\"\n",
    "                                else:\n",
    "                                    wav_path = self.paths.paths['audio_wav_truth'] / f\"{video_name}.wav\"\n",
    "                                \n",
    "                                if not wav_path.exists():\n",
    "                                    logger.log_warning(f\"  âš ï¸ WAV file missing, will retry\")\n",
    "                                    self._update_file_status(filename, 'wav_conversion', False)\n",
    "                                    wav_path = None\n",
    "                            \n",
    "                            if wav_path is None:\n",
    "                                logger.log(f\"  ðŸ”„ WAV conversion: PROCESSING\")\n",
    "                                wav_path = self._convert_to_wav(video_path, label_name, logger)\n",
    "                                \n",
    "                                if wav_path and wav_path.exists():\n",
    "                                    self._update_file_status(filename, 'wav_conversion', True)\n",
    "                                    logger.log(f\"  âœ“ WAV conversion: SUCCESS\")\n",
    "                                else:\n",
    "                                    self._update_file_status(filename, 'wav_conversion', False)\n",
    "                                    logger.log_error(f\"  âŒ WAV conversion: FAILED\")\n",
    "                                    stats['failed_text_conversions'] += 1\n",
    "                                    stats['failed_audio_extractions'] += 1\n",
    "                                    failed_samples.append({'filename': filename, 'reason': 'WAV conversion failed', 'label': label})\n",
    "                                    processed_files.add(filename)\n",
    "                                    continue\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 2: Extract text\n",
    "                            # ========================================\n",
    "                            text = None\n",
    "                            text_success = False\n",
    "                            text_metadata = {}\n",
    "                            \n",
    "                            if self._get_file_status(filename, 'transcript_extraction'):\n",
    "                                logger.log(f\"  âœ“ Transcript: SKIP (already done)\")\n",
    "                                # Load from cache\n",
    "                                cached = self.transcript_cache.load_transcript(filename)\n",
    "                                if cached:\n",
    "                                    text = cached['text']\n",
    "                                    text_success = True\n",
    "                                    text_metadata = {\n",
    "                                        'source': cached['source'],\n",
    "                                        'from_cache': True,\n",
    "                                        'attempt': cached.get('attempt', 0),\n",
    "                                        'mode': cached.get('mode', 'normal'),\n",
    "                                        'stt_energy_threshold': cached.get('stt_energy_threshold', 0.0)\n",
    "                                    }\n",
    "                                    logger.log(f\"  â””â”€ âœ“ Loaded from cache: '{text[:50]}...'\")\n",
    "                                else:\n",
    "                                    logger.log_warning(f\"  âš ï¸ Transcript cache missing, will retry\")\n",
    "                                    self._update_file_status(filename, 'transcript_extraction', False)\n",
    "                            \n",
    "                            if not text_success:\n",
    "                                logger.log(f\"  ðŸ”„ Transcript: PROCESSING\")\n",
    "                                text, text_success, text_metadata = audio_processor.extract_text(\n",
    "                                    wav_path, logger, \n",
    "                                    ultra_mode=False, \n",
    "                                    filename=filename,\n",
    "                                    transcript_cache=self.transcript_cache,\n",
    "                                    decision_log=self.decision_log,\n",
    "                                    force_retranscribe=force_retranscribe\n",
    "                                )\n",
    "                                \n",
    "                                if text_success:\n",
    "                                    self._update_file_status(filename, 'transcript_extraction', True)\n",
    "                                    logger.log(f\"  âœ“ Transcript: SUCCESS\")\n",
    "                                    \n",
    "                                    if text_metadata['from_cache']:\n",
    "                                        stats['cached_transcripts_used'] += 1\n",
    "                                    if text_metadata['source'] == 'manual':\n",
    "                                        stats['manual_transcripts_used'] += 1\n",
    "                                else:\n",
    "                                    self._update_file_status(filename, 'transcript_extraction', False)\n",
    "                                    logger.log_warning(f\"  âš ï¸ Transcript: FAILED (will try re-extraction)\")\n",
    "                            \n",
    "                            # STEP 3: Re-extraction if failed\n",
    "                            if not text_success and self.config.get('reextraction.enabled', True):\n",
    "                                logger.log(f\"  â””â”€ ðŸ”„ Triggering re-extraction (ultra mode)...\")\n",
    "                                stats['reextraction_triggered'] += 1\n",
    "                                file_reextraction_triggered = True\n",
    "                                \n",
    "                                # Get audio dBFS for decision logging\n",
    "                                try:\n",
    "                                    audio = AudioSegment.from_wav(wav_path)\n",
    "                                    audio_dbfs = audio.dBFS\n",
    "                                except:\n",
    "                                    audio_dbfs = 0.0\n",
    "                                \n",
    "                                # Enhance audio (ultra mode) with fixed parameters\n",
    "                                enhanced_path, enhancement_params = audio_processor.enhance_audio(\n",
    "                                    wav_path, logger, ultra_mode=True\n",
    "                                )\n",
    "                                \n",
    "                                reextraction_params = enhancement_params\n",
    "                                \n",
    "                                # Extract text (ultra mode)\n",
    "                                text, text_success, text_metadata = audio_processor.extract_text(\n",
    "                                    enhanced_path, logger, \n",
    "                                    ultra_mode=True, \n",
    "                                    filename=filename,\n",
    "                                    transcript_cache=self.transcript_cache,\n",
    "                                    decision_log=self.decision_log,\n",
    "                                    force_retranscribe=False\n",
    "                                )\n",
    "                                \n",
    "                                # Log re-extraction decision\n",
    "                                if self.config.get('reextraction.log_decisions', True):\n",
    "                                    self.decision_log.log_reextraction(\n",
    "                                        filename=filename,\n",
    "                                        reason='text_extraction_failed',\n",
    "                                        audio_dbfs=audio_dbfs,\n",
    "                                        params=enhancement_params,\n",
    "                                        success=text_success\n",
    "                                    )\n",
    "                                \n",
    "                                if text_success:\n",
    "                                    self._update_file_status(filename, 'transcript_extraction', True)\n",
    "                                    stats['reextraction_successful'] += 1\n",
    "                                    file_reextraction_success = True\n",
    "                                    logger.log(f\"  â””â”€ âœ… Re-extraction successful!\")\n",
    "                            \n",
    "                            # Final transcript status\n",
    "                            if text_success:\n",
    "                                stats['successful_text_conversions'] += 1\n",
    "                            else:\n",
    "                                stats['failed_text_conversions'] += 1\n",
    "                                failed_samples.append({\n",
    "                                    'filename': filename, \n",
    "                                    'reason': 'Text extraction failed', \n",
    "                                    'label': label\n",
    "                                })\n",
    "                                text = \"\"\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 4: Extract number features\n",
    "                            # ========================================\n",
    "                            number_features = extract_number_features(text)\n",
    "                            if number_features['has_numbers']:\n",
    "                                stats['texts_with_numbers'] += 1\n",
    "                                logger.log(f\"  â””â”€ ðŸ”¢ Numbers detected: {number_features['number_count']}\")\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 5: Normalize numbers\n",
    "                            # ========================================\n",
    "                            text_normalized = \"\"\n",
    "                            if self.config.get('text.normalize_numbers', True) and text:\n",
    "                                text_normalized = normalize_numbers_in_text(text)\n",
    "                                if text != text_normalized:\n",
    "                                    stats['numbers_normalized'] += 1\n",
    "                                    logger.log(f\"  â””â”€ âœ“ Numbers normalized\")\n",
    "                            else:\n",
    "                                text_normalized = text\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 6: Translate to English\n",
    "                            # ========================================\n",
    "                            text_en = \"\"\n",
    "                            translation_success = False\n",
    "                            \n",
    "                            if self._get_file_status(filename, 'translation'):\n",
    "                                logger.log(f\"  âœ“ Translation: SKIP (already done)\")\n",
    "                                # Try to load from cache\n",
    "                                cached_translation = self.translation_cache.get_translation(text_normalized)\n",
    "                                if cached_translation:\n",
    "                                    text_en = cached_translation\n",
    "                                    translation_success = True\n",
    "                                    stats['cached_translations_used'] += 1\n",
    "                                    logger.log(f\"  â””â”€ âœ“ Loaded from cache\")\n",
    "                                else:\n",
    "                                    logger.log_warning(f\"  âš ï¸ Translation cache missing, will retry\")\n",
    "                                    self._update_file_status(filename, 'translation', False)\n",
    "                            \n",
    "                            if not translation_success and text_success and self.config.get('text.translate_to_english', True):\n",
    "                                logger.log(f\"  ðŸ”„ Translation: PROCESSING\")\n",
    "                                \n",
    "                                text_en = translate_text_with_cache(\n",
    "                                    text_normalized, \n",
    "                                    self.translation_cache,\n",
    "                                    self.decision_log,\n",
    "                                    filename,\n",
    "                                    logger=logger\n",
    "                                )\n",
    "                                \n",
    "                                if text_en:\n",
    "                                    self._update_file_status(filename, 'translation', True)\n",
    "                                    stats['successful_translations'] += 1\n",
    "                                    logger.log(f\"  âœ“ Translation: SUCCESS\")\n",
    "                                else:\n",
    "                                    self._update_file_status(filename, 'translation', False)\n",
    "                                    stats['failed_translations'] += 1\n",
    "                                    logger.log_warning(f\"  âš ï¸ Translation: FAILED\")\n",
    "                                    text_en = \"\"\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 7: Analyze texts\n",
    "                            # ========================================\n",
    "                            features_id = analyze_text_content(text_normalized, 'id')\n",
    "                            features_en = analyze_text_content(text_en, 'en')\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 8: Extract audio features (94)\n",
    "                            # ========================================\n",
    "                            audio_features = None\n",
    "                            \n",
    "                            if self._get_file_status(filename, 'audio_features'):\n",
    "                                logger.log(f\"  âœ“ Audio features: SKIP (already done)\")\n",
    "                                # Try to find in text_audio_data\n",
    "                                existing_entry = next((e for e in text_audio_data if e['filename'] == filename), None)\n",
    "                                if existing_entry:\n",
    "                                    # Extract audio features from existing entry\n",
    "                                    audio_feature_names = create_audio_feature_names()\n",
    "                                    audio_features = np.array([existing_entry[name] for name in audio_feature_names])\n",
    "                                    logger.log(f\"  â””â”€ âœ“ Loaded from checkpoint data\")\n",
    "                                else:\n",
    "                                    logger.log_warning(f\"  âš ï¸ Audio features data missing, will retry\")\n",
    "                                    self._update_file_status(filename, 'audio_features', False)\n",
    "                            \n",
    "                            if audio_features is None:\n",
    "                                logger.log(f\"  ðŸ”„ Audio features: PROCESSING\")\n",
    "                                audio_features = extract_audio_features(wav_path, self.config, logger)\n",
    "                                \n",
    "                                if audio_features is not None:\n",
    "                                    self._update_file_status(filename, 'audio_features', True)\n",
    "                                    stats['successful_audio_extractions'] += 1\n",
    "                                    logger.log(f\"  âœ“ Audio features: SUCCESS\")\n",
    "                                else:\n",
    "                                    self._update_file_status(filename, 'audio_features', False)\n",
    "                                    stats['failed_audio_extractions'] += 1\n",
    "                                    logger.log_warning(f\"  âš ï¸ Audio features: FAILED\")\n",
    "                                    audio_features = np.zeros(94)\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 9: Extract pause features\n",
    "                            # ========================================\n",
    "                            pause_features = extract_pause_silence_features(wav_path, self.config, logger)\n",
    "                            if pause_features['num_pauses'] > 0:\n",
    "                                stats['pause_features_extracted'] += 1\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 10: Check audio quality\n",
    "                            # ========================================\n",
    "                            quality_info = check_audio_quality(wav_path, self.config, logger)\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 11: Store TEXT + AUDIO data\n",
    "                            # ========================================\n",
    "                            # Remove old entry if exists (for partial resume)\n",
    "                            text_audio_data = [e for e in text_audio_data if e['filename'] != filename]\n",
    "                            \n",
    "                            data_entry = {\n",
    "                                'filename': filename,\n",
    "                                'text_indonesian_original': text,\n",
    "                                'text_indonesian_normalized': text_normalized,\n",
    "                                'text_english': text_en,\n",
    "                                'label': label,\n",
    "                                'dataset': 'I3D',\n",
    "                                \n",
    "                                # Transcript metadata\n",
    "                                'transcript_source': text_metadata.get('source', 'unknown'),\n",
    "                                'transcript_from_cache': text_metadata.get('from_cache', False),\n",
    "                                'transcript_attempt': text_metadata.get('attempt', 0),\n",
    "                                'transcript_mode': text_metadata.get('mode', 'normal'),\n",
    "                                'stt_energy_threshold': text_metadata.get('stt_energy_threshold', 0.0),\n",
    "                                \n",
    "                                # Re-extraction metadata\n",
    "                                'reextraction_triggered': file_reextraction_triggered,\n",
    "                                'reextraction_success': file_reextraction_success,\n",
    "                                'reextraction_params': json.dumps(reextraction_params) if reextraction_params else '',\n",
    "                                \n",
    "                                # Number features\n",
    "                                'has_numbers': number_features['has_numbers'],\n",
    "                                'number_count': number_features['number_count'],\n",
    "                                'has_vague_quantifiers': number_features['has_vague_quantifiers'],\n",
    "                                'has_exact_quantifiers': number_features['has_exact_quantifiers'],\n",
    "                                'number_word_ratio': number_features['number_word_ratio'],\n",
    "                                'has_large_numbers': number_features['has_large_numbers'],\n",
    "                                'has_decimal_numbers': number_features['has_decimal_numbers'],\n",
    "                                \n",
    "                                # Indonesian text features\n",
    "                                'char_count_id': features_id['char_count'],\n",
    "                                'word_count_id': features_id['word_count'],\n",
    "                                'sentiment_id': features_id['sentiment'],\n",
    "                                'subjectivity_id': features_id['subjectivity'],\n",
    "                                'complexity_id': features_id['complexity'],\n",
    "                                'lexical_diversity_id': features_id['lexical_diversity'],\n",
    "                                \n",
    "                                # English text features\n",
    "                                'char_count_en': features_en['char_count'],\n",
    "                                'word_count_en': features_en['word_count'],\n",
    "                                'sentiment_en': features_en['sentiment'],\n",
    "                                'subjectivity_en': features_en['subjectivity'],\n",
    "                                'complexity_en': features_en['complexity'],\n",
    "                                'lexical_diversity_en': features_en['lexical_diversity'],\n",
    "                                \n",
    "                                # Audio quality\n",
    "                                'audio_quality_score': quality_info.get('quality_score', 0),\n",
    "                                'audio_duration': quality_info.get('duration', 0),\n",
    "                                'audio_snr': quality_info.get('snr', 0),\n",
    "                                'audio_snr_computation': quality_info.get('snr_computation', ''),\n",
    "                                'audio_rms': quality_info.get('rms', 0),\n",
    "                                'audio_spectral_centroid': quality_info.get('spectral_centroid', 0),\n",
    "                                'audio_dynamic_range': quality_info.get('dynamic_range', 0)\n",
    "                            }\n",
    "                            \n",
    "                            # Add audio features (94)\n",
    "                            audio_feature_names = create_audio_feature_names()\n",
    "                            for i, feature_name in enumerate(audio_feature_names):\n",
    "                                data_entry[feature_name] = float(audio_features[i])\n",
    "                            \n",
    "                            # Add pause features\n",
    "                            for feature_name, feature_value in pause_features.items():\n",
    "                                data_entry[f'pause_{feature_name}'] = feature_value\n",
    "                            \n",
    "                            text_audio_data.append(data_entry)\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # STEP 12: Extract LANDMARKS\n",
    "                            # ========================================\n",
    "                            landmarks_done = False\n",
    "                            \n",
    "                            if self._get_file_status(filename, 'landmarks'):\n",
    "                                logger.log(f\"  âœ“ Landmarks: SKIP (already done)\")\n",
    "                                # Check if landmarks exist in CSV\n",
    "                                # (We can't easily verify this without reading the entire CSV,\n",
    "                                #  so we trust the status flag)\n",
    "                                landmarks_done = True\n",
    "                                stats['successful_landmark_extractions'] += 1\n",
    "                            \n",
    "                            if not landmarks_done:\n",
    "                                logger.log(f\"  ðŸ”„ Landmarks: PROCESSING\")\n",
    "                                \n",
    "                                cap = cv2.VideoCapture(str(video_path))\n",
    "                                \n",
    "                                if not cap.isOpened():\n",
    "                                    logger.log_error(f\"âŒ Failed to open video: {filename}\")\n",
    "                                    self._update_file_status(filename, 'landmarks', False)\n",
    "                                    stats['failed_landmark_extractions'] += 1\n",
    "                                    processed_files.add(filename)\n",
    "                                    continue\n",
    "                                \n",
    "                                frame_count = 0\n",
    "                                landmark_detected_count = 0\n",
    "                                sample_rate = self.config.get('video.sample_rate', 1)\n",
    "                                \n",
    "                                detection_summary = {\n",
    "                                    'total_frames': 0,\n",
    "                                    'face_detected': 0,\n",
    "                                    'iris_detected': 0,\n",
    "                                    'pose_detected': 0,\n",
    "                                    'strategies_used': {}\n",
    "                                }\n",
    "                                \n",
    "                                while cap.isOpened():\n",
    "                                    ret, frame = cap.read()\n",
    "                                    if not ret:\n",
    "                                        break\n",
    "                                    \n",
    "                                    if frame_count % sample_rate != 0:\n",
    "                                        frame_count += 1\n",
    "                                        continue\n",
    "                                    \n",
    "                                    stats['total_frames_processed'] += 1\n",
    "                                    detection_summary['total_frames'] += 1\n",
    "                                    \n",
    "                                    # Extract face landmarks\n",
    "                                    face_landmarks_dict, iris_count, strategy = landmark_extractor.extract_with_multi_strategy(\n",
    "                                        frame, face_mesh, iris_focus=True\n",
    "                                    )\n",
    "                                    \n",
    "                                    # âœ… Validate face/iris landmark count\n",
    "                                    if face_landmarks_dict:\n",
    "                                        num_face_landmarks = len(face_landmarks_dict)\n",
    "                                        expected_face_landmarks = 478  # 468 face + 10 iris\n",
    "                                        \n",
    "                                        if num_face_landmarks != expected_face_landmarks:\n",
    "                                            logger.log_warning(\n",
    "                                                f\"âš ï¸ Unexpected face/iris landmark count: {num_face_landmarks} \"\n",
    "                                                f\"(expected {expected_face_landmarks}) for {filename} frame {frame_count}. \"\n",
    "                                                f\"MediaPipe version: {self.env_info['packages'].get('mediapipe', 'unknown')}\"\n",
    "                                            )\n",
    "                                            stats['landmark_count_warnings'] += 1\n",
    "                                        \n",
    "                                        detection_summary['face_detected'] += 1\n",
    "                                        stats['face_detected'] += 1\n",
    "                                        \n",
    "                                        if strategy:\n",
    "                                            detection_summary['strategies_used'][strategy] = \\\n",
    "                                                detection_summary['strategies_used'].get(strategy, 0) + 1\n",
    "                                    \n",
    "                                    if iris_count >= 5:\n",
    "                                        detection_summary['iris_detected'] += 1\n",
    "                                        stats['iris_detected'] += 1\n",
    "                                        landmark_detected_count += 1\n",
    "                                    \n",
    "                                    # Extract pose landmarks\n",
    "                                    pose_landmarks_dict = None\n",
    "                                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                                    pose_results = pose.process(frame_rgb)\n",
    "                                    \n",
    "                                    if pose_results.pose_landmarks:\n",
    "                                        pose_landmarks_dict = {}\n",
    "                                        for idx, landmark in enumerate(pose_results.pose_landmarks.landmark):\n",
    "                                            pose_landmarks_dict[idx] = [landmark.x, landmark.y, landmark.z]\n",
    "                                        \n",
    "                                        # âœ… Validate pose landmark count\n",
    "                                        num_pose_landmarks = len(pose_landmarks_dict)\n",
    "                                        expected_pose_landmarks = 33\n",
    "                                        \n",
    "                                        if num_pose_landmarks != expected_pose_landmarks:\n",
    "                                            logger.log_warning(\n",
    "                                                f\"âš ï¸ Unexpected pose landmark count: {num_pose_landmarks} \"\n",
    "                                                f\"(expected {expected_pose_landmarks}) for {filename} frame {frame_count}. \"\n",
    "                                                f\"MediaPipe version: {self.env_info['packages'].get('mediapipe', 'unknown')}\"\n",
    "                                            )\n",
    "                                            stats['landmark_count_warnings'] += 1\n",
    "                                        \n",
    "                                        detection_summary['pose_detected'] += 1\n",
    "                                        stats['pose_detected'] += 1\n",
    "                                    \n",
    "                                    # Write to CSV\n",
    "                                    if face_landmarks_dict or pose_landmarks_dict:\n",
    "                                        row = [video_name, frame_count]\n",
    "                                        \n",
    "                                        # Face + Iris landmarks (478)\n",
    "                                        for i in range(478):\n",
    "                                            if face_landmarks_dict and i in face_landmarks_dict:\n",
    "                                                row.extend(face_landmarks_dict[i])\n",
    "                                            else:\n",
    "                                                row.extend([0.0, 0.0, 0.0])\n",
    "                                        \n",
    "                                        # Pose landmarks (33)\n",
    "                                        for i in range(33):\n",
    "                                            if pose_landmarks_dict and i in pose_landmarks_dict:\n",
    "                                                row.extend(pose_landmarks_dict[i])\n",
    "                                            else:\n",
    "                                                row.extend([0.0, 0.0, 0.0])\n",
    "                                        \n",
    "                                        row.append(label)\n",
    "                                        landmark_writer.writerow(row)\n",
    "                                    \n",
    "                                    frame_count += 1\n",
    "                                \n",
    "                                cap.release()\n",
    "                                \n",
    "                                # Log detection summary\n",
    "                                if detection_summary['total_frames'] > 0:\n",
    "                                    face_rate = (detection_summary['face_detected'] / detection_summary['total_frames']) * 100\n",
    "                                    iris_rate = (detection_summary['iris_detected'] / detection_summary['total_frames']) * 100\n",
    "                                    pose_rate = (detection_summary['pose_detected'] / detection_summary['total_frames']) * 100\n",
    "                                    \n",
    "                                    logger.log(f\"  â””â”€ âœ“ Landmarks extracted: {landmark_detected_count} frames\")\n",
    "                                    logger.log(f\"     Face: {face_rate:.1f}% ({detection_summary['face_detected']}/{detection_summary['total_frames']})\")\n",
    "                                    logger.log(f\"     Iris: {iris_rate:.1f}% ({detection_summary['iris_detected']}/{detection_summary['total_frames']})\")\n",
    "                                    logger.log(f\"     Pose: {pose_rate:.1f}% ({detection_summary['pose_detected']}/{detection_summary['total_frames']})\")\n",
    "                                    \n",
    "                                    if landmark_detected_count > 0:\n",
    "                                        self._update_file_status(filename, 'landmarks', True)\n",
    "                                        stats['successful_landmark_extractions'] += 1\n",
    "                                        logger.log(f\"  âœ“ Landmarks: SUCCESS\")\n",
    "                                    else:\n",
    "                                        self._update_file_status(filename, 'landmarks', False)\n",
    "                                        stats['failed_landmark_extractions'] += 1\n",
    "                                        logger.log_warning(f\"  âš ï¸ Landmarks: FAILED\")\n",
    "                                else:\n",
    "                                    self._update_file_status(filename, 'landmarks', False)\n",
    "                                    stats['failed_landmark_extractions'] += 1\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # Mark as processed\n",
    "                            # ========================================\n",
    "                            processed_files.add(filename)\n",
    "                            files_processed += 1\n",
    "                            \n",
    "                            # Log completion status\n",
    "                            if self._is_file_completed(filename):\n",
    "                                logger.log(f\"  âœ… File FULLY COMPLETED: {filename}\")\n",
    "                            else:\n",
    "                                logger.log(f\"  âš ï¸  File PARTIALLY COMPLETED: {filename}\")\n",
    "                                incomplete_steps = [\n",
    "                                    step for step in ['wav_conversion', 'transcript_extraction', 'audio_features', 'landmarks']\n",
    "                                    if not self._get_file_status(filename, step)\n",
    "                                ]\n",
    "                                logger.log(f\"     Incomplete steps: {', '.join(incomplete_steps)}\")\n",
    "                            \n",
    "                            # ========================================\n",
    "                            # SAVE CHECKPOINT (with status tracking)\n",
    "                            # ========================================\n",
    "                            if self.config.get('checkpoint.enabled', True):\n",
    "                                save_interval = self.config.get('checkpoint.save_interval', 5)\n",
    "                                if files_processed % save_interval == 0:\n",
    "                                    checkpoint_data_save = {\n",
    "                                        'processed_files': list(processed_files),\n",
    "                                        'text_audio_data': text_audio_data,\n",
    "                                        'failed_samples': failed_samples,\n",
    "                                        'stats': stats,\n",
    "                                        'status_by_file': self.status_by_file  # âœ… NEW: Save status\n",
    "                                    }\n",
    "                                    self.checkpoint_manager.save_checkpoint(checkpoint_data_save)\n",
    "                                    \n",
    "                                    # Count completion stats\n",
    "                                    complete_count = sum(1 for s in self.status_by_file.values() if s.get('completed', False))\n",
    "                                    partial_count = len(processed_files) - complete_count\n",
    "                                    \n",
    "                                    logger.log(f\"  â””â”€ ðŸ’¾ Checkpoint saved ({files_processed} files)\")\n",
    "                                    logger.log(f\"     Complete: {complete_count}, Partial: {partial_count}\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            logger.log_error(f\"âŒ Error processing {filename}: {str(e)}\")\n",
    "                            import traceback\n",
    "                            logger.log_error(traceback.format_exc())\n",
    "                            \n",
    "                            # Mark all steps as failed\n",
    "                            self._update_file_status(filename, 'wav_conversion', False)\n",
    "                            self._update_file_status(filename, 'transcript_extraction', False)\n",
    "                            self._update_file_status(filename, 'translation', False)\n",
    "                            self._update_file_status(filename, 'audio_features', False)\n",
    "                            self._update_file_status(filename, 'landmarks', False)\n",
    "                            \n",
    "                            stats['errors'].append(filename)\n",
    "                            stats['failed_text_conversions'] += 1\n",
    "                            stats['failed_audio_extractions'] += 1\n",
    "                            stats['failed_landmark_extractions'] += 1\n",
    "                            failed_samples.append({'filename': filename, 'reason': str(e), 'label': label})\n",
    "                            processed_files.add(filename)\n",
    "                            continue\n",
    "        \n",
    "        finally:\n",
    "            # Close landmark CSV\n",
    "            landmark_csv.close()\n",
    "        \n",
    "        # Save datasets\n",
    "        output_paths = self._save_datasets(text_audio_data, logger)\n",
    "        \n",
    "        # Save failed samples\n",
    "        if failed_samples:\n",
    "            failed_df = pd.DataFrame(failed_samples)\n",
    "            failed_path = self.paths.paths['reextraction'] / 'failed_samples.csv'\n",
    "            failed_df.to_csv(failed_path, index=False, encoding='utf-8')\n",
    "            logger.log(f\"\\nâš ï¸ Failed samples saved: {failed_path}\")\n",
    "            logger.log(f\"  â””â”€ {len(failed_samples)} samples need manual review\")\n",
    "        \n",
    "        # Generate statistics\n",
    "        self._generate_statistics(text_audio_data, stats, logger)\n",
    "        \n",
    "        # Generate data dictionary\n",
    "        end_time = datetime.now()\n",
    "        if self.config.get('output.generate_data_dictionary', True):\n",
    "            dict_path = generate_data_dictionary(self.paths)\n",
    "            logger.log(f\"\\nâœ“ Data dictionary generated: {dict_path}\")\n",
    "        \n",
    "        # Generate processing summary\n",
    "        summary_path = generate_processing_summary(text_audio_data, self.paths)\n",
    "        logger.log(f\"âœ“ Processing summary generated: {summary_path}\")\n",
    "        \n",
    "        # Generate run manifest\n",
    "        if self.config.get('output.generate_run_manifest', True):\n",
    "            manifest_path = generate_run_manifest(\n",
    "                self.env_info, self.config, self.paths, stats, all_video_files\n",
    "            )\n",
    "            logger.log(f\"âœ“ Run manifest generated: {manifest_path}\")\n",
    "        \n",
    "        # Generate markdown report\n",
    "        if self.config.get('output.generate_markdown_report', True):\n",
    "            markdown_path = generate_markdown_report(\n",
    "                stats, output_paths, self.paths, start_time, end_time, self.env_info, self.config\n",
    "            )\n",
    "            logger.log(f\"âœ“ Markdown report generated: {markdown_path}\")\n",
    "        \n",
    "        # âœ… Log landmark validation warnings\n",
    "        if stats.get('landmark_count_warnings', 0) > 0:\n",
    "            logger.log(f\"\\nâš ï¸ Landmark Count Warnings: {stats['landmark_count_warnings']}\")\n",
    "            logger.log(f\"   Some frames had unexpected landmark counts.\")\n",
    "            logger.log(f\"   This may indicate MediaPipe version differences.\")\n",
    "            logger.log(f\"   See log for details.\")\n",
    "        \n",
    "        # âœ… Log smart resume statistics\n",
    "        logger.log(f\"\\nðŸ“Š Smart Resume Statistics:\")\n",
    "        complete_count = sum(1 for s in self.status_by_file.values() if s.get('completed', False))\n",
    "        partial_count = len(processed_files) - complete_count\n",
    "        logger.log(f\"   - Fully completed files: {complete_count}\")\n",
    "        logger.log(f\"   - Partially completed files: {partial_count}\")\n",
    "        \n",
    "        if partial_count > 0:\n",
    "            logger.log(f\"\\nðŸ’¡ Partial files breakdown:\")\n",
    "            for filename, status in self.status_by_file.items():\n",
    "                if not status.get('completed', False):\n",
    "                    failed_steps = [step for step, success in status.items() \n",
    "                                if step != 'completed' and not success]\n",
    "                    if failed_steps:\n",
    "                        logger.log(f\"   - {filename}: Failed steps: {', '.join(failed_steps)}\")\n",
    "        \n",
    "        # Log cache statistics\n",
    "        logger.log(f\"\\nðŸ“Š Cache Statistics:\")\n",
    "        logger.log(f\"   - Transcript cache: {self.transcript_cache.transcripts_dir}\")\n",
    "        logger.log(f\"   - Translation cache: {self.translation_cache.get_cache_size()} entries\")\n",
    "        logger.log(f\"   - Decision log: {self.decision_log.decisions_file}\")\n",
    "        \n",
    "        # Clear checkpoint after successful completion\n",
    "        self.checkpoint_manager.clear_checkpoint()\n",
    "        \n",
    "        logger.finalize()\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'stats': stats,\n",
    "            'failed_samples': len(failed_samples),\n",
    "            'output_paths': output_paths,\n",
    "            'env_info': self.env_info,\n",
    "            'smart_resume_stats': {\n",
    "                'complete_count': complete_count,\n",
    "                'partial_count': partial_count\n",
    "            }\n",
    "        }\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"ðŸŽ¯ {PIPELINE_NAME} v{VERSION}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"âœ¨ SCIENTIFIC DATA READY - Production Version\")\n",
    "    print()\n",
    "    print(\"ðŸ”§ CRITICAL FIXES:\")\n",
    "    print(\"   âœ… Audio features: 94 (verified with assert)\")\n",
    "    print(\"   âœ… MediaPipe landmarks: 478 face (468+10 iris) + 33 pose\")\n",
    "    print(\"   âœ… Explicit librosa parameters (n_fft, hop_length, etc)\")\n",
    "    print(\"   âœ… Deterministic STT mode available\")\n",
    "    print(\"   âœ… Per-file re-extraction flags (not global)\")\n",
    "    print(\"   âœ… Complete data dictionary generated\")\n",
    "    print(\"   âœ… Run manifest for full reproducibility\")\n",
    "    print(\"   âœ… Processing summary (per-file quality metrics)\")\n",
    "    print(\"   âœ… Decision logging (JSONL format)\")\n",
    "    print(\"   âœ… Environment capture (Python, packages, FFmpeg)\")\n",
    "    print(\"   âœ… SNR computation documented (STFT parameters)\")\n",
    "    print(\"   âœ… Sentiment analysis caveat documented\")\n",
    "    print(\"   âœ… Filename leakage warning\")\n",
    "    print(\"   âœ… Missing landmark policy: (0.0, 0.0, 0.0)\")\n",
    "    print(\"   âœ… Ethics & privacy statement included\")\n",
    "    print()\n",
    "    print(\"ðŸ“¦ Standard Features:\")\n",
    "    print(\"   â€¢ Language: Indonesian\")\n",
    "    print(\"   â€¢ ðŸ“ Manual Transcript Support (33 files)\")\n",
    "    print(\"   â€¢ Transcript Freezing (cache after first extraction)\")\n",
    "    print(\"   â€¢ Translation Freezing (MD5-based cache)\")\n",
    "    print(\"   â€¢ Indonesian number normalization (0-Trillion)\")\n",
    "    print(\"   â€¢ Number features extraction (7 features)\")\n",
    "    print(\"   â€¢ Audio Features: 94 (MFCC + Spectral + Energy)\")\n",
    "    print(\"   â€¢ Pause/Silence Analysis (17 features)\")\n",
    "    print(\"   â€¢ FaceMesh Landmarks (468 facial + 10 iris = 478)\")\n",
    "    print(\"   â€¢ Pose Landmarks (33 body keypoints)\")\n",
    "    print(\"   â€¢ Advanced Video Preprocessing\")\n",
    "    print(\"   â€¢ Multi-level Zoom (up to 10x)\")\n",
    "    print(\"   â€¢ Integrated Re-extraction\")\n",
    "    print(\"   â€¢ ðŸ’¾ AUTO-SAVE CHECKPOINT (Resume if interrupted)\")\n",
    "    print(\"   â€¢ Comprehensive Reporting\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize configuration\n",
    "    base_dir = os.getcwd()\n",
    "    config_path = os.path.join(base_dir, \"config_I3D.yaml\")\n",
    "    \n",
    "    config = I3DConfig(config_path if os.path.exists(config_path) else None)\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        config.save_to_file(config_path)\n",
    "        print(f\"\\nâœ“ Default configuration saved to: {config_path}\")\n",
    "        print(\"  You can edit this file to customize settings.\")\n",
    "        print()\n",
    "        print(\"  Key options:\")\n",
    "        print(\"    text.force_retranscribe: false  # Set true to ignore transcript cache\")\n",
    "        print(\"    speech_recognition.deterministic_mode: true  # âœ… TRUE by default (reproducible)\")\n",
    "        print(\"    text.use_transcript_cache: true  # Enable/disable transcript caching\")\n",
    "        print(\"    text.use_translation_cache: true # Enable/disable translation caching\")\n",
    "    \n",
    "    # âœ… NEW: Check deterministic mode and warn if disabled\n",
    "    deterministic_mode = config.get('speech_recognition.deterministic_mode', True)\n",
    "    if not deterministic_mode:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âš ï¸  WARNING: DETERMINISTIC MODE DISABLED\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"   â€¢ Dynamic energy threshold enabled (non-reproducible)\")\n",
    "        print(\"   â€¢ STT results may vary between runs\")\n",
    "        print(\"   â€¢ For Scientific Data submission, deterministic_mode=true is recommended\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        response = input(\"\\nContinue with non-deterministic mode? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"âŒ Processing cancelled.\")\n",
    "            print(\"ðŸ’¡ Set deterministic_mode: true in config_I3D.yaml\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"\\nâœ… Deterministic mode ENABLED (reproducible STT)\")\n",
    "\n",
    "    # Initialize path manager\n",
    "    path_manager = I3DPathManager(base_dir)\n",
    "    path_manager.create_directories()\n",
    "    \n",
    "    # Initialize checkpoint manager\n",
    "    checkpoint_manager = CheckpointManager(path_manager.paths['checkpoints'])\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    checkpoint_data = None\n",
    "    if checkpoint_manager.checkpoint_exists():\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸ”„ CHECKPOINT DETECTED!\")\n",
    "        print(\"=\"*70)\n",
    "        checkpoint_data_temp = checkpoint_manager.load_checkpoint()\n",
    "        if checkpoint_data_temp:\n",
    "            processed_count = len(checkpoint_data_temp.get('processed_files', []))\n",
    "            print(f\"   â€¢ Previously processed: {processed_count} files\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            response = input(\"\\nðŸ”„ Resume from checkpoint? (y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                print(\"âœ“ Resuming from checkpoint...\")\n",
    "                print(\"  â””â”€ All caches will be preserved\")\n",
    "                checkpoint_data = checkpoint_data_temp\n",
    "            else:\n",
    "                response_clear = input(\"âš ï¸  Clear checkpoint and start fresh? (y/n): \")\n",
    "                if response_clear.lower() == 'y':\n",
    "                    checkpoint_manager.clear_checkpoint()\n",
    "                    print(\"âœ“ Checkpoint cleared. Starting fresh...\")\n",
    "                    print(\"  â””â”€ Note: Transcript and translation caches will still be used\")\n",
    "                    print(\"  â””â”€ Set force_retranscribe=true in config to ignore transcript cache\")\n",
    "                else:\n",
    "                    print(\"âŒ Processing cancelled.\")\n",
    "                    return\n",
    "    \n",
    "    # Check cache status\n",
    "    transcript_cache = TranscriptCacheManager(path_manager.paths['cache'])\n",
    "    translation_cache = TranslationCacheManager(path_manager.paths['cache'])\n",
    "    \n",
    "    # Count existing caches\n",
    "    transcript_count = len(list(transcript_cache.transcripts_dir.glob('*.json')))\n",
    "    translation_count = translation_cache.get_cache_size()\n",
    "    \n",
    "    if transcript_count > 0 or translation_count > 0:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸ’¾ CACHE DETECTED!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"   â€¢ Transcript cache: {transcript_count} files\")\n",
    "        print(f\"   â€¢ Translation cache: {translation_count} entries\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"âœ“ These caches will be used for deterministic processing\")\n",
    "        print(\"  â””â”€ Transcripts will be loaded from cache (skip STT)\")\n",
    "        print(\"  â””â”€ Translations will be loaded from cache (skip Google API)\")\n",
    "        print()\n",
    "        \n",
    "        force_retranscribe = config.get('text.force_retranscribe', False)\n",
    "        if force_retranscribe:\n",
    "            print(\"âš ï¸  WARNING: force_retranscribe=true in config\")\n",
    "            print(\"  â””â”€ Transcript cache will be IGNORED\")\n",
    "            print(\"  â””â”€ All files will be re-transcribed via STT\")\n",
    "        else:\n",
    "            print(\"ðŸ’¡ TIP: To force re-transcription, set force_retranscribe=true in config\")\n",
    "    \n",
    "    # Check ffmpeg\n",
    "    try:\n",
    "        subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.PIPE, \n",
    "                      stderr=subprocess.PIPE, check=True)\n",
    "        print(\"\\nâœ“ ffmpeg detected\")\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"\\nâŒ ffmpeg not found!\")\n",
    "        print(\"ðŸ’¡ Please install ffmpeg: https://ffmpeg.org/download.html\")\n",
    "        return\n",
    "    \n",
    "    # Confirm processing\n",
    "    if not checkpoint_data:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âš ï¸  IMPORTANT FOR SCIENTIFIC DATA:\")\n",
    "        print(\"   â€¢ Internet required for translation (if not cached)\")\n",
    "        print(\"   â€¢ External APIs: Google STT + Google Translate\")\n",
    "        print(\"   â€¢ First run: Non-deterministic (API variability)\")\n",
    "        print(\"   â€¢ Subsequent runs: Deterministic for cached files\")\n",
    "        print(\"   â€¢ All outputs will be frozen and documented\")\n",
    "        print(\"   â€¢ ðŸ“ 33 manual transcripts will be used automatically\")\n",
    "        print(\"   â€¢ ðŸ”’ Transcript caching enabled (deterministic)\")\n",
    "        print(\"   â€¢ ðŸŒ Translation caching enabled (deterministic)\")\n",
    "        print(\"   â€¢ ðŸ’¾ Auto-checkpoint every 5 files\")\n",
    "        print(\"   â€¢ ðŸ“Š Complete data dictionary will be generated\")\n",
    "        print(\"   â€¢ ðŸ“‹ Run manifest for reproducibility\")\n",
    "        print(\"   â€¢ âš ï¸  Filename contains label (LIE_/TRUTH_) - remove before ML\")\n",
    "        print(\"   â€¢ ðŸ”’ MediaPipe: 478 face (468+10 iris) + 33 pose landmarks\")\n",
    "        print(\"   â€¢ âš ï¸  Missing landmarks: (0.0, 0.0, 0.0) - NOT actual position\")\n",
    "        print(\"   â€¢ ðŸ›¡ï¸  Ethics: Interrogation data - see report for privacy notes\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nðŸ“ Expected directory structure:\")\n",
    "        print(\"   dataset/raw/I3D/lie/       <- Place LIE videos here\")\n",
    "        print(\"   dataset/raw/I3D/truth/     <- Place TRUTH videos here\")\n",
    "        \n",
    "        print(\"\\nðŸ“‚ Output structure:\")\n",
    "        print(\"   dataset/processed/I3D/     <- Processed datasets\")\n",
    "        print(\"   dataset/metadata/I3D/      <- Data dictionary + run manifest\")\n",
    "        print(\"   dataset/validation/I3D/    <- Quality reports + statistics\")\n",
    "        print(\"   dataset/_cache/I3D/        <- Transcript & translation cache\")\n",
    "        print(\"   dataset/_logs/I3D/         <- Processing logs & decisions\")\n",
    "        print(\"   dataset/_checkpoints/I3D/  <- Resume checkpoints\")\n",
    "        \n",
    "        response = input(\"\\nðŸš€ Start processing? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"âŒ Processing cancelled.\")\n",
    "            return\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = I3DProcessor(config, path_manager)\n",
    "    \n",
    "    # Process dataset\n",
    "    try:\n",
    "        result = processor.process(checkpoint_data)\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"âœ… I3D DATASET PROCESSING COMPLETE!\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"ðŸ“Š Total files: {result['stats']['total_files']}\")\n",
    "            print(f\"âœ“  Text extraction: {result['stats']['successful_text_conversions']}/{result['stats']['total_files']}\")\n",
    "            print(f\"ðŸ“ Manual transcripts: {result['stats']['manual_transcripts_used']}\")\n",
    "            print(f\"ðŸ’¾ Cached transcripts: {result['stats']['cached_transcripts_used']}\")\n",
    "            print(f\"âœ“  Numbers normalized: {result['stats']['numbers_normalized']}/{result['stats']['texts_with_numbers']}\")\n",
    "            print(f\"âœ“  Translation: {result['stats']['successful_translations']}/{result['stats']['successful_text_conversions']}\")\n",
    "            print(f\"ðŸŒ Cached translations: {result['stats']['cached_translations_used']}\")\n",
    "            print(f\"âœ“  Audio features (94): {result['stats']['successful_audio_extractions']}/{result['stats']['total_files']}\")\n",
    "            print(f\"âœ“  Landmark extraction: {result['stats']['successful_landmark_extractions']}/{result['stats']['total_files']}\")\n",
    "            print(f\"ðŸ‘ï¸  Iris detection: {result['stats']['iris_detected']:,} frames\")\n",
    "            print(f\"ðŸ”„ Re-extractions: {result['stats']['reextraction_triggered']} triggered, {result['stats']['reextraction_successful']} successful\")\n",
    "            print(f\"âš ï¸  Failed samples: {result['failed_samples']}\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\nðŸ“¦ Output directory: {path_manager.paths['processed']}\")\n",
    "            print(f\"ðŸ“‹ Detailed logs: {path_manager.paths['logs']}\")\n",
    "            print(f\"ðŸ’¾ Cache directory: {path_manager.paths['cache']}\")\n",
    "            print(f\"ðŸ“Š Decision log: {path_manager.paths['decision_logs']}\")\n",
    "            print(f\"ðŸ“š Metadata: {path_manager.paths['metadata']}\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            # Show cache statistics\n",
    "            print(\"\\nðŸ”’ Deterministic Processing Summary:\")\n",
    "            print(f\"   â€¢ Transcript cache: {result['stats']['cached_transcripts_used']}/{result['stats']['successful_text_conversions']} from cache\")\n",
    "            print(f\"   â€¢ Translation cache: {result['stats']['cached_translations_used']}/{result['stats']['successful_translations']} from cache\")\n",
    "            print(f\"   â€¢ Manual transcripts: {result['stats']['manual_transcripts_used']} files\")\n",
    "            print(f\"   â€¢ Re-extractions: {result['stats']['reextraction_successful']}/{result['stats']['reextraction_triggered']} successful\")\n",
    "            \n",
    "            cache_efficiency = 0\n",
    "            if result['stats']['successful_text_conversions'] > 0:\n",
    "                cache_efficiency = (result['stats']['cached_transcripts_used'] / result['stats']['successful_text_conversions']) * 100\n",
    "            \n",
    "            print(f\"\\nðŸ“ˆ Cache Efficiency: {cache_efficiency:.1f}%\")\n",
    "            if cache_efficiency > 80:\n",
    "                print(\"   âœ… Excellent! Most transcripts loaded from cache\")\n",
    "            elif cache_efficiency > 50:\n",
    "                print(\"   âœ“  Good cache utilization\")\n",
    "            else:\n",
    "                print(\"   ðŸ’¡ First run or cache cleared - building cache\")\n",
    "            \n",
    "            print(\"\\nðŸ’¡ Next Run:\")\n",
    "            print(\"   â€¢ Transcripts will be loaded from cache (instant)\")\n",
    "            print(\"   â€¢ Translations will be loaded from cache (instant)\")\n",
    "            print(\"   â€¢ Processing will be much faster!\")\n",
    "            print(\"   â€¢ Results will be identical for cached files (deterministic)\")\n",
    "            \n",
    "            print(\"\\nðŸ”§ Configuration Options:\")\n",
    "            print(\"   â€¢ Force re-transcription: set force_retranscribe=true in config\")\n",
    "            print(\"   â€¢ Deterministic STT: set deterministic_mode=true in config\")\n",
    "            print(\"   â€¢ Disable caching: set use_transcript_cache=false\")\n",
    "            \n",
    "            print(\"\\nðŸ“š Documentation for Scientific Data:\")\n",
    "            print(\"   â€¢ Extraction report: dataset/processed/I3D/I3D_EXTRACTION_REPORT.md\")\n",
    "            print(\"   â€¢ Data dictionary: dataset/metadata/I3D/data_dictionary.csv\")\n",
    "            print(\"   â€¢ Run manifest: dataset/metadata/I3D/run_manifest.json\")\n",
    "            print(\"   â€¢ Processing summary: dataset/validation/I3D/quality_reports/processing_summary.csv\")\n",
    "            print(\"   â€¢ Decision log: dataset/_logs/I3D/decisions/decision_log.jsonl\")\n",
    "            print(\"   â€¢ Statistics: dataset/validation/I3D/statistical_analyses/processing_statistics.json\")\n",
    "            \n",
    "            print(\"\\nâš ï¸  IMPORTANT NOTES:\")\n",
    "            print(\"   â€¢ Audio features: 94 (verified with assert)\")\n",
    "            print(\"   â€¢ MediaPipe landmarks: 478 face (468+10 iris) + 33 pose\")\n",
    "            print(\"   â€¢ Filename contains label: Remove before ML training\")\n",
    "            print(\"   â€¢ Sentiment (Indonesian): Proxy only - use English version\")\n",
    "            print(\"   â€¢ Missing landmarks: (0.0, 0.0, 0.0) - NOT actual position\")\n",
    "            print(\"   â€¢ External APIs: Google STT + Translate (non-deterministic on first run)\")\n",
    "            print(\"   â€¢ Ethics: Interrogation data - consult IRB before public release\")\n",
    "            \n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Processing failed!\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâš ï¸  PROCESSING INTERRUPTED!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"ðŸ’¾ Checkpoint has been saved automatically.\")\n",
    "        print(\"   Run the script again and choose 'Resume from checkpoint'\")\n",
    "        print(\"   to continue where you left off.\")\n",
    "        print()\n",
    "        print(\"ðŸ”’ Cache Status:\")\n",
    "        print(\"   â€¢ Transcript cache: PRESERVED\")\n",
    "        print(\"   â€¢ Translation cache: PRESERVED\")\n",
    "        print(\"   â€¢ All processed transcripts are cached\")\n",
    "        print(\"   â€¢ Resume will use cached data (deterministic)\")\n",
    "        print(\"=\"*70)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nðŸ’¾ Checkpoint saved. You can resume later.\")\n",
    "        print(\"ðŸ”’ All caches preserved for next run.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bisa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
