{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3910800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ” DUAL DATASET QUALITY CHECKER\n",
      "======================================================================\n",
      "Checking quality for both I3D and RLT datasets\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# CHECKING: I3D\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ CHECKING FILE EXISTENCE - I3D\n",
      "======================================================================\n",
      "âœ… text_indonesian           |     0.44 MB\n",
      "âœ… text_english              |     0.30 MB\n",
      "âœ… number_features           |     0.12 MB\n",
      "âœ… audio_features            |     2.91 MB\n",
      "âœ… pause_features            |     0.32 MB\n",
      "âœ… landmarks                 |  7536.75 MB\n",
      "âœ… multimodal_full           |     3.86 MB\n",
      "âœ… publication               |     0.71 MB\n",
      "======================================================================\n",
      "ğŸ“Š Summary: 8/8 files found\n",
      "âœ… All expected files exist!\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š CHECKING DATASET STRUCTURE - I3D\n",
      "======================================================================\n",
      "\n",
      "ğŸ“„ text_indonesian\n",
      "   â”œâ”€ Rows: 1,568\n",
      "   â”œâ”€ Columns: 16\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 784\n",
      "   â”‚  â””â”€ LIE (1): 784\n",
      "   â””â”€ Class balance: 50.0% LIE\n",
      "\n",
      "ğŸ“„ text_english\n",
      "   â”œâ”€ Rows: 1,568\n",
      "   â”œâ”€ Columns: 11\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 784\n",
      "   â”‚  â””â”€ LIE (1): 784\n",
      "   â””â”€ Class balance: 50.0% LIE\n",
      "\n",
      "ğŸ“„ number_features\n",
      "   â”œâ”€ Rows: 1,568\n",
      "   â”œâ”€ Columns: 10\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 784\n",
      "   â”‚  â””â”€ LIE (1): 784\n",
      "   â””â”€ Class balance: 50.0% LIE\n",
      "\n",
      "ğŸ“„ audio_features\n",
      "   â”œâ”€ Rows: 1,568\n",
      "   â”œâ”€ Columns: 104\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 784\n",
      "   â”‚  â””â”€ LIE (1): 784\n",
      "   â””â”€ Class balance: 50.0% LIE\n",
      "\n",
      "ğŸ“„ pause_features\n",
      "   â”œâ”€ Rows: 1,568\n",
      "   â”œâ”€ Columns: 20\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 784\n",
      "   â”‚  â””â”€ LIE (1): 784\n",
      "   â””â”€ Class balance: 50.0% LIE\n",
      "\n",
      "ğŸ“„ landmarks\n",
      "   â”œâ”€ Rows: 316,116\n",
      "   â”œâ”€ Columns: 1536\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 23106\n",
      "   â”‚  â””â”€ LIE (1): 293010\n",
      "   â””â”€ Class balance: 92.7% LIE\n",
      "      âš ï¸  Class imbalance detected!\n",
      "\n",
      "ğŸ“„ multimodal_full\n",
      "   â”œâ”€ Rows: 1,568\n",
      "   â”œâ”€ Columns: 151\n",
      "   â”œâ”€ Missing values: 1,568 (0.66%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 784\n",
      "   â”‚  â””â”€ LIE (1): 784\n",
      "   â””â”€ Class balance: 50.0% LIE\n",
      "\n",
      "ğŸ“„ publication\n",
      "   â”œâ”€ Rows: 1,568\n",
      "   â”œâ”€ Columns: 24\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 784\n",
      "   â”‚  â””â”€ LIE (1): 784\n",
      "   â””â”€ Class balance: 50.0% LIE\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ CHECKING TEXT QUALITY - I3D\n",
      "======================================================================\n",
      "\n",
      "ğŸ“„ Indonesian Text:\n",
      "   â”œâ”€ Total samples: 1568\n",
      "   â”œâ”€ Empty texts: 0 (0.0%)\n",
      "   â”œâ”€ Very short texts (< 3 words): 102\n",
      "   â”œâ”€ Avg word count: 16.1\n",
      "   â”œâ”€ Avg char count: 102.7\n",
      "   â”œâ”€ Avg sentiment: 0.000\n",
      "   â””â”€ Avg lexical diversity: 0.913\n",
      "\n",
      "ğŸ“„ English Text (Translation):\n",
      "   â”œâ”€ Total samples: 1568\n",
      "   â”œâ”€ Empty translations: 0 (0.0%)\n",
      "   â”œâ”€ Translation success rate: 100.0%\n",
      "   â”œâ”€ Avg word count: 19.4\n",
      "   â””â”€ Avg char count: 104.3\n",
      "\n",
      "======================================================================\n",
      "ğŸµ CHECKING AUDIO QUALITY - I3D\n",
      "======================================================================\n",
      "\n",
      "ğŸµ Audio Quality Metrics:\n",
      "   â”œâ”€ Total samples: 1568\n",
      "   â”œâ”€ Avg quality score: 0.711\n",
      "   â”œâ”€ Avg SNR: 79.8 dB\n",
      "   â”œâ”€ Avg duration: 13.71 seconds\n",
      "   â”œâ”€ Avg RMS: 0.0636\n",
      "   â”‚\n",
      "   â”œâ”€ Quality distribution:\n",
      "   â”‚  â”œâ”€ Excellent (â‰¥0.7): 770 (49.1%)\n",
      "   â”‚  â”œâ”€ Good (0.5-0.7): 797 (50.8%)\n",
      "   â”‚  â”œâ”€ Fair (0.3-0.5): 1 (0.1%)\n",
      "   â”‚  â””â”€ Poor (<0.3): 0 (0.0%)\n",
      "   â”‚\n",
      "   â”œâ”€ Low quality samples (< 0.3): 0 (0.0%)\n",
      "   â”œâ”€ Low SNR samples (< 10 dB): 0\n",
      "   â””â”€ Too short samples (< 0.5s): 0\n",
      "\n",
      "   ğŸ“Š Pause Features: 17 features\n",
      "      â”œâ”€ Avg hesitation score: 0.467\n",
      "      â”œâ”€ Avg pause frequency: 0.443 pauses/sec\n",
      "      â””â”€ Avg number of pauses: 5.6\n",
      "\n",
      "======================================================================\n",
      "ğŸ‘¤ CHECKING LANDMARK QUALITY - I3D\n",
      "======================================================================\n",
      "ğŸ“Š Loading landmark dataset (this may take a while)...\n",
      "\n",
      "ğŸ‘¤ Landmark Extraction:\n",
      "   â”œâ”€ Unique videos: 917\n",
      "   â”œâ”€ Total frames: 351,129\n",
      "   â”‚\n",
      "   â”œâ”€ Face landmarks:\n",
      "   â”‚  â”œâ”€ Count: 468 landmarks\n",
      "   â”‚  â”œâ”€ Detected: 321,624 frames\n",
      "   â”‚  â””â”€ Detection rate: 91.6%\n",
      "   â”‚\n",
      "   â”œâ”€ Iris landmarks:\n",
      "   â”‚  â”œâ”€ Count: 10 landmarks\n",
      "   â”‚  â”œâ”€ Detected: 321,624 frames\n",
      "   â”‚  â””â”€ Detection rate: 91.6%\n",
      "   â”‚\n",
      "   â””â”€ Pose landmarks:\n",
      "      â”œâ”€ Count: 33 landmarks\n",
      "      â”œâ”€ Detected: 351,129 frames\n",
      "      â””â”€ Detection rate: 100.0%\n",
      "\n",
      "======================================================================\n",
      "ğŸ”„ CHECKING CROSS-DATASET CONSISTENCY - I3D\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Sample Counts:\n",
      "   â”œâ”€ text_indonesian          : 1,568 samples\n",
      "   â”œâ”€ text_english             : 1,568 samples\n",
      "   â”œâ”€ number_features          : 1,568 samples\n",
      "   â”œâ”€ audio_features           : 1,568 samples\n",
      "   â”œâ”€ pause_features           : 1,568 samples\n",
      "   â”œâ”€ landmarks                : 393,243 samples\n",
      "   â”œâ”€ multimodal_full          : 1,568 samples\n",
      "   â”œâ”€ publication              : 1,568 samples\n",
      "\n",
      "   âœ… All non-landmark datasets have consistent sample counts: 1568\n",
      "\n",
      "ğŸ“„ Filename Consistency:\n",
      "   âš ï¸  Filename mismatch detected:\n",
      "      â”œâ”€ Missing in multimodal: 1568\n",
      "      â””â”€ Missing in text: 1568\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š GENERATING QUALITY VISUALIZATIONS - I3D\n",
      "======================================================================\n",
      "âœ… Audio quality visualization saved\n",
      "âœ… Text quality visualization saved\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ GENERATING QUALITY REPORT - I3D\n",
      "======================================================================\n",
      "âœ… JSON report saved: f:\\MULAI LAGI\\eKSTRAKSI - FULL DATA\\dataset\\validation\\I3D\\quality_reports\\quality_check_report_I3D.json\n",
      "âœ… Markdown summary saved: f:\\MULAI LAGI\\eKSTRAKSI - FULL DATA\\dataset\\validation\\I3D\\quality_reports\\QUALITY_CHECK_SUMMARY_I3D.md\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# CHECKING: RLT\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ CHECKING FILE EXISTENCE - RLT\n",
      "======================================================================\n",
      "âœ… text_indonesian           |     0.05 MB\n",
      "âœ… text_english              |     0.05 MB\n",
      "âœ… number_features           |     0.01 MB\n",
      "âœ… audio_features            |     0.23 MB\n",
      "âœ… pause_features            |     0.02 MB\n",
      "âœ… landmarks                 |  2534.68 MB\n",
      "âœ… multimodal_full           |     0.39 MB\n",
      "âœ… publication               |     0.11 MB\n",
      "======================================================================\n",
      "ğŸ“Š Summary: 8/8 files found\n",
      "âœ… All expected files exist!\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š CHECKING DATASET STRUCTURE - RLT\n",
      "======================================================================\n",
      "\n",
      "ğŸ“„ text_indonesian\n",
      "   â”œâ”€ Rows: 121\n",
      "   â”œâ”€ Columns: 10\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 60\n",
      "   â”‚  â””â”€ LIE (1): 61\n",
      "   â””â”€ Class balance: 50.4% LIE\n",
      "\n",
      "ğŸ“„ text_english\n",
      "   â”œâ”€ Rows: 121\n",
      "   â”œâ”€ Columns: 10\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 60\n",
      "   â”‚  â””â”€ LIE (1): 61\n",
      "   â””â”€ Class balance: 50.4% LIE\n",
      "\n",
      "ğŸ“„ number_features\n",
      "   â”œâ”€ Rows: 121\n",
      "   â”œâ”€ Columns: 10\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 60\n",
      "   â”‚  â””â”€ LIE (1): 61\n",
      "   â””â”€ Class balance: 50.4% LIE\n",
      "\n",
      "ğŸ“„ audio_features\n",
      "   â”œâ”€ Rows: 121\n",
      "   â”œâ”€ Columns: 103\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 60\n",
      "   â”‚  â””â”€ LIE (1): 61\n",
      "   â””â”€ Class balance: 50.4% LIE\n",
      "\n",
      "ğŸ“„ pause_features\n",
      "   â”œâ”€ Rows: 121\n",
      "   â”œâ”€ Columns: 20\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 60\n",
      "   â”‚  â””â”€ LIE (1): 61\n",
      "   â””â”€ Class balance: 50.4% LIE\n",
      "\n",
      "ğŸ“„ landmarks\n",
      "   â”œâ”€ Rows: 89,853\n",
      "   â”œâ”€ Columns: 1536\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 41855\n",
      "   â”‚  â””â”€ LIE (1): 47998\n",
      "   â””â”€ Class balance: 53.4% LIE\n",
      "\n",
      "ğŸ“„ multimodal_full\n",
      "   â”œâ”€ Rows: 121\n",
      "   â”œâ”€ Columns: 148\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 60\n",
      "   â”‚  â””â”€ LIE (1): 61\n",
      "   â””â”€ Class balance: 50.4% LIE\n",
      "\n",
      "ğŸ“„ publication\n",
      "   â”œâ”€ Rows: 121\n",
      "   â”œâ”€ Columns: 23\n",
      "   â”œâ”€ Missing values: 0 (0.00%)\n",
      "   â”œâ”€ Class distribution:\n",
      "   â”‚  â”œâ”€ TRUTH (0): 60\n",
      "   â”‚  â””â”€ LIE (1): 61\n",
      "   â””â”€ Class balance: 50.4% LIE\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ CHECKING TEXT QUALITY - RLT\n",
      "======================================================================\n",
      "\n",
      "ğŸ“„ Indonesian Text:\n",
      "   â”œâ”€ Total samples: 121\n",
      "   â”œâ”€ Empty texts: 0 (0.0%)\n",
      "   â”œâ”€ Very short texts (< 3 words): 0\n",
      "   â”œâ”€ Avg word count: 56.0\n",
      "   â”œâ”€ Avg char count: 370.4\n",
      "   â”œâ”€ Avg sentiment: 0.003\n",
      "   â””â”€ Avg lexical diversity: 0.776\n",
      "\n",
      "======================================================================\n",
      "ğŸµ CHECKING AUDIO QUALITY - RLT\n",
      "======================================================================\n",
      "\n",
      "ğŸµ Audio Quality Metrics:\n",
      "   â”œâ”€ Total samples: 121\n",
      "   â”œâ”€ Avg quality score: 0.775\n",
      "   â”œâ”€ Avg SNR: 67.0 dB\n",
      "   â”œâ”€ Avg duration: 27.96 seconds\n",
      "   â”œâ”€ Avg RMS: 0.1208\n",
      "   â”‚\n",
      "   â”œâ”€ Quality distribution:\n",
      "   â”‚  â”œâ”€ Excellent (â‰¥0.7): 95 (78.5%)\n",
      "   â”‚  â”œâ”€ Good (0.5-0.7): 26 (21.5%)\n",
      "   â”‚  â”œâ”€ Fair (0.3-0.5): 0 (0.0%)\n",
      "   â”‚  â””â”€ Poor (<0.3): 0 (0.0%)\n",
      "   â”‚\n",
      "   â”œâ”€ Low quality samples (< 0.3): 0 (0.0%)\n",
      "   â”œâ”€ Low SNR samples (< 10 dB): 0\n",
      "   â””â”€ Too short samples (< 0.5s): 0\n",
      "\n",
      "   ğŸ“Š Pause Features: 17 features\n",
      "      â”œâ”€ Avg hesitation score: 0.199\n",
      "      â”œâ”€ Avg pause frequency: 0.175 pauses/sec\n",
      "      â””â”€ Avg number of pauses: 4.8\n",
      "\n",
      "======================================================================\n",
      "ğŸ‘¤ CHECKING LANDMARK QUALITY - RLT\n",
      "======================================================================\n",
      "ğŸ“Š Loading landmark dataset (this may take a while)...\n",
      "\n",
      "ğŸ‘¤ Landmark Extraction:\n",
      "   â”œâ”€ Unique videos: 121\n",
      "   â”œâ”€ Total frames: 89,853\n",
      "   â”‚\n",
      "   â”œâ”€ Face landmarks:\n",
      "   â”‚  â”œâ”€ Count: 468 landmarks\n",
      "   â”‚  â”œâ”€ Detected: 86,248 frames\n",
      "   â”‚  â””â”€ Detection rate: 96.0%\n",
      "   â”‚\n",
      "   â”œâ”€ Iris landmarks:\n",
      "   â”‚  â”œâ”€ Count: 10 landmarks\n",
      "   â”‚  â”œâ”€ Detected: 86,248 frames\n",
      "   â”‚  â””â”€ Detection rate: 96.0%\n",
      "   â”‚\n",
      "   â””â”€ Pose landmarks:\n",
      "      â”œâ”€ Count: 33 landmarks\n",
      "      â”œâ”€ Detected: 89,583 frames\n",
      "      â””â”€ Detection rate: 99.7%\n",
      "\n",
      "======================================================================\n",
      "ğŸ”„ CHECKING CROSS-DATASET CONSISTENCY - RLT\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Sample Counts:\n",
      "   â”œâ”€ text_indonesian          : 121 samples\n",
      "   â”œâ”€ text_english             : 121 samples\n",
      "   â”œâ”€ number_features          : 121 samples\n",
      "   â”œâ”€ audio_features           : 121 samples\n",
      "   â”œâ”€ pause_features           : 121 samples\n",
      "   â”œâ”€ landmarks                : 89,853 samples\n",
      "   â”œâ”€ multimodal_full          : 121 samples\n",
      "   â”œâ”€ publication              : 121 samples\n",
      "\n",
      "   âœ… All non-landmark datasets have consistent sample counts: 121\n",
      "\n",
      "ğŸ“„ Filename Consistency:\n",
      "   âœ… All filenames match between datasets\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š GENERATING QUALITY VISUALIZATIONS - RLT\n",
      "======================================================================\n",
      "âœ… Audio quality visualization saved\n",
      "âœ… Text quality visualization saved\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ GENERATING QUALITY REPORT - RLT\n",
      "======================================================================\n",
      "âœ… JSON report saved: f:\\MULAI LAGI\\eKSTRAKSI - FULL DATA\\dataset\\validation\\RLT\\quality_reports\\quality_check_report_RLT.json\n",
      "âœ… Markdown summary saved: f:\\MULAI LAGI\\eKSTRAKSI - FULL DATA\\dataset\\validation\\RLT\\quality_reports\\QUALITY_CHECK_SUMMARY_RLT.md\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š FINAL SUMMARY - BOTH DATASETS\n",
      "======================================================================\n",
      "\n",
      "I3D:\n",
      "  âœ… Status: PASS\n",
      "  â”œâ”€ Issues: 0\n",
      "  â”œâ”€ Warnings: 0\n",
      "  â””â”€ Reports: f:\\MULAI LAGI\\eKSTRAKSI - FULL DATA\\dataset\\validation\\I3D\\quality_reports\n",
      "\n",
      "RLT:\n",
      "  âœ… Status: PASS\n",
      "  â”œâ”€ Issues: 0\n",
      "  â”œâ”€ Warnings: 0\n",
      "  â””â”€ Reports: f:\\MULAI LAGI\\eKSTRAKSI - FULL DATA\\dataset\\validation\\RLT\\quality_reports\n",
      "\n",
      "======================================================================\n",
      "âœ… DUAL DATASET QUALITY CHECK COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "=============================================================================\n",
    "DUAL DATASET QUALITY CHECKER - I3D & RLT\n",
    "=============================================================================\n",
    "Comprehensive quality validation for both I3D and RLT datasets\n",
    "Fully automated - processes both datasets sequentially\n",
    "=============================================================================\n",
    "Version: 2.0\n",
    "Date: 2025-01-14\n",
    "Author: Yeni Dwi Rahayu\n",
    "Institution: Universitas Muhammadiyah Jember\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"Convert NumPy types to Python native types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(convert_numpy_types(item) for item in obj)\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "class QualityCheckConfig:\n",
    "    \"\"\"Configuration for quality checks\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name: str = 'I3D'):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.base_dir = Path(os.getcwd())\n",
    "        self.dataset_dir = self.base_dir / \"dataset\" / \"processed\" / dataset_name\n",
    "        \n",
    "        # Expected paths\n",
    "        self.paths = {\n",
    "            'text_indonesian': self.dataset_dir / \"text\" / \"TextDataset_Indonesian.csv\",\n",
    "            'text_english': self.dataset_dir / \"text\" / \"TextDataset_English.csv\",\n",
    "            'number_features': self.dataset_dir / \"text\" / \"NumberFeatures.csv\",\n",
    "            'audio_features': self.dataset_dir / \"audio\" / \"AudioDataset_Features.csv\",\n",
    "            'pause_features': self.dataset_dir / \"audio\" / \"PauseFeatures.csv\",\n",
    "            'landmarks': self.dataset_dir / \"visual\" / \"LandmarkDataset.csv\",\n",
    "            'multimodal_full': self.dataset_dir / \"multimodal\" / \"MultimodalDataset_Full.csv\",\n",
    "            'publication': self.dataset_dir / \"multimodal\" / \"PublicationDataset.csv\"\n",
    "        }\n",
    "        \n",
    "        # Output paths\n",
    "        self.validation_dir = self.base_dir / \"dataset\" / \"validation\" / dataset_name\n",
    "        self.quality_reports_dir = self.validation_dir / \"quality_reports\"\n",
    "        self.quality_figures_dir = self.validation_dir / \"quality_figures\"\n",
    "        \n",
    "        # Create output directories\n",
    "        self.quality_reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.quality_figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Quality thresholds\n",
    "        self.thresholds = {\n",
    "            'min_audio_quality': 0.3,\n",
    "            'min_snr': 10,\n",
    "            'min_duration': 0.5,\n",
    "            'min_word_count': 3,\n",
    "            'min_face_detection_rate': 50,  # %\n",
    "            'min_iris_detection_rate': 30,  # %\n",
    "            'max_missing_rate': 20,  # %\n",
    "            'class_balance_tolerance': 0.2  # 40-60% is acceptable\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== FILE EXISTENCE CHECK ====================\n",
    "def check_file_existence(config: QualityCheckConfig) -> Dict:\n",
    "    \"\"\"Check if all expected files exist\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“ CHECKING FILE EXISTENCE - {config.dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {\n",
    "        'total_expected': len(config.paths),\n",
    "        'found': 0,\n",
    "        'missing': 0,\n",
    "        'files': {}\n",
    "    }\n",
    "    \n",
    "    for name, path in config.paths.items():\n",
    "        exists = path.exists()\n",
    "        \n",
    "        if exists:\n",
    "            size = path.stat().st_size\n",
    "            size_mb = size / (1024 * 1024)\n",
    "            results['found'] += 1\n",
    "            results['files'][name] = {\n",
    "                'exists': True,\n",
    "                'path': str(path),\n",
    "                'size_bytes': size,\n",
    "                'size_mb': round(size_mb, 2)\n",
    "            }\n",
    "            print(f\"âœ… {name:25s} | {size_mb:8.2f} MB\")\n",
    "        else:\n",
    "            results['missing'] += 1\n",
    "            results['files'][name] = {\n",
    "                'exists': False,\n",
    "                'path': str(path)\n",
    "            }\n",
    "            print(f\"âŒ {name:25s} | MISSING\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ğŸ“Š Summary: {results['found']}/{results['total_expected']} files found\")\n",
    "    \n",
    "    if results['missing'] > 0:\n",
    "        print(f\"âš ï¸  WARNING: {results['missing']} files are missing!\")\n",
    "    else:\n",
    "        print(f\"âœ… All expected files exist!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==================== DATASET STRUCTURE CHECK ====================\n",
    "def check_dataset_structure(config: QualityCheckConfig) -> Dict:\n",
    "    \"\"\"Check structure and basic statistics of each dataset\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š CHECKING DATASET STRUCTURE - {config.dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, path in config.paths.items():\n",
    "        if not path.exists():\n",
    "            print(f\"â­ï¸  Skipping {name} (file not found)\")\n",
    "            results[name] = {'status': 'missing'}\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding='utf-8')\n",
    "            \n",
    "            # Basic statistics\n",
    "            n_rows = len(df)\n",
    "            n_cols = len(df.columns)\n",
    "            missing_total = df.isnull().sum().sum()\n",
    "            missing_rate = (missing_total / (n_rows * n_cols) * 100) if n_rows * n_cols > 0 else 0\n",
    "            \n",
    "            # Check for label column\n",
    "            label_col = None\n",
    "            if 'label' in df.columns:\n",
    "                label_col = 'label'\n",
    "            elif 'Label' in df.columns:\n",
    "                label_col = 'Label'\n",
    "            elif 'Class' in df.columns:\n",
    "                label_col = 'Class'\n",
    "            \n",
    "            has_label = label_col is not None\n",
    "            \n",
    "            if has_label:\n",
    "                class_dist = df[label_col].value_counts().to_dict()\n",
    "                n_truth = class_dist.get(0, 0)\n",
    "                n_lie = class_dist.get(1, 0)\n",
    "                balance = n_lie / (n_lie + n_truth) if (n_lie + n_truth) > 0 else 0\n",
    "            else:\n",
    "                class_dist = None\n",
    "                n_truth = 0\n",
    "                n_lie = 0\n",
    "                balance = 0\n",
    "            \n",
    "            results[name] = {\n",
    "                'status': 'ok',\n",
    "                'n_rows': n_rows,\n",
    "                'n_cols': n_cols,\n",
    "                'missing_total': int(missing_total),\n",
    "                'missing_rate': float(missing_rate),\n",
    "                'has_label': has_label,\n",
    "                'class_distribution': class_dist,\n",
    "                'n_truth': int(n_truth),\n",
    "                'n_lie': int(n_lie),\n",
    "                'balance': float(balance),\n",
    "                'columns': list(df.columns)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ“„ {name}\")\n",
    "            print(f\"   â”œâ”€ Rows: {n_rows:,}\")\n",
    "            print(f\"   â”œâ”€ Columns: {n_cols}\")\n",
    "            print(f\"   â”œâ”€ Missing values: {missing_total:,} ({missing_rate:.2f}%)\")\n",
    "            \n",
    "            if has_label:\n",
    "                print(f\"   â”œâ”€ Class distribution:\")\n",
    "                print(f\"   â”‚  â”œâ”€ TRUTH (0): {n_truth}\")\n",
    "                print(f\"   â”‚  â””â”€ LIE (1): {n_lie}\")\n",
    "                print(f\"   â””â”€ Class balance: {balance*100:.1f}% LIE\")\n",
    "                \n",
    "                # Check balance\n",
    "                if abs(balance - 0.5) > config.thresholds['class_balance_tolerance']:\n",
    "                    print(f\"      âš ï¸  Class imbalance detected!\")\n",
    "            else:\n",
    "                print(f\"   â””â”€ No label column found\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading {name}: {str(e)}\")\n",
    "            results[name] = {'status': 'error', 'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==================== TEXT QUALITY CHECK ====================\n",
    "def check_text_quality(config: QualityCheckConfig) -> Dict:\n",
    "    \"\"\"Check quality of text extraction\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“ CHECKING TEXT QUALITY - {config.dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Check Indonesian text\n",
    "    text_id_path = config.paths['text_indonesian']\n",
    "    if text_id_path.exists():\n",
    "        df_id = pd.read_csv(text_id_path)\n",
    "        \n",
    "        # Determine text column\n",
    "        text_col = None\n",
    "        if 'text_indonesian_normalized' in df_id.columns:\n",
    "            text_col = 'text_indonesian_normalized'\n",
    "        elif 'text_indonesian_original' in df_id.columns:\n",
    "            text_col = 'text_indonesian_original'\n",
    "        elif 'text_indonesian' in df_id.columns:\n",
    "            text_col = 'text_indonesian'\n",
    "        \n",
    "        if text_col:\n",
    "            empty_texts = df_id[text_col].isnull().sum() + (df_id[text_col] == '').sum()\n",
    "            very_short = (df_id['word_count_id'] < config.thresholds['min_word_count']).sum() if 'word_count_id' in df_id.columns else 0\n",
    "            \n",
    "            results['indonesian'] = {\n",
    "                'total_samples': len(df_id),\n",
    "                'empty_texts': int(empty_texts),\n",
    "                'very_short_texts': int(very_short),\n",
    "                'avg_word_count': float(df_id['word_count_id'].mean()) if 'word_count_id' in df_id.columns else 0,\n",
    "                'avg_char_count': float(df_id['char_count_id'].mean()) if 'char_count_id' in df_id.columns else 0,\n",
    "                'avg_sentiment': float(df_id['sentiment_id'].mean()) if 'sentiment_id' in df_id.columns else 0,\n",
    "                'avg_lexical_diversity': float(df_id['lexical_diversity_id'].mean()) if 'lexical_diversity_id' in df_id.columns else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ“„ Indonesian Text:\")\n",
    "            print(f\"   â”œâ”€ Total samples: {len(df_id)}\")\n",
    "            print(f\"   â”œâ”€ Empty texts: {empty_texts} ({empty_texts/len(df_id)*100:.1f}%)\")\n",
    "            print(f\"   â”œâ”€ Very short texts (< {config.thresholds['min_word_count']} words): {very_short}\")\n",
    "            print(f\"   â”œâ”€ Avg word count: {results['indonesian']['avg_word_count']:.1f}\")\n",
    "            print(f\"   â”œâ”€ Avg char count: {results['indonesian']['avg_char_count']:.1f}\")\n",
    "            print(f\"   â”œâ”€ Avg sentiment: {results['indonesian']['avg_sentiment']:.3f}\")\n",
    "            print(f\"   â””â”€ Avg lexical diversity: {results['indonesian']['avg_lexical_diversity']:.3f}\")\n",
    "            \n",
    "            if empty_texts > 0:\n",
    "                print(f\"      âš ï¸  {empty_texts} samples have empty text!\")\n",
    "    \n",
    "    # Check English text\n",
    "    text_en_path = config.paths['text_english']\n",
    "    if text_en_path.exists():\n",
    "        df_en = pd.read_csv(text_en_path)\n",
    "        \n",
    "        if 'text_english' in df_en.columns:\n",
    "            empty_texts = df_en['text_english'].isnull().sum() + (df_en['text_english'] == '').sum()\n",
    "            very_short = (df_en['word_count_en'] < config.thresholds['min_word_count']).sum() if 'word_count_en' in df_en.columns else 0\n",
    "            \n",
    "            results['english'] = {\n",
    "                'total_samples': len(df_en),\n",
    "                'empty_texts': int(empty_texts),\n",
    "                'very_short_texts': int(very_short),\n",
    "                'avg_word_count': float(df_en['word_count_en'].mean()) if 'word_count_en' in df_en.columns else 0,\n",
    "                'avg_char_count': float(df_en['char_count_en'].mean()) if 'char_count_en' in df_en.columns else 0,\n",
    "                'translation_success_rate': float((len(df_en) - empty_texts) / len(df_en) * 100) if len(df_en) > 0 else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ“„ English Text (Translation):\")\n",
    "            print(f\"   â”œâ”€ Total samples: {len(df_en)}\")\n",
    "            print(f\"   â”œâ”€ Empty translations: {empty_texts} ({empty_texts/len(df_en)*100:.1f}%)\")\n",
    "            print(f\"   â”œâ”€ Translation success rate: {results['english']['translation_success_rate']:.1f}%\")\n",
    "            print(f\"   â”œâ”€ Avg word count: {results['english']['avg_word_count']:.1f}\")\n",
    "            print(f\"   â””â”€ Avg char count: {results['english']['avg_char_count']:.1f}\")\n",
    "            \n",
    "            if empty_texts > len(df_en) * 0.1:\n",
    "                print(f\"      âš ï¸  More than 10% translations failed!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==================== AUDIO QUALITY CHECK ====================\n",
    "def check_audio_quality(config: QualityCheckConfig) -> Dict:\n",
    "    \"\"\"Check quality of audio features\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸµ CHECKING AUDIO QUALITY - {config.dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Try to load from audio_features (which includes quality metrics)\n",
    "    audio_path = config.paths['audio_features']\n",
    "    if audio_path.exists():\n",
    "        df = pd.read_csv(audio_path)\n",
    "        \n",
    "        # Check if quality columns exist\n",
    "        has_quality = 'audio_quality_score' in df.columns\n",
    "        \n",
    "        if has_quality:\n",
    "            # Quality metrics\n",
    "            low_quality = (df['audio_quality_score'] < config.thresholds['min_audio_quality']).sum()\n",
    "            low_snr = (df['audio_snr'] < config.thresholds['min_snr']).sum() if 'audio_snr' in df.columns else 0\n",
    "            too_short = (df['audio_duration'] < config.thresholds['min_duration']).sum() if 'audio_duration' in df.columns else 0\n",
    "            \n",
    "            results = {\n",
    "                'total_samples': len(df),\n",
    "                'low_quality_count': int(low_quality),\n",
    "                'low_quality_rate': float(low_quality / len(df) * 100) if len(df) > 0 else 0,\n",
    "                'low_snr_count': int(low_snr),\n",
    "                'too_short_count': int(too_short),\n",
    "                'avg_quality_score': float(df['audio_quality_score'].mean()),\n",
    "                'avg_snr': float(df['audio_snr'].mean()) if 'audio_snr' in df.columns else 0,\n",
    "                'avg_duration': float(df['audio_duration'].mean()) if 'audio_duration' in df.columns else 0,\n",
    "                'avg_rms': float(df['audio_rms'].mean()) if 'audio_rms' in df.columns else 0,\n",
    "                'quality_distribution': {\n",
    "                    'excellent': int((df['audio_quality_score'] >= 0.7).sum()),\n",
    "                    'good': int(((df['audio_quality_score'] >= 0.5) & (df['audio_quality_score'] < 0.7)).sum()),\n",
    "                    'fair': int(((df['audio_quality_score'] >= 0.3) & (df['audio_quality_score'] < 0.5)).sum()),\n",
    "                    'poor': int((df['audio_quality_score'] < 0.3).sum())\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸµ Audio Quality Metrics:\")\n",
    "            print(f\"   â”œâ”€ Total samples: {len(df)}\")\n",
    "            print(f\"   â”œâ”€ Avg quality score: {results['avg_quality_score']:.3f}\")\n",
    "            print(f\"   â”œâ”€ Avg SNR: {results['avg_snr']:.1f} dB\")\n",
    "            print(f\"   â”œâ”€ Avg duration: {results['avg_duration']:.2f} seconds\")\n",
    "            print(f\"   â”œâ”€ Avg RMS: {results['avg_rms']:.4f}\")\n",
    "            print(f\"   â”‚\")\n",
    "            print(f\"   â”œâ”€ Quality distribution:\")\n",
    "            print(f\"   â”‚  â”œâ”€ Excellent (â‰¥0.7): {results['quality_distribution']['excellent']} ({results['quality_distribution']['excellent']/len(df)*100:.1f}%)\")\n",
    "            print(f\"   â”‚  â”œâ”€ Good (0.5-0.7): {results['quality_distribution']['good']} ({results['quality_distribution']['good']/len(df)*100:.1f}%)\")\n",
    "            print(f\"   â”‚  â”œâ”€ Fair (0.3-0.5): {results['quality_distribution']['fair']} ({results['quality_distribution']['fair']/len(df)*100:.1f}%)\")\n",
    "            print(f\"   â”‚  â””â”€ Poor (<0.3): {results['quality_distribution']['poor']} ({results['quality_distribution']['poor']/len(df)*100:.1f}%)\")\n",
    "            print(f\"   â”‚\")\n",
    "            print(f\"   â”œâ”€ Low quality samples (< {config.thresholds['min_audio_quality']}): {low_quality} ({results['low_quality_rate']:.1f}%)\")\n",
    "            print(f\"   â”œâ”€ Low SNR samples (< {config.thresholds['min_snr']} dB): {low_snr}\")\n",
    "            print(f\"   â””â”€ Too short samples (< {config.thresholds['min_duration']}s): {too_short}\")\n",
    "            \n",
    "            if results['low_quality_rate'] > 20:\n",
    "                print(f\"      âš ï¸  More than 20% samples have low audio quality!\")\n",
    "        \n",
    "        # Check pause features\n",
    "        pause_path = config.paths['pause_features']\n",
    "        if pause_path.exists():\n",
    "            df_pause = pd.read_csv(pause_path)\n",
    "            \n",
    "            print(f\"\\n   ğŸ“Š Pause Features: {len(df_pause.columns) - 3} features\")  # -3 for filename, label, dataset\n",
    "            \n",
    "            if 'pause_hesitation_score' in df_pause.columns:\n",
    "                avg_hesitation = df_pause['pause_hesitation_score'].mean()\n",
    "                results['avg_hesitation_score'] = float(avg_hesitation)\n",
    "                print(f\"      â”œâ”€ Avg hesitation score: {avg_hesitation:.3f}\")\n",
    "            \n",
    "            if 'pause_pause_frequency' in df_pause.columns:\n",
    "                avg_pause_freq = df_pause['pause_pause_frequency'].mean()\n",
    "                results['avg_pause_frequency'] = float(avg_pause_freq)\n",
    "                print(f\"      â”œâ”€ Avg pause frequency: {avg_pause_freq:.3f} pauses/sec\")\n",
    "            \n",
    "            if 'pause_num_pauses' in df_pause.columns:\n",
    "                avg_num_pauses = df_pause['pause_num_pauses'].mean()\n",
    "                results['avg_num_pauses'] = float(avg_num_pauses)\n",
    "                print(f\"      â””â”€ Avg number of pauses: {avg_num_pauses:.1f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==================== LANDMARK QUALITY CHECK ====================\n",
    "def check_landmark_quality(config: QualityCheckConfig) -> Dict:\n",
    "    \"\"\"Check quality of landmark extraction\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ‘¤ CHECKING LANDMARK QUALITY - {config.dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    landmark_path = config.paths['landmarks']\n",
    "    if landmark_path.exists():\n",
    "        print(\"ğŸ“Š Loading landmark dataset (this may take a while)...\")\n",
    "        df = pd.read_csv(landmark_path)\n",
    "        \n",
    "        # Count unique videos\n",
    "        unique_videos = df['Video_Name'].nunique() if 'Video_Name' in df.columns else 0\n",
    "        total_frames = len(df)\n",
    "        \n",
    "        # Identify landmark columns\n",
    "        face_cols = [col for col in df.columns if col.startswith('Landmark_') and \n",
    "                     col.endswith(('_X', '_Y', '_Z')) and \n",
    "                     int(col.split('_')[1]) < 468]\n",
    "        \n",
    "        iris_cols = [col for col in df.columns if col.startswith('Landmark_') and \n",
    "                     col.endswith(('_X', '_Y', '_Z')) and \n",
    "                     468 <= int(col.split('_')[1]) < 478]\n",
    "        \n",
    "        pose_cols = [col for col in df.columns if col.startswith('Pose_') and \n",
    "                     col.endswith(('_X', '_Y', '_Z'))]\n",
    "        \n",
    "        # Count frames with detected landmarks\n",
    "        face_detected = (df[face_cols].sum(axis=1) != 0).sum() if face_cols else 0\n",
    "        face_detection_rate = (face_detected / total_frames * 100) if total_frames > 0 else 0\n",
    "        \n",
    "        iris_detected = (df[iris_cols].sum(axis=1) != 0).sum() if iris_cols else 0\n",
    "        iris_detection_rate = (iris_detected / total_frames * 100) if total_frames > 0 else 0\n",
    "        \n",
    "        pose_detected = (df[pose_cols].sum(axis=1) != 0).sum() if pose_cols else 0\n",
    "        pose_detection_rate = (pose_detected / total_frames * 100) if total_frames > 0 else 0\n",
    "        \n",
    "        results = {\n",
    "            'unique_videos': int(unique_videos),\n",
    "            'total_frames': int(total_frames),\n",
    "            'face_landmarks_count': len(face_cols) // 3,\n",
    "            'iris_landmarks_count': len(iris_cols) // 3,\n",
    "            'pose_landmarks_count': len(pose_cols) // 3,\n",
    "            'face_detected': int(face_detected),\n",
    "            'face_detection_rate': float(face_detection_rate),\n",
    "            'iris_detected': int(iris_detected),\n",
    "            'iris_detection_rate': float(iris_detection_rate),\n",
    "            'pose_detected': int(pose_detected),\n",
    "            'pose_detection_rate': float(pose_detection_rate)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ‘¤ Landmark Extraction:\")\n",
    "        print(f\"   â”œâ”€ Unique videos: {unique_videos}\")\n",
    "        print(f\"   â”œâ”€ Total frames: {total_frames:,}\")\n",
    "        print(f\"   â”‚\")\n",
    "        print(f\"   â”œâ”€ Face landmarks:\")\n",
    "        print(f\"   â”‚  â”œâ”€ Count: {results['face_landmarks_count']} landmarks\")\n",
    "        print(f\"   â”‚  â”œâ”€ Detected: {face_detected:,} frames\")\n",
    "        print(f\"   â”‚  â””â”€ Detection rate: {face_detection_rate:.1f}%\")\n",
    "        print(f\"   â”‚\")\n",
    "        print(f\"   â”œâ”€ Iris landmarks:\")\n",
    "        print(f\"   â”‚  â”œâ”€ Count: {results['iris_landmarks_count']} landmarks\")\n",
    "        print(f\"   â”‚  â”œâ”€ Detected: {iris_detected:,} frames\")\n",
    "        print(f\"   â”‚  â””â”€ Detection rate: {iris_detection_rate:.1f}%\")\n",
    "        print(f\"   â”‚\")\n",
    "        print(f\"   â””â”€ Pose landmarks:\")\n",
    "        print(f\"      â”œâ”€ Count: {results['pose_landmarks_count']} landmarks\")\n",
    "        print(f\"      â”œâ”€ Detected: {pose_detected:,} frames\")\n",
    "        print(f\"      â””â”€ Detection rate: {pose_detection_rate:.1f}%\")\n",
    "        \n",
    "        # Warnings\n",
    "        if face_detection_rate < config.thresholds['min_face_detection_rate']:\n",
    "            print(f\"\\n      âš ï¸  Face detection rate is below {config.thresholds['min_face_detection_rate']}%!\")\n",
    "        \n",
    "        if iris_detection_rate < config.thresholds['min_iris_detection_rate']:\n",
    "            print(f\"      âš ï¸  Iris detection rate is below {config.thresholds['min_iris_detection_rate']}%!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==================== CROSS-DATASET CONSISTENCY CHECK ====================\n",
    "def check_cross_dataset_consistency(config: QualityCheckConfig) -> Dict:\n",
    "    \"\"\"Check consistency across different datasets\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ”„ CHECKING CROSS-DATASET CONSISTENCY - {config.dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Load all datasets\n",
    "    datasets = {}\n",
    "    \n",
    "    for name, path in config.paths.items():\n",
    "        if path.exists():\n",
    "            try:\n",
    "                datasets[name] = pd.read_csv(path)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"âš ï¸  No datasets loaded for consistency check\")\n",
    "        return results\n",
    "    \n",
    "    # Check sample counts\n",
    "    sample_counts = {name: len(df) for name, df in datasets.items()}\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Sample Counts:\")\n",
    "    for name, count in sample_counts.items():\n",
    "        print(f\"   â”œâ”€ {name:25s}: {count:,} samples\")\n",
    "    \n",
    "    # Check if counts match (excluding landmarks which have frames)\n",
    "    non_landmark_datasets = {k: v for k, v in sample_counts.items() if k != 'landmarks'}\n",
    "    \n",
    "    if non_landmark_datasets:\n",
    "        counts = list(non_landmark_datasets.values())\n",
    "        all_match = all(c == counts[0] for c in counts)\n",
    "        \n",
    "        results['sample_count_consistency'] = {\n",
    "            'all_match': all_match,\n",
    "            'counts': non_landmark_datasets\n",
    "        }\n",
    "        \n",
    "        if all_match:\n",
    "            print(f\"\\n   âœ… All non-landmark datasets have consistent sample counts: {counts[0]}\")\n",
    "        else:\n",
    "            print(f\"\\n   âš ï¸  Sample counts don't match across datasets!\")\n",
    "    \n",
    "    # Check filename consistency\n",
    "    if 'multimodal_full' in datasets and 'text_indonesian' in datasets:\n",
    "        filenames_multi = set(datasets['multimodal_full']['filename'].values)\n",
    "        filenames_text = set(datasets['text_indonesian']['filename'].values)\n",
    "        \n",
    "        missing_in_multi = filenames_text - filenames_multi\n",
    "        missing_in_text = filenames_multi - filenames_text\n",
    "        \n",
    "        results['filename_consistency'] = {\n",
    "            'match': len(missing_in_multi) == 0 and len(missing_in_text) == 0,\n",
    "            'missing_in_multimodal': len(missing_in_multi),\n",
    "            'missing_in_text': len(missing_in_text)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ“„ Filename Consistency:\")\n",
    "        if results['filename_consistency']['match']:\n",
    "            print(f\"   âœ… All filenames match between datasets\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  Filename mismatch detected:\")\n",
    "            print(f\"      â”œâ”€ Missing in multimodal: {len(missing_in_multi)}\")\n",
    "            print(f\"      â””â”€ Missing in text: {len(missing_in_text)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==================== GENERATE QUALITY VISUALIZATIONS ====================\n",
    "def generate_quality_visualizations(config: QualityCheckConfig):\n",
    "    \"\"\"Generate quality check visualizations\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š GENERATING QUALITY VISUALIZATIONS - {config.dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    try:\n",
    "        # Load datasets\n",
    "        df_audio = pd.read_csv(config.paths['audio_features'])\n",
    "        df_text_id = pd.read_csv(config.paths['text_indonesian'])\n",
    "        \n",
    "        # ===== VISUALIZATION 1: Audio Quality Distribution =====\n",
    "        if 'audio_quality_score' in df_audio.columns:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            \n",
    "            # Quality score distribution\n",
    "            axes[0, 0].hist(df_audio['audio_quality_score'], bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "            axes[0, 0].axvline(config.thresholds['min_audio_quality'], color='red', linestyle='--', \n",
    "                              label=f'Threshold ({config.thresholds[\"min_audio_quality\"]})')\n",
    "            axes[0, 0].set_xlabel('Audio Quality Score')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].set_title('Audio Quality Score Distribution')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # SNR distribution\n",
    "            if 'audio_snr' in df_audio.columns:\n",
    "                axes[0, 1].hist(df_audio['audio_snr'], bins=30, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "                axes[0, 1].axvline(config.thresholds['min_snr'], color='red', linestyle='--', \n",
    "                                  label=f'Threshold ({config.thresholds[\"min_snr\"]} dB)')\n",
    "                axes[0, 1].set_xlabel('SNR (dB)')\n",
    "                axes[0, 1].set_ylabel('Frequency')\n",
    "                axes[0, 1].set_title('Signal-to-Noise Ratio Distribution')\n",
    "                axes[0, 1].legend()\n",
    "                axes[0, 1].grid(alpha=0.3)\n",
    "            \n",
    "            # Duration distribution\n",
    "            if 'audio_duration' in df_audio.columns:\n",
    "                axes[1, 0].hist(df_audio['audio_duration'], bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "                axes[1, 0].axvline(config.thresholds['min_duration'], color='red', linestyle='--', \n",
    "                                  label=f'Threshold ({config.thresholds[\"min_duration\"]}s)')\n",
    "                axes[1, 0].set_xlabel('Duration (seconds)')\n",
    "                axes[1, 0].set_ylabel('Frequency')\n",
    "                axes[1, 0].set_title('Audio Duration Distribution')\n",
    "                axes[1, 0].legend()\n",
    "                axes[1, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # Quality categories\n",
    "            quality_categories = pd.cut(df_audio['audio_quality_score'], \n",
    "                                        bins=[0, 0.3, 0.5, 0.7, 1.0],\n",
    "                                        labels=['Poor', 'Fair', 'Good', 'Excellent'])\n",
    "            quality_counts = quality_categories.value_counts()\n",
    "            \n",
    "            axes[1, 1].bar(quality_counts.index, quality_counts.values, \n",
    "                          color=['#e74c3c', '#f39c12', '#3498db', '#2ecc71'], \n",
    "                          alpha=0.7, edgecolor='black')\n",
    "            axes[1, 1].set_xlabel('Quality Category')\n",
    "            axes[1, 1].set_ylabel('Count')\n",
    "            axes[1, 1].set_title('Audio Quality Categories')\n",
    "            axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.suptitle(f'Audio Quality Analysis - {config.dataset_name}', fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(config.quality_figures_dir / 'audio_quality_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"âœ… Audio quality visualization saved\")\n",
    "        \n",
    "        # ===== VISUALIZATION 2: Text Quality Distribution =====\n",
    "        if 'word_count_id' in df_text_id.columns:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            \n",
    "            # Word count distribution\n",
    "            axes[0, 0].hist(df_text_id['word_count_id'], bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "            axes[0, 0].axvline(config.thresholds['min_word_count'], color='red', linestyle='--', \n",
    "                              label=f'Threshold ({config.thresholds[\"min_word_count\"]})')\n",
    "            axes[0, 0].set_xlabel('Word Count')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].set_title('Word Count Distribution (Indonesian)')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # Sentiment distribution\n",
    "            if 'sentiment_id' in df_text_id.columns:\n",
    "                axes[0, 1].hist(df_text_id['sentiment_id'], bins=30, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "                axes[0, 1].set_xlabel('Sentiment Score')\n",
    "                axes[0, 1].set_ylabel('Frequency')\n",
    "                axes[0, 1].set_title('Sentiment Distribution')\n",
    "                axes[0, 1].grid(alpha=0.3)\n",
    "            \n",
    "            # Lexical diversity\n",
    "            if 'lexical_diversity_id' in df_text_id.columns:\n",
    "                axes[1, 0].hist(df_text_id['lexical_diversity_id'], bins=30, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "                axes[1, 0].set_xlabel('Lexical Diversity')\n",
    "                axes[1, 0].set_ylabel('Frequency')\n",
    "                axes[1, 0].set_title('Lexical Diversity Distribution')\n",
    "                axes[1, 0].grid(alpha=0.3)\n",
    "            \n",
    "            # Complexity\n",
    "            if 'complexity_id' in df_text_id.columns:\n",
    "                axes[1, 1].hist(df_text_id['complexity_id'], bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "                axes[1, 1].set_xlabel('Complexity Score')\n",
    "                axes[1, 1].set_ylabel('Frequency')\n",
    "                axes[1, 1].set_title('Text Complexity Distribution')\n",
    "                axes[1, 1].grid(alpha=0.3)\n",
    "            \n",
    "            plt.suptitle(f'Text Quality Analysis - {config.dataset_name}', fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(config.quality_figures_dir / 'text_quality_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"âœ… Text quality visualization saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating visualizations: {str(e)}\")\n",
    "\n",
    "\n",
    "# ==================== GENERATE QUALITY REPORT ====================\n",
    "def generate_quality_report(all_results: Dict, config: QualityCheckConfig) -> Dict:\n",
    "    \"\"\"Generate comprehensive quality report\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“ GENERATING QUALITY REPORT - {config.dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    report = {\n",
    "        'report_info': {\n",
    "            'generated_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'version': '2.0',\n",
    "            'dataset_name': config.dataset_name,\n",
    "            'dataset_directory': str(config.dataset_dir)\n",
    "        },\n",
    "        'file_existence': all_results.get('file_existence', {}),\n",
    "        'dataset_structure': all_results.get('dataset_structure', {}),\n",
    "        'text_quality': all_results.get('text_quality', {}),\n",
    "        'audio_quality': all_results.get('audio_quality', {}),\n",
    "        'landmark_quality': all_results.get('landmark_quality', {}),\n",
    "        'cross_dataset_consistency': all_results.get('cross_dataset_consistency', {}),\n",
    "        'thresholds_used': config.thresholds,\n",
    "        'overall_assessment': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate overall assessment\n",
    "    issues = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check file existence\n",
    "    if all_results.get('file_existence', {}).get('missing', 0) > 0:\n",
    "        issues.append(f\"{all_results['file_existence']['missing']} files are missing\")\n",
    "    \n",
    "    # Check text quality\n",
    "    text_quality = all_results.get('text_quality', {})\n",
    "    if text_quality.get('indonesian', {}).get('empty_texts', 0) > 0:\n",
    "        empty_count = text_quality['indonesian']['empty_texts']\n",
    "        total = text_quality['indonesian']['total_samples']\n",
    "        warnings.append(f\"{empty_count} samples have empty text ({empty_count/total*100:.1f}%)\")\n",
    "    \n",
    "    # Check audio quality\n",
    "    audio_quality = all_results.get('audio_quality', {})\n",
    "    if audio_quality.get('low_quality_rate', 0) > 20:\n",
    "        warnings.append(f\"High rate of low-quality audio: {audio_quality['low_quality_rate']:.1f}%\")\n",
    "    \n",
    "    # Check landmark detection\n",
    "    landmark_quality = all_results.get('landmark_quality', {})\n",
    "    if landmark_quality.get('face_detection_rate', 0) < config.thresholds['min_face_detection_rate']:\n",
    "        warnings.append(f\"Low face detection rate: {landmark_quality['face_detection_rate']:.1f}%\")\n",
    "    \n",
    "    if landmark_quality.get('iris_detection_rate', 0) < config.thresholds['min_iris_detection_rate']:\n",
    "        warnings.append(f\"Low iris detection rate: {landmark_quality['iris_detection_rate']:.1f}%\")\n",
    "    \n",
    "    report['overall_assessment'] = {\n",
    "        'status': 'PASS' if len(issues) == 0 else 'FAIL',\n",
    "        'issues': issues,\n",
    "        'warnings': warnings,\n",
    "        'recommendation': 'Dataset is ready for use' if len(issues) == 0 and len(warnings) == 0 else 'Please review issues and warnings'\n",
    "    }\n",
    "    \n",
    "    # Convert numpy types\n",
    "    report = convert_numpy_types(report)\n",
    "    \n",
    "    # Save JSON report\n",
    "    report_path = config.quality_reports_dir / f'quality_check_report_{config.dataset_name}.json'\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… JSON report saved: {report_path}\")\n",
    "    \n",
    "    # Generate markdown summary\n",
    "    markdown_summary = f\"\"\"# ğŸ” Dataset Quality Check Report - {config.dataset_name}\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Dataset:** {config.dataset_name}  \n",
    "**Directory:** `{config.dataset_dir}`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Overall Assessment\n",
    "\n",
    "**Status:** {'âœ… PASS' if report['overall_assessment']['status'] == 'PASS' else 'âŒ FAIL'}\n",
    "\n",
    "### Issues ({len(issues)})\n",
    "{chr(10).join([f'- âŒ {issue}' for issue in issues]) if issues else '- âœ… No critical issues found'}\n",
    "\n",
    "### Warnings ({len(warnings)})\n",
    "{chr(10).join([f'- âš ï¸ {warning}' for warning in warnings]) if warnings else '- âœ… No warnings'}\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ File Existence\n",
    "\n",
    "- **Total Expected:** {all_results.get('file_existence', {}).get('total_expected', 0)}\n",
    "- **Found:** {all_results.get('file_existence', {}).get('found', 0)}\n",
    "- **Missing:** {all_results.get('file_existence', {}).get('missing', 0)}\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Dataset Statistics\n",
    "\n",
    "### Text Quality\n",
    "- **Indonesian Total:** {text_quality.get('indonesian', {}).get('total_samples', 0)}\n",
    "- **English Total:** {text_quality.get('english', {}).get('total_samples', 0)}\n",
    "- **Avg Word Count (ID):** {text_quality.get('indonesian', {}).get('avg_word_count', 0):.1f}\n",
    "- **Avg Word Count (EN):** {text_quality.get('english', {}).get('avg_word_count', 0):.1f}\n",
    "- **Translation Success:** {text_quality.get('english', {}).get('translation_success_rate', 0):.1f}%\n",
    "\n",
    "### Audio Quality\n",
    "- **Total Samples:** {audio_quality.get('total_samples', 0)}\n",
    "- **Avg Quality Score:** {audio_quality.get('avg_quality_score', 0):.3f}\n",
    "- **Avg SNR:** {audio_quality.get('avg_snr', 0):.1f} dB\n",
    "- **Low Quality Rate:** {audio_quality.get('low_quality_rate', 0):.1f}%\n",
    "\n",
    "### Landmark Quality\n",
    "- **Total Frames:** {landmark_quality.get('total_frames', 0):,}\n",
    "- **Face Detection:** {landmark_quality.get('face_detection_rate', 0):.1f}%\n",
    "- **Iris Detection:** {landmark_quality.get('iris_detection_rate', 0):.1f}%\n",
    "- **Pose Detection:** {landmark_quality.get('pose_detection_rate', 0):.1f}%\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Recommendation\n",
    "\n",
    "{report['overall_assessment']['recommendation']}\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by Dual Dataset Quality Checker v2.0*\n",
    "\"\"\"\n",
    "    \n",
    "    markdown_path = config.quality_reports_dir / f'QUALITY_CHECK_SUMMARY_{config.dataset_name}.md'\n",
    "    with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_summary)\n",
    "    \n",
    "    print(f\"âœ… Markdown summary saved: {markdown_path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# ==================== DUAL PROCESSOR ====================\n",
    "class DualQualityChecker:\n",
    "    \"\"\"Process both I3D and RLT datasets\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.datasets = ['I3D', 'RLT']\n",
    "        self.results = {}\n",
    "    \n",
    "    def check_all(self):\n",
    "        \"\"\"Check both datasets\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"ğŸ” DUAL DATASET QUALITY CHECKER\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Checking quality for both I3D and RLT datasets\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for dataset_name in self.datasets:\n",
    "            print(f\"\\n\\n{'#'*70}\")\n",
    "            print(f\"# CHECKING: {dataset_name}\")\n",
    "            print(f\"{'#'*70}\")\n",
    "            \n",
    "            try:\n",
    "                result = self.check_single_dataset(dataset_name)\n",
    "                self.results[dataset_name] = result\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Error checking {dataset_name}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                self.results[dataset_name] = {'status': 'error', 'message': str(e)}\n",
    "        \n",
    "        # Final summary\n",
    "        self.print_final_summary()\n",
    "    \n",
    "    def check_single_dataset(self, dataset_name: str) -> Dict:\n",
    "        \"\"\"Check single dataset\"\"\"\n",
    "        # Initialize\n",
    "        config = QualityCheckConfig(dataset_name)\n",
    "        \n",
    "        # Store all results\n",
    "        all_results = {}\n",
    "        \n",
    "        # Run checks\n",
    "        all_results['file_existence'] = check_file_existence(config)\n",
    "        all_results['dataset_structure'] = check_dataset_structure(config)\n",
    "        all_results['text_quality'] = check_text_quality(config)\n",
    "        all_results['audio_quality'] = check_audio_quality(config)\n",
    "        all_results['landmark_quality'] = check_landmark_quality(config)\n",
    "        all_results['cross_dataset_consistency'] = check_cross_dataset_consistency(config)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        generate_quality_visualizations(config)\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report = generate_quality_report(all_results, config)\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'overall_status': report['overall_assessment']['status'],\n",
    "            'issues_count': len(report['overall_assessment']['issues']),\n",
    "            'warnings_count': len(report['overall_assessment']['warnings']),\n",
    "            'reports_dir': str(config.quality_reports_dir)\n",
    "        }\n",
    "    \n",
    "    def print_final_summary(self):\n",
    "        \"\"\"Print final summary for both datasets\"\"\"\n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“Š FINAL SUMMARY - BOTH DATASETS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for dataset_name, result in self.results.items():\n",
    "            print(f\"\\n{dataset_name}:\")\n",
    "            \n",
    "            if result.get('status') == 'success':\n",
    "                print(f\"  âœ… Status: {result.get('overall_status', 'UNKNOWN')}\")\n",
    "                print(f\"  â”œâ”€ Issues: {result.get('issues_count', 0)}\")\n",
    "                print(f\"  â”œâ”€ Warnings: {result.get('warnings_count', 0)}\")\n",
    "                print(f\"  â””â”€ Reports: {result.get('reports_dir', 'N/A')}\")\n",
    "            elif result.get('status') == 'error':\n",
    "                print(f\"  âŒ Status: ERROR\")\n",
    "                print(f\"  â””â”€ Message: {result.get('message', 'Unknown error')}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ… DUAL DATASET QUALITY CHECK COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ==================== MAIN ====================\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    checker = DualQualityChecker()\n",
    "    checker.check_all()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bisa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
